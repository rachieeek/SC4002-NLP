{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c3ee3d",
   "metadata": {},
   "source": [
    "# SC4002 / CE4045 / CZ4045 NLP â€” Part 3.4: Targeted Improvements on Specific Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7f80d",
   "metadata": {},
   "source": [
    "(3.4) Instead of looking at generic performance improvement, think about targeted improvement on specific topics. Based on your findings in Part 2(e), design strategies aimed at improving performance specifically for those weaker topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94941f3",
   "metadata": {},
   "source": [
    "## From 2(e)\n",
    "| topic | test_acc |\n",
    "|-------|----------|\n",
    "| DESC  | 0.942029 |\n",
    "| ENTY  | 0.606383 |\n",
    "| HUM   | 0.953846 |\n",
    "| ABBR  | 0.333333 |\n",
    "| NUM   | 0.831858 |\n",
    "| LOC   | 0.888889 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3a8ee",
   "metadata": {},
   "source": [
    "## Weakest topics: (edit accordingly after Part 1 n 2 finalised)\n",
    "1. ABBR: Small number of samples available in the training and testing data. \n",
    "2. ENTY: Diverse questions, overlapping patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47077ec",
   "metadata": {},
   "source": [
    "### Approach #1: \n",
    "\n",
    "1. ABBR: Tackly scarcity of examples in the dataset: Oversample ABBR in training data (data side)\n",
    "2. ENTY: apply class-balanced loss / class weights to ensure the model does not bias toward frequent classes. (model side)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464da59",
   "metadata": {},
   "source": [
    "### Approach #2: \n",
    "\n",
    "1. ABBR: Apply class-balanced loss to penalise errors on rare classes more heavily\n",
    "2. ENTY: Introduce dual-channel embeddings (static + trainable) to better capture semantic variety within entity questions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
