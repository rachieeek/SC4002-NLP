{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51529dd9",
   "metadata": {},
   "source": [
    "---\n",
    "# SC4002 Natural Language Processing Group Project\n",
    "\n",
    "Group Members:\n",
    "1. Asher Lim Guojun (U2220846H)\n",
    "2. Celeste Ang Jianing (U2222319H)\n",
    "3. Koh Jia Hui Rachel (U2222747H)\n",
    "4. Lim Kiat Sen, Jaron (U2222010K)\n",
    "5. Pang Boslyn (U2221298A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170cd69",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec801a",
   "metadata": {},
   "source": [
    "Dev notes: I directly installed the model en_core_web_sm 3.7.1 via 'uv add'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "552a94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed = 0):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn as nn\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27e0fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 4362\n",
      "Validation examples: 1090\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    " # For tokenization\n",
    "TEXT = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm',include_lengths = True, pad_token='<pad>')\n",
    "# For multi - class classification labels\n",
    "LABEL = data.LabelField()\n",
    "# Load the TREC dataset\n",
    "train_data , test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained = False)\n",
    "# Split train into train/valid (80/20)\n",
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=0.8,\n",
    "    random_state=random.seed(42)  # <-- use random.seed here\n",
    ")\n",
    "\n",
    "print(f\"Train examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebf6e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['What', 'is', 'the', 'most', 'famous', 'German', 'word', 'in', 'the', 'English', 'language', '?'], 'label': 'ENTY'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469e348",
   "metadata": {},
   "source": [
    "Let's use Glove model for word embeddings.     \n",
    "(Start with GloVe 6B.100d or 6B.300d as it provides a good balance between coverage and performance for TREC's question classification task. If results are underwhelming, experiment with word2vec Google News vectors as an alternative, but empirical results in the literature suggest GloVe performs comparably or slightly better on this specific dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef350631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe, Vectors\n",
    "glove = GloVe(name='6B', dim=300)\n",
    "\n",
    "# Option 1: Use built-in GloVe (torchtext will download automatically)\n",
    "TEXT.build_vocab(train_data, vectors=glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b03cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc6243",
   "metadata": {},
   "source": [
    "---\n",
    "## Qn 1: Word Embeddings\n",
    "**a) What is the size of the vocabulary formed from your training data according to your tokenization method?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7e79a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Tokens: [['What', 'is', 'the', 'most', 'famous', 'German', 'word', 'in', 'the', 'English', 'language', '?'], ['Where', 'are', 'the', 'leading', 'medical', 'groups', 'specializing', 'in', 'lung', 'diseases', '?'], ['What', 'actor', 'said', 'in', 'A', 'Day', 'at', 'the', 'Races', ':', '`', '`', 'Either', 'he', \"'s\", 'dead', 'or', 'my', 'watch', 'has', 'stopped', \"''\", '?'], ['Which', 'area', 'produces', 'the', 'least', 'acidic', 'coffee', '?'], ['When', 'did', 'Princess', 'Diana', 'and', 'Prince', 'Charles', 'get', 'married', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the training dataset\n",
    "train_tokens = [example.text for example in train_data.examples]\n",
    "\n",
    "# Print the first 5 preprocessed sentences\n",
    "print(\"First 5 Tokens:\", train_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd14100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'?': 4280, 'the': 2883, 'What': 2562, 'is': 1350, 'of': 1231, 'in': 913, 'a': 807, '`': 691, 'How': 622, \"'s\": 588, 'was': 529, 'to': 460, ',': 456, 'Who': 453, 'are': 363, 'for': 362, 'and': 360, \"''\": 330, 'did': 312, 'does': 312, '-': 293, 'many': 268, 'do': 263, 'on': 244, 'Where': 226, 'name': 206, 'first': 163, 'I': 160, 'you': 147, 'can': 140, 'The': 132, 'that': 122, 'from': 120, 'When': 111, 'what': 110, 'by': 107, 'with': 107, 'U.S.': 107, 'most': 106, 'world': 102, 'as': 99, 'have': 97, 'country': 97, '.': 96, 'an': 94, 'has': 89, 'Which': 84, 'city': 84, \"'\": 81, 'there': 79, 'Why': 78, 'Name': 76, 'it': 74, 'people': 71, 'were': 71, 'be': 71, 'find': 69, 'his': 67, 'year': 67, 'made': 64, 'at': 63, 'largest': 63, 'called': 62, 'get': 61, 'American': 60, 'fear': 60, 'In': 60, 'two': 58, 'much': 57, 'mean': 56, 'state': 56, 'its': 51, 'between': 49, 'long': 48, ':': 47, 'who': 47, 'used': 47, 'origin': 47, 'company': 44, 'word': 43, 'about': 43, 'famous': 42, 'known': 42, 'when': 42, 'TV': 41, 'game': 40, 'one': 40, 'War': 40, 'movie': 40, 'or': 39, 'World': 38, 'film': 38, 'kind': 38, 'which': 37, 'he': 36, 'New': 36, 'only': 36, 'stand': 36, 'make': 35, 'out': 34, 'up': 34, 'wrote': 34, 'countries': 34, 'all': 33, 'invented': 33, 'best': 33, 'your': 32, 'not': 32, 'book': 32, 'color': 32, 'live': 32, 'come': 31, 'baseball': 31, 'won': 31, 'time': 31, 'take': 31, 'play': 31, 'into': 30, 'team': 30, 'President': 30, 'John': 30, 'South': 28, 'America': 28, 'term': 27, 'call': 27, 'president': 27, 'named': 27, 'home': 26, 'highest': 26, 'use': 26, 'their': 26, 'born': 25, 'last': 25, '1': 25, 'if': 25, 'average': 24, 'number': 24, 'second': 24, 'died': 24, 'four': 24, 'novel': 24, 'man': 24, 'body': 23, 'say': 23, 'played': 23, 'information': 23, 'United': 23, 'three': 23, 'had': 23, 'old': 23, 'food': 23, 'common': 23, 'character': 23, 'star': 23, 'English': 22, 'actor': 22, 'my': 22, 'work': 22, 'become': 22, 'like': 22, 'difference': 22, 'show': 22, 'during': 21, 'York': 21, 'would': 21, 'after': 21, 'actress': 21, 'said': 20, 'population': 20, 'been': 20, 'go': 20, 'war': 20, 'computer': 20, 'song': 20, 'killed': 20, 'top': 20, 'names': 20, 'day': 20, 'money': 20, 'group': 20, 'how': 19, 'dog': 19, 'major': 19, 'water': 19, 'located': 19, 'popular': 19, 'States': 19, 'drink': 19, 'years': 19, 'portrayed': 19, 'California': 19, 'King': 19, 'woman': 19, 'will': 19, 'some': 19, 'became': 18, 'over': 18, 'cost': 18, 'new': 18, 'college': 18, 'than': 18, 'sport': 18, 'space': 17, 'black': 17, 'person': 17, 'times': 17, 'capital': 17, 'they': 17, 'Randy': 17, 'Craft': 17, 'part': 17, 'title': 17, 'different': 17, 'river': 17, 'longest': 17, 'animal': 17, 'language': 16, 'miles': 16, 'North': 16, 'so': 16, 'Internet': 16, 'contains': 16, 'makes': 16, 'letter': 16, 'West': 16, 'school': 16, 'nickname': 16, 't': 16, 'good': 16, 'history': 16, 'more': 16, 'line': 16, 'Charles': 15, 'leader': 15, 'Earth': 15, 'mountain': 15, 'French': 15, 'car': 15, 'die': 15, 'through': 15, 'II': 15, 'each': 15, 'built': 15, 'NN': 15, 'A': 14, 'Day': 14, 'disease': 14, 'found': 14, 'Nixon': 14, 'British': 14, 'date': 14, '5': 14, 'human': 14, 'her': 14, 'address': 14, 'feet': 14, 'way': 14, 'James': 14, 'children': 14, 'island': 14, 'National': 14, 'office': 14, 'begin': 14, 'boasts': 13, 'St.': 13, 'form': 13, 'life': 13, 'Spanish': 13, 'me': 13, 'Airport': 13, 'City': 13, 'Russian': 13, 'player': 13, 'Germany': 13, 'someone': 13, 'Hitler': 13, 'real': 13, 'write': 13, 'created': 13, 'type': 13, 'century': 13, 'men': 13, 'Park': 13, 'abbreviation': 13, 'power': 13, 'air': 13, 'colors': 13, 'five': 13, 'features': 13, 'product': 13, 'area': 12, 'married': 12, 'biggest': 12, 'On': 12, 'moon': 12, 'Washington': 12, 'high': 12, 'happened': 12, 'William': 12, 'soft': 12, 'should': 12, 'business': 12, 'causes': 12, 'letters': 12, 'know': 12, 'meaning': 12, 'US': 12, 'lawyer': 12, 'female': 12, 'television': 12, 'Japanese': 12, 'eat': 12, 'following': 12, 'held': 12, 'Indians': 12, 'European': 12, 'tree': 12, 'blood': 12, '2': 12, '&': 12, 'system': 12, 'San': 12, 'father': 12, 'Great': 12, 'rate': 11, 'win': 11, 'animals': 11, 'hit': 11, 'Mississippi': 11, 'once': 11, 'University': 11, 'Kennedy': 11, 'cities': 11, 'building': 11, 'place': 11, 'River': 11, 'love': 11, 'singing': 11, 'son': 11, 'comedian': 11, 'George': 10, '1984': 10, 'Prize': 10, 'ball': 10, 'seven': 10, 'code': 10, 'birth': 10, '3': 10, 'them': 10, 'horse': 10, 'before': 10, 'no': 10, 'Marvel': 10, 'ever': 10, 'fast': 10, 'main': 10, 'Man': 10, 'Bible': 10, 'runs': 10, 'CNN': 10, 'Latin': 10, 'list': 10, 'My': 10, 'lives': 10, 'league': 10, 'beer': 10, 'down': 10, '$': 10, 'states': 10, 'store': 10, 'ship': 10, 'Christmas': 10, 'Greek': 10, 'hole': 10, 'prime': 10, 'minister': 10, 'tuberculosis': 10, 'newspaper': 10, 'where': 10, 'Queen': 10, 'species': 10, 'record': 10, 'chemical': 10, 'size': 10, 'mile': 10, 'Kentucky': 10, 'Whose': 10, 'law': 10, 'Jack': 9, 'plays': 9, 'Little': 9, 'Canada': 9, 'baby': 9, 'served': 9, 'cold': 9, 'another': 9, 'Sea': 9, 'left': 9, 'Jaws': 9, 'department': 9, 'words': 9, 'Big': 9, 'flag': 9, 'Old': 9, '10': 9, 'shot': 9, 'famed': 9, 'London': 9, 'lived': 9, 'six': 9, 'Texas': 9, 'music': 9, 'role': 9, 'see': 9, 'end': 9, 'appear': 9, 'Americans': 9, 'women': 9, 'Steven': 9, 'produce': 9, 'Jackson': 9, 'comic': 9, 'Europe': 9, 'being': 9, 'Pope': 9, 'months': 9, 'death': 9, 'sports': 9, 'football': 9, 'other': 9, 'introduced': 9, 'Star': 9, 'golf': 9, 'Berlin': 9, '6': 9, 'travel': 9, 'cartoon': 9, 'basketball': 9, 'big': 9, 'England': 9, 'brand': 9, 'May': 9, 'website': 9, 'soldiers': 9, 'Japan': 9, 'light': 8, 'Red': 8, 'Richard': 8, 'gold': 8, 'general': 8, 'International': 8, 'Soviet': 8, 'board': 8, 'start': 8, 'stop': 8, 'Bowl': 8, 'now': 8, 'member': 8, 'flight': 8, 'sea': 8, 'France': 8, 'ocean': 8, 'Island': 8, 'square': 8, 'tell': 8, 'greatest': 8, 'battle': 8, 'while': 8, 'making': 8, 'oldest': 8, 'F.': 8, 'saw': 8, 'we': 8, 'fought': 8, 'tallest': 8, 'El': 8, 'red': 8, 'Bridge': 8, 'former': 8, 'started': 8, 'Olympics': 8, 'claim': 8, 'far': 8, 'Australia': 8, 'weight': 8, 'Indian': 8, 'Sioux': 8, 'cowboy': 8, 'Horse': 8, 'starred': 8, 'setting': 8, '8': 8, 'Chicago': 8, 'General': 8, 'symbol': 8, 'face': 8, 'art': 8, 'Tom': 8, 'Africa': 8, 'Vietnam': 8, 'East': 8, 'founded': 8, 'organization': 8, 'originate': 8, 'then': 8, 'run': 8, 'head': 8, 'buried': 8, 'radio': 8, 'king': 8, 'rock': 8, 'March': 8, 'magazine': 8, 'fastest': 8, 'girl': 8, 'series': 8, 'O': 8, 'German': 7, 'medical': 7, 'watch': 7, 'least': 7, 'Mount': 7, 'look': 7, 'poet': 7, 'Peter': 7, 'minimum': 7, 'wage': 7, 'hand': 7, 'because': 7, 'selling': 7, 'mail': 7, 'Jimmy': 7, 'musical': 7, 'online': 7, 'need': 7, 'white': 7, 'worth': 7, 'Thomas': 7, 'current': 7, 'inside': 7, 'dubbed': 7, 'balls': 7, 'favorite': 7, 'web': 7, 'Aaron': 7, 'cars': 7, 'also': 7, 'percentage': 7, 'paper': 7, 'oil': 7, 'international': 7, 'plant': 7, 'Chinese': 7, 'Show': 7, 'wine': 7, 'got': 7, 'middle': 7, 'characters': 7, 'gave': 7, 'video': 7, 'Monopoly': 7, 'definition': 7, 'income': 7, 'NNP': 7, 'games': 7, 'allowed': 7, 'Battle': 7, 'sometimes': 7, 'singer': 7, 'Shakespeare': 7, 'God': 7, 'every': 7, 'university': 7, 'nationality': 7, 'this': 7, 'numbers': 7, 'fire': 7, 'caused': 7, 'Civil': 7, 'instrument': 7, 'successful': 7, 'director': 7, 'based': 7, 'never': 7, 'age': 7, 'child': 7, 'products': 7, 'male': 7, 'story': 7, 'rights': 7, 'considered': 7, 'ice': 7, 'us': 7, '1983': 7, 'produced': 7, 'telephone': 7, 'Bill': 7, 'center': 7, 'Dick': 7, 'Olympic': 7, 'hair': 7, 'author': 7, 'lost': 7, 'Henry': 7, 'wife': 7, 'E': 7, 'commercial': 7, 'Shea': 7, 'Gould': 7, 'Roman': 7, 'paint': 7, 'month': 7, 'titled': 7, 'leading': 6, 'Prince': 6, 'Asian': 6, 'race': 6, 'earth': 6, 'mayor': 6, 'build': 6, 'writer': 6, 'Nobel': 6, 'point': 6, 'stars': 6, '1991': 6, 'without': 6, 'follow': 6, 'went': 6, 'create': 6, 'right': 6, 'band': 6, 'believe': 6, 'events': 6, 'richest': 6, 'sleep': 6, 'Spielberg': 6, 'glass': 6, 'County': 6, 'police': 6, 'Italy': 6, 'From': 6, 'alphabet': 6, 'Law': 6, 'full': 6, 'government': 6, 'wear': 6, 'friend': 6, 'design': 6, 'eyes': 6, 'under': 6, 'tall': 6, 'Kid': 6, 'read': 6, 'ways': 6, 'Yankee': 6, 'types': 6, 'degrees': 6, 'expression': 6, 'de': 6, 'but': 6, '1963': 6, 'turned': 6, 'Night': 6, 'Tokyo': 6, 'Castle': 6, 'eggs': 6, 'small': 6, 'planet': 6, 'per': 6, 'brain': 6, 'buy': 6, 'tax': 6, 'Michael': 6, 'tennis': 6, '0': 6, 'inches': 6, 'eye': 6, 'Los': 6, 'Columbia': 6, '15': 6, 'bottle': 6, 'near': 6, 'claimed': 6, 'queen': 6, 'Ray': 6, 'African': 6, '12': 6, 'lake': 6, 'players': 6, 'border': 6, 'million': 6, 'salary': 6, 'equal': 6, 'automobile': 6, 'cards': 6, 'field': 6, 'bridge': 6, 'ten': 6, 'Super': 6, 'Spumante': 6, 'written': 6, 'schools': 6, 'program': 6, 'Mutombo': 6, 'around': 6, 'students': 6, 'Massachusetts': 6, 'husband': 6, 'own': 6, 'file': 6, 'sound': 6, 'gas': 6, 'historical': 6, 'featured': 6, 'hands': 6, 'China': 6, 'McCarren': 6, 'To': 6, 'Alaska': 6, 'living': 6, 'cancer': 6, 'whom': 6, 'nuclear': 6, 'taste': 6, 'White': 6, 'Games': 6, 'give': 6, 'fly': 6, 'painting': 6, 'Union': 6, 'thing': 6, 'formed': 6, 'mother': 6, 'India': 6, 'cover': 6, 'bone': 6, 'fame': 6, 'Golden': 6, 'diamond': 6, 'producer': 6, 'dead': 5, 'produces': 5, '000': 5, 'pregnancy': 5, 'birds': 5, 'Garry': 5, 'Kasparov': 5, 'Irish': 5, 'green': 5, '!': 5, 'State': 5, 'Spain': 5, 'using': 5, 'developed': 5, 'clouds': 5, 'always': 5, 'sing': 5, 'Atlantic': 5, 'Ocean': 5, 'large': 5, 'murder': 5, 'Louis': 5, 'Street': 5, 'keep': 5, 'spent': 5, 'comics': 5, 'cocaine': 5, 'these': 5, 'score': 5, 'himself': 5, 'Bear': 5, 'qigong': 5, 'elements': 5, 'itself': 5, 'family': 5, 'humans': 5, 'blue': 5, 'Harvey': 5, 'Vegas': 5, 'covers': 5, 'airplane': 5, 'court': 5, 'e': 5, 'members': 5, 'House': 5, 'official': 5, 'books': 5, 'correct': 5, 'roll': 5, 'twins': 5, 'companies': 5, 'Academy': 5, 'put': 5, 'town': 5, 'Islands': 5, 'Mrs.': 5, 'site': 5, 'road': 5, 'Muppets': 5, 'career': 5, 'Answers.com': 5, 'Asia': 5, 'effect': 5, 'often': 5, 'celebrated': 5, 'detective': 5, '11': 5, 'represented': 5, 'pitcher': 5, 'clean': 5, 'birthday': 5, 'Watergate': 5, 'strip': 5, 'followed': 5, 'Stuart': 5, 'Angeles': 5, 'comes': 5, 'foot': 5, 'Paul': 5, 'page': 5, 'Kevin': 5, 'Costner': 5, 'ages': 5, 'hero': 5, 'contain': 5, '24': 5, 'serve': 5, 'Titanic': 5, 'wings': 5, 'films': 5, 'season': 5, 'vice': 5, 'occur': 5, 'Jane': 5, 'declared': 5, 'must': 5, 'Wars': 5, 'weigh': 5, 'fruit': 5, 'convicted': 5, 'Islamic': 5, 'counterpart': 5, 'Cross': 5, 'League': 5, 'grow': 5, 'appearance': 5, 'Jude': 5, 'Wall': 5, 'Mr.': 5, 'seen': 5, 'weather': 5, 'growing': 5, 'told': 5, 'early': 5, 'languages': 5, 'literary': 5, 'Simpsons': 5, 'cigarette': 5, 'medium': 5, 'Hamblen': 5, 'profession': 5, 'reason': 5, 'great': 5, 'universe': 5, 'given': 5, 'cream': 5, 'Rogers': 5, 'Florida': 5, '1994': 5, 'Dikembe': 5, 'fish': 5, 'non': 5, 'Ireland': 5, 'attend': 5, 'complete': 5, 'engine': 5, 'close': 5, 'Franklin': 5, 'Roosevelt': 5, 'milk': 5, 'Diego': 5, 'set': 5, 'control': 5, 'caffeine': 5, 'Black': 5, 'Reims': 5, 'discovered': 5, 'turn': 5, 'Mozambique': 5, 'Hollywood': 5, 'Space': 5, 'pop': 5, 'visit': 5, 'Charlie': 5, 'purpose': 5, 'corpus': 5, 'continent': 5, 'Mary': 5, 'album': 5, 'Nicholas': 5, 'rule': 5, 'machines': 5, 'awarded': 5, 'Van': 5, 'Brown': 5, 'Microsoft': 5, 'order': 5, 'artist': 5, 'could': 5, 'upon': 5, 'days': 5, '7': 5, 'presidents': 5, 'signed': 5, 'kept': 5, 'You': 5, 'she': 5, 'along': 5, 'Lakes': 5, 'statistics': 5, 'Mother': 5, '1939': 5, 'religion': 5, 'assassinated': 5, 'winter': 5, 'Time': 5, 'Bond': 5, 'Thatcher': 5, 'elected': 5, 'help': 5, '21': 5, 'commonly': 5, 'original': 5, 'Caribbean': 5, 'Jewish': 5, 'same': 5, '..': 5, 'Miss': 5, 'auto': 5, 'Christian': 5, 'headquarters': 5, 'Procter': 5, 'Gamble': 5, 'D.C.': 5, 'pull': 5, 'Britain': 5, 'lose': 5, 'Captain': 5, 'Rome': 5, 'house': 5, 'substance': 5, 'transplant': 5, 'trial': 5, 'contact': 5, 'kids': 5, 'coffee': 4, 'surrounds': 4, 'cells': 4, 'Colin': 4, 'Powell': 4, 'Carolina': 4, 'asked': 4, 'join': 4, 'Good': 4, 'Stone': 4, 'classical': 4, 'All': 4, 'army': 4, 'manufactured': 4, 'composer': 4, 'Sullivan': 4, 'such': 4, 'Do': 4, 'submarine': 4, 'sperm': 4, 'Logan': 4, 'card': 4, 'distance': 4, 'potlatch': 4, 'half': 4, 'side': 4, 'oceans': 4, 'points': 4, 'meat': 4, 'owns': 4, 'weapon': 4, 'Italian': 4, 'restaurant': 4, 'Orange': 4, 'Maurizio': 4, 'Pellegrin': 4, 'plastic': 4, 'Ventura': 4, 'county': 4, 'amount': 4, 'meant': 4, 'market': 4, 'perfect': 4, 'exercise': 4, 'pregnant': 4, 'technique': 4, 'Define': 4, 'silly': 4, '1950': 4, 'civil': 4, 'stripes': 4, 'Lee': 4, 'Las': 4, 'writing': 4, 'federal': 4, 'brothers': 4, 'criminal': 4, 'sings': 4, 'stole': 4, 'cork': 4, 'Pittsburgh': 4, 'appeared': 4, 'borders': 4, 'answers': 4, 'questions': 4, 'bee': 4, 'sites': 4, 'Act': 4, 'unemployment': 4, 'December': 4, '1980': 4, 'islands': 4, 'Ruth': 4, 'equivalent': 4, 'southern': 4, 'tip': 4, 'cup': 4, 'ring': 4, 'Oscars': 4, 'latitude': 4, 'longitude': 4, 'screen': 4, 'remove': 4, 'future': 4, 'dictator': 4, 'outside': 4, 'contract': 4, \"n't\": 4, 'painter': 4, 'week': 4, 'Desmond': 4, 'No': 4, 'speed': 4, 'affect': 4, 'temperature': 4, 'door': 4, 'nation': 4, 'jockey': 4, 'happens': 4, 'occupy': 4, 'CD': 4, 'going': 4, 'Empire': 4, 'receive': 4, 'Windows': 4, '98': 4, 'directed': 4, 'typical': 4, 'away': 4, 'super': 4, 'working': 4, 'tried': 4, 'Einstein': 4, 'walk': 4, 'leave': 4, 'hour': 4, 'patent': 4, 'less': 4, 'rivers': 4, 'teaspoon': 4, 'matter': 4, 'Edgar': 4, 'element': 4, 'poem': 4, 'infectious': 4, 'led': 4, '1969': 4, '19': 4, 'Lincoln': 4, 'daughter': 4, 'spoken': 4, 'presidential': 4, 'third': 4, 'Confederate': 4, 'Cleveland': 4, 'sink': 4, 'drugs': 4, 'Bob': 4, 'style': 4, '1899': 4, '1940': 4, 'cat': 4, 'organ': 4, 'Williams': 4, 'terms': 4, 'DC': 4, 'Justice': 4, 'reach': 4, 'motion': 4, 'tiger': 4, 'Sinn': 4, 'Fein': 4, 'Alley': 4, 'paid': 4, 'trees': 4, 'clothing': 4, 'coach': 4, 'legal': 4, '4': 4, 'Oscar': 4, 'businesses': 4, 'boy': 4, 'eleven': 4, 'just': 4, 'function': 4, 'hockey': 4, 'Pollock': 4, 'chapter': 4, 'stock': 4, 'Way': 4, 'Johnson': 4, 'included': 4, 'Robert': 4, 'inspired': 4, 'night': 4, 'Jersey': 4, 'troops': 4, 'Pacific': 4, 'Superman': 4, 'Vatican': 4, 'thunder': 4, 'Lindbergh': 4, 'Bureau': 4, 'Investigation': 4, 'boxer': 4, 'think': 4, 'reign': 4, 'against': 4, 'party': 4, 'holds': 4, 'NFL': 4, '27': 4, 'cross': 4, 'heart': 4, '%': 4, 'any': 4, '2000': 4, 'Shirley': 4, 'measure': 4, '1942': 4, 'Cage': 4, 'DT': 4, 'email': 4, '1978': 4, 'dollar': 4, 'policy': 4, 'nations': 4, 'citizen': 4, 'Society': 4, 'cartoons': 4, 'snow': 4, 'painted': 4, 'drug': 4, 'motto': 4, 'Mexico': 4, 'worked': 4, 'banned': 4, 'Francisco': 4, 'magic': 4, 'operating': 4, 'IBM': 4, 'late': 4, 'natural': 4, 'sun': 4, 'enter': 4, 'originally': 4, 'Kuwait': 4, 'host': 4, 'Philippines': 4, 'Pulitzer': 4, 'short': 4, 'question': 4, 'Popeye': 4, 'astronauts': 4, 'opera': 4, 'Ben': 4, 'daily': 4, 'Long': 4, 'catch': 4, 'Nadia': 4, 'Comaneci': 4, 'Brothers': 4, 'Lake': 4, 'pounds': 4, 'Phoenix': 4, 'callosum': 4, 'Martin': 4, 'Luther': 4, 'wives': 4, 'Michelangelo': 4, 'acid': 4, 'off': 4, 'USA': 4, 'clock': 4, 'Everest': 4, 'calories': 4, 'Award': 4, 'Seven': 4, 'desert': 4, 'location': 4, '16th': 4, 'doing': 4, 'rum': 4, 'ads': 4, 'presidency': 4, 'publish': 4, 'fathom': 4, 'occupation': 4, 'Goodall': 4, 'Pennsylvania': 4, 'Fawaz': 4, 'Younis': 4, 'still': 4, 'unaccounted': 4, 'lyrics': 4, 'behind': 4, 'came': 4, 'better': 4, 'Winnie': 4, 'Pooh': 4, 'L.A.': 4, '25': 4, 'AIDS': 4, 'our': 4, 'mammal': 4, 'took': 4, 'Hearst': 4, 'single': 4, 'surface': 4, 'Gone': 4, 'Wind': 4, 'Company': 4, 'Mercury': 4, 'Victoria': 4, 'Blue': 4, 'Butler': 4, '1967': 4, 'received': 4, '1975': 4, 'tab': 4, 'winning': 4, 'Scotland': 4, 'songs': 4, 'beach': 4, 'Life': 4, 'K': 4, 'having': 4, 'Gate': 4, 'Ford': 4, 'back': 4, 'theory': 4, '13': 4, 'swimming': 4, 'discover': 4, 'medicine': 4, 'Edmund': 4, 'Russia': 4, 'prophet': 4, 'trials': 4, 'College': 4, 'direct': 4, 'firm': 4, 'phone': 4, 'cereal': 4, 'gives': 4, 'course': 4, 'diseases': 3, 'stopped': 3, 'bowling': 3, 'loss': 3, 'movement': 3, 'diameter': 3, 'Helen': 3, 'significant': 3, 'eruption': 3, 'determine': 3, 'appearances': 3, 'gods': 3, 'perpetual': 3, 'calendar': 3, 'birthstone': 3, 'want': 3, 'manned': 3, 'station': 3, 'glitters': 3, 'Taylor': 3, 'literature': 3, '1948': 3, 'Rotary': 3, 'engines': 3, 'infamous': 3, 'graced': 3, 'o': 3, 'owned': 3, 'landmark': 3, '1956': 3, 'sang': 3, 'marijuana': 3, 'Mediterranean': 3, 'method': 3, 'salt': 3, '1960': 3, 'Christ': 3, 'shuttle': 3, 'Master': 3, '...': 3, 'designer': 3, 'shoe': 3, 'About': 3, 'theme': 3, 'School': 3, 'monster': 3, 'education': 3, 'broken': 3, 'owner': 3, 'seized': 3, 'stamp': 3, 'Here': 3, 'Eternity': 3, 'seas': 3, 'dew': 3, '6th': 3, 'D': 3, 'chain': 3, 'inventor': 3, 'putty': 3, 'command': 3, 'Canyon': 3, 'Easter': 3, 'percent': 3, 'agent': 3, 'cable': 3, 'network': 3, 'root': 3, 'released': 3, 'Nazi': 3, 'sergeant': 3, 'VHS': 3, 'Oswald': 3, 'Samuel': 3, 'vaccine': 3, 'chicken': 3, 'software': 3, 'empire': 3, 'Hemingway': 3, 'maneuver': 3, 'peak': 3, 'animated': 3, 'Festival': 3, 'Michigan': 3, 'contemptible': 3, 'scoundrel': 3, 'lunch': 3, 'hot': 3, 'oven': 3, 'Peachy': 3, 'Oat': 3, 'Muffins': 3, 'Americas': 3, 'Stewart': 3, 'others': 3, 'Home': 3, 'figure': 3, 'feature': 3, 'ends': 3, 'Benny': 3, 'Carter': 3, 'supreme': 3, 'production': 3, 'ride': 3, 'Jamiroquai': 3, 'executed': 3, '1945': 3, 'effects': 3, 'organism': 3, 'toilet': 3, 'penned': 3, 'Babe': 3, 'opposite': 3, 'add': 3, 'Harry': 3, 'north': 3, 'present': 3, 'Jones': 3, '1999': 3, 'Paso': 3, 'arch': 3, 'designed': 3, 'screenplay': 3, 'training': 3, 'Conrad': 3, 'save': 3, 'chocolate': 3, '19th': 3, 'quit': 3, 'Live': 3, 'include': 3, 'secretary': 3, 'Arthur': 3, 'level': 3, 'generator': 3, 'draft': 3, 'steal': 3, 'next': 3, 'Ewoks': 3, 'important': 3, 'celebrities': 3, 'disc': 3, 'shape': 3, 'fuel': 3, 'Bay': 3, 'capita': 3, 'case': 3, 'Disabilities': 3, 'Nazis': 3, 'Harrison': 3, 'Boxing': 3, 'Romans': 3, 'Billy': 3, 'Foundation': 3, 'Cup': 3, 'movies': 3, 'hunter': 3, 'club': 3, 'royal': 3, 'Andrew': 3, 'forces': 3, 'swimmer': 3, 'uses': 3, 'F': 3, 'governor': 3, 'Web': 3, 'Joe': 3, 'chairman': 3, 'Senate': 3, 'bones': 3, 'Columbus': 3, 'adult': 3, 'caliente': 3, 'Gandhi': 3, 'properties': 3, 'doctor': 3, 'library': 3, 'Southern': 3, 'starts': 3, \"'re\": 3, 'Homelite': 3, 'Inc.': 3, 'Year': 3, 'nicknamed': 3, 'election': 3, 'perform': 3, 'toll': 3, 'Abraham': 3, 'novelist': 3, 'post': 3, 'region': 3, 'export': 3, 'fans': 3, '1981': 3, 'Windsor': 3, 'toy': 3, 'Wonder': 3, 'professional': 3, 'Sam': 3, 'Casablanca': 3, 'Tennessee': 3, 'may': 3, 'treat': 3, 'Georgia': 3, 'south': 3, 'bowl': 3, 'Arnold': 3, 'Institute': 3, 'Israel': 3, 'Baby': 3, 'trademark': 3, 'invent': 3, 'playing': 3, 'ask': 3, 'today': 3, 'Moore': 3, 'try': 3, 'assassinate': 3, 'It': 3, 'Grand': 3, 'populated': 3, '1953': 3, '17': 3, 'creature': 3, 'companion': 3, 'widely': 3, 'detect': 3, 'defects': 3, 'Saint': 3, 'houses': 3, 'Silent': 3, 'lady': 3, 'bug': 3, 'lack': 3, 'stage': 3, 'model': 3, 'acted': 3, 'mission': 3, 'vs.': 3, 'lowest': 3, 'Triple': 3, 'Crown': 3, 'bees': 3, 'horses': 3, 'era': 3, 'dry': 3, 'spend': 3, '1993': 3, 'Robin': 3, 'Jean': 3, 'Enterprise': 3, 'Roger': 3, 'range': 3, 'marks': 3, 'flows': 3, 'standard': 3, 'minutes': 3, 'camera': 3, 'Congress': 3, 'Vienna': 3, 'process': 3, 'outcome': 3, 'Yalta': 3, 'Conference': 3, 'Rascals': 3, 'promote': 3, 'Constitution': 3, 'sex': 3, 'exchange': 3, 'thank': 3, 'fare': 3, 'monarch': 3, 'display': 3, 'rules': 3, 'skin': 3, 'suit': 3, 'W.C.': 3, 'Fields': 3, 'yellow': 3, 'facial': 3, 'Sherlock': 3, 'Holmes': 3, 'began': 3, 'contest': 3, 'material': 3, 'verses': 3, 'launched': 3, 'spacecraft': 3, 'Come': 3, 'flavor': 3, 'witch': 3, 'Santa': 3, 'Shelley': 3, 'offer': 3, 'Nevada': 3, 'Security': 3, 'Council': 3, 'websites': 3, 'Maryland': 3, 'ago': 3, 'church': 3, 'legs': 3, 'return': 3, 'CPR': 3, 'project': 3, 'Indonesia': 3, 'blind': 3, 'Four': 3, 'chemicals': 3, 'Hawaiian': 3, 'nine': 3, 'direction': 3, 'pitchers': 3, 'toward': 3, 'drive': 3, 'D.': 3, ';': 3, 'god': 3, 'For': 3, 'apples': 3, '16': 3, 'Dakota': 3, 'tourist': 3, 'Playboy': 3, 'morning': 3, 'carries': 3, 'source': 3, 'Barbie': 3, 'likely': 3, 'send': 3, 'bibliography': 3, 'little': 3, '1935': 3, 'Darth': 3, 'Vader': 3, 'Fifth': 3, 'Amendment': 3, 'Radio': 3, 'vacuum': 3, 'opens': 3, 'MacLaine': 3, 'Angels': 3, 'keyboard': 3, 'label': 3, 'odds': 3, 'Hillary': 3, 'Rose': 3, 'Navy': 3, 'sides': 3, 'Travels': 3, 'lead': 3, 'takes': 3, 'done': 3, 'Pass': 3, 'struck': 3, 'society': 3, 'busiest': 3, 'stations': 3, 'Gene': 3, 'Venus': 3, '=': 3, 'mc2': 3, 'Albert': 3, 'Medal': 3, 'Larry': 3, 'job': 3, 'fired': 3, 'Maria': 3, 'position': 3, 'deadliest': 3, 'Days': 3, 'Steinbeck': 3, 'success': 3, 'hearing': 3, '9': 3, 'Jerry': 3, 'manufacturer': 3, 'meters': 3, 'Lord': 3, 'Panama': 3, 'birthplace': 3, 'Poe': 3, 'compatible': 3, 'IOC': 3, 'Whom': 3, 'science': 3, 'modern': 3, 'co': 3, 'founder': 3, 'Magna': 3, 'Carta': 3, 'races': 3, 'plants': 3, 'becoming': 3, 'championship': 3, 'soccer': 3, 'architect': 3, 'event': 3, 'Fitzgerald': 3, 'Stadium': 3, 'Angel': 3, 'ears': 3, 'sentence': 3, 'Hall': 3, 'Yankees': 3, 'intercourse': 3, 'Peace': 3, 'eight': 3, 'De': 3, 'Summer': 3, 'Me': 3, 'possession': 3, 'Beauty': 3, 'Dracula': 3, '1965': 3, 'Honecker': 3, 'gulf': 3, 'dancing': 3, 'mystery': 3, 'Degas': 3, 'Doodle': 3, 'stick': 3, 'hold': 3, 'rest': 3, 'forest': 3, 'Boy': 3, 'Beach': 3, 'Family': 3, 'Davies': 3, 'flying': 3, 'voice': 3, '1966': 3, 'railroad': 3, 'Hugo': 3, 'leg': 3, 'move': 3, 'ton': 3, 'L.': 3, 'J.': 3, '1968': 3, 'rainbow': 3, 'Center': 3, 'astronaut': 3, 'gay': 3, 'Central': 3, 'Salt': 3, 'buffalo': 3, 'Bolivia': 3, 'national': 3, 'electric': 3, 'Rolling': 3, 'Independence': 3, 'Court': 3, 'Moon': 3, 'sign': 3, 'Jr.': 3, 'Will': 3, 'remain': 3, 'Missouri': 3, 'describe': 3, 'Mark': 3, 'sensitive': 3, 'involved': 3, 'folic': 3, 'Drew': 3, 'Barrymore': 3, 'soda': 3, 'If': 3, 've': 3, 'tale': 3, 'Dwarfs': 3, 'Computer': 3, 'teams': 3, 'Story': 3, 'raise': 3, 'coming': 3, 'price': 3, 'Teddy': 3, 'bear': 3, 'Johnny': 3, 'cats': 3, 'flower': 3, 'happen': 3, 'ended': 3, 'Davis': 3, 'Madonna': 3, 'Rubber': 3, 'High': 3, 'deep': 3, '1974': 3, 'storm': 3, 'credit': 3, 'met': 3, 'planted': 3, 'safe': 3, 'deer': 3, 'meter': 3, 'votes': 3, 'Colorado': 3, 'wheel': 3, 'Rocky': 3, 'shows': 3, 'thalassemia': 3, 'p.m.': 3, 'vegetable': 3, 'clothes': 3, 'girls': 3, 'attractions': 3, 'Hass': 3, 'recorded': 3, '1923': 3, 'NHL': 3, 'research': 3, 'established': 3, 'starring': 3, 'Bastille': 3, 'fox': 3, 'northeast': 3, 'Pole': 3, 'typewriter': 3, 'Movie': 3, 'purchase': 3, 'insurance': 3, '1998': 3, 'crop': 3, 'Republic': 3, '1973': 3, 'seasons': 3, 'example': 3, 'Wolfe': 3, 'hours': 3, 'Tutu': 3, 'beat': 3, 'Camp': 3, 'Our': 3, 'fingers': 3, 'manufactures': 3, 'Derby': 3, 'target': 3, 'picture': 3, 'ceremony': 3, 'normal': 3, 'Dead': 3, 'autobiography': 3, 'Mexican': 3, 'Beatles': 3, 'reactivity': 3, 'La': 3, 'probability': 3, 'Korea': 3, 'Lion': 3, 'test': 3, 'August': 3, 'secret': 3, 'NBA': 3, 'Canadian': 3, 'tie': 3, 'vote': 3, '1992': 3, 'Finger': 3, 'September': 3, 'nature': 3, 'seller': 3, 'bought': 3, 'Sir': 3, 'Federal': 3, 'Broadway': 3, 'winner': 3, 'sold': 3, 'fiction': 3, 'soap': 3, 'running': 3, 'Poker': 3, 'cooking': 3, 'period': 3, 'double': 3, 'brush': 3, 'actually': 3, 'Tristar': 3, 'bar': 3, 'Fred': 3, 'July': 3, 'Gates': 3, 'Al': 3, 'free': 3, 'airline': 3, 'Emperor': 3, '14': 3, 'Yahoo': 3, 'failure': 3, 'military': 3, 'cans': 3, 'Scarlett': 3, 'Aspartame': 3, 'span': 3, 'info': 3, '1952': 3, 'sexual': 3, 'HIV': 3, 'Association': 3, 'pound': 3, 'idea': 3, 'Lawrence': 3, 'Malaysia': 3, 'Ileana': 3, 'Cotrubas': 3, 'groups': 2, 'lung': 2, 'Races': 2, 'Princess': 2, 'handheld': 2, 'neurological': 2, 'protein': 2, 'causing': 2, 'requirements': 2, 'strong': 2, 'box': 2, 'Gulliver': 2, 'goat': 2, 'hurricane': 2, 'dinosaur': 2, 'November': 2, 'Aldrin': 2, 'colonial': 2, 'rooftops': 2, 'Gilbert': 2, 'pseudonym': 2, 'Secretary': 2, 'administration': 2, 'Nostradamus': 2, 'elephant': 2, 'leads': 2, 'slave': 2, 'revolt': 2, 'steps': 2, 'appears': 2, 'digits': 2, 'Mouse': 2, 'ambassador': 2, 'Hungary': 2, 'volcano': 2, 'Incredible': 2, 'fusion': 2, 'Judy': 2, 'Garland': 2, 'split': 2, '1988': 2, 'ash': 2, 'underworld': 2, 'horsepower': 2, 'boosters': 2, 'decathlon': 2, 'chefs': 2, 'Spice': 2, 'junk': 2, 'snail': 2, 'spawned': 2, '28': 2, 'tenses': 2, 'Nothing': 2, 'map': 2, 'charge': 2, 'closest': 2, 'Yellow': 2, 'rocks': 2, 'cement': 2, 'wall': 2, 'employ': 2, 'cent': 2, 'Jefferson': 2, 'Greece': 2, 'Austria': 2, 'Switzerland': 2, 'Q': 2, 'gymnastics': 2, 'am': 2, 'Jake': 2, 'interview': 2, 'spelling': 2, 'violins': 2, 'cosmology': 2, 'pronounced': 2, 'helicopter': 2, 'Kings': 2, 'flies': 2, 'figures': 2, 'district': 2, 'Three': 2, 'orange': 2, 'benefits': 2, 'whisky': 2, 'keeps': 2, 'canal': 2, 'Ten': 2, 'Commandments': 2, 'Army': 2, 'bog': 2, 'format': 2, 'everything': 2, 'approximate': 2, 'Sydney': 2, 'Iron': 2, 'armor': 2, '49': 2, 'inventors': 2, 'leaders': 2, 'chancellor': 2, 'Ernest': 2, 'Andy': 2, 'feudal': 2, 'Stanford': 2, 'variety': 2, 'breeds': 2, 'flags': 2, 'Frankenstein': 2, 'arma': 2, 'ad': 2, 'addresses': 2, 'Democratic': 2, 'spoke': 2, 'Brazil': 2, 'explorer': 2, 'Hank': 2, 'Howard': 2, 'seaport': 2, 'claims': 2, 'Jordan': 2, 'sister': 2, 'aspartame': 2, 'fictional': 2, 'NJ': 2, 'c': 2, 'naval': 2, 'rotary': 2, 'silver': 2, 'bars': 2, 'linked': 2, 'Genesis': 2, 'classic': 2, '5th': 2, 'graders': 2, 'Rule': 2, 'Utah': 2, 'cranberry': 2, 'economic': 2, 'employee': 2, 'low': 2, 'concentration': 2, 'WWII': 2, 'drinking': 2, 'dropped': 2, 'candle': 2, 'blow': 2, 'triplets': 2, 'etc': 2, 'swap': 2, 'math': 2, 'fresh': 2, 'almost': 2, 'Mao': 2, 'zero': 2, 'faces': 2, 'Six': 2, 'cap': 2, 'emperor': 2, 'soldier': 2, 'Dow': 2, 'Awards': 2, 'pole': 2, 'Place': 2, 'Lite': 2, 'Maurice': 2, 'stains': 2, 'powerful': 2, 'felt': 2, 'Costa': 2, 'Rica': 2, 'Belushi': 2, 'Saturday': 2, 'housewife': 2, 'round': 2, 'table': 2, 'Public': 2, 'spins': 2, 'baking': 2, 'alternative': 2, 'measures': 2, 'Clinton': 2, 'Marcos': 2, 'Herb': 2, 'habitat': 2, 'PLO': 2, 'supposed': 2, 'surname': 2, 'terry': 2, 'details': 2, 'brought': 2, 'Zealand': 2, 'fight': 2, 'Club': 2, 'Winter': 2, 'Colombia': 2, 'lightning': 2, 'LSD': 2, 'nowadays': 2, '199': 2, 'creatures': 2, 'NNS': 2, 'tune': 2, 'too': 2, 'He': 2, 'trilogy': 2, 'Gang': 2, 'sees': 2, 'amateur': 2, 'constellation': 2, 'wood': 2, 'rubber': 2, 'suite': 2, 'penis': 2, 'database': 2, 'purposes': 2, 'park': 2, 'Falls': 2, 'Russians': 2, 'landed': 2, 'glove': 2, 'medals': 2, '1972': 2, 'Jackie': 2, 'Virginia': 2, 'hate': 2, 'broadcast': 2, 'da': 2, 'hitting': 2, 'Chappellet': 2, 'vineyard': 2, 'select': 2, 'bottom': 2, 'borrow': 2, 'teeth': 2, 'Lucy': 2, 'Peanut': 2, 'IQ': 2, 'Lloyd': 2, 'M.': 2, 'candy': 2, 'muscles': 2, 'airport': 2, 'Tammy': 2, 'Whitcomb': 2, 'Judson': 2, 'evil': 2, 'alphabetically': 2, 'synonymous': 2, 'Rice': 2, 'strontium': 2, 'purified': 2, 'lips': 2, 'hazmat': 2, 'kills': 2, 'worldwide': 2, 'Hastings': 2, '1977': 2, 'breed': 2, 'plane': 2, 'Wright': 2, 'flew': 2, 'highway': 2, '2th': 2, 'biography': 2, 'Victorian': 2, 'smoking': 2, 'colorful': 2, 'seventh': 2, 'cousins': 2, 'removed': 2, 'martinis': 2, '1920s': 2, 'rode': 2, 'condition': 2, 'likes': 2, 'attempt': 2, 'originated': 2, 'Helens': 2, 'foods': 2, 'social': 2, 'recipe': 2, 'maximum': 2, 'golfer': 2, 'truth': 2, 'museum': 2, 'havoc': 2, 'Town': 2, 'land': 2, 'Dr.': 2, 'begins': 2, 'Butterfield': 2, 'pizza': 2, 'calculator': 2, 'sizes': 2, 'wide': 2, 'Alexander': 2, 'duel': 2, 'Go': 2, 'expensive': 2, 'total': 2, 'bears': 2, 'jean': 2, 'satellites': 2, 'tournament': 2, 'journal': 2, 'camp': 2, 'legged': 2, 'study': 2, 'Massacre': 2, 'Melissa': 2, 'lengths': 2, 'dooby': 2, 'fifth': 2, 'dialing': 2, 'thought': 2, 'incident': 2, 'conference': 2, 'Khrushchev': 2, 'yet': 2, 'Love': 2, 'western': 2, 'Silver': 2, 'Valdez': 2, 'usually': 2, 'January': 2, 'cases': 2, 'III': 2, 'winners': 2, 'Cat': 2, 'formula': 2, 'Coca': 2, 'Cola': 2, 'Howdy': 2, 'Doody': 2, 'labor': 2, 'recommended': 2, 'Nino': 2, 'results': 2, 'temperatures': 2, 'Line': 2, 'object': 2, 'oriented': 2, 'betting': 2, 'racing': 2, 'empty': 2, 'Trek': 2, 'wake': 2, 'outer': 2, 'Clara': 2, 'past': 2, 'Temple': 2, 'energy': 2, 'revolutions': 2, 'shoot': 2, 'gender': 2, 'gallon': 2, 'spill': 2, 'Jules': 2, 'Verne': 2, 'scientists': 2, 'damage': 2, 'celebrity': 2, 'performer': 2, 'Mikhail': 2, 'Gorbachev': 2, 'clown': 2, 'Brunei': 2, 'dealt': 2, 'cathedral': 2, 'murdered': 2, 'Afghanistan': 2, 'Australian': 2, 'notes': 2, 'ticket': 2, 'Cairo': 2, 'Barbados': 2, 'Christopher': 2, 'Marlowe': 2, 'seizure': 2, 'jail': 2, 'Taiwanese': 2, 'correctly': 2, 'pronounce': 2, 'dentist': 2, 'Beers': 2, 'room': 2, 'private': 2, 'Is': 2, 'Give': 2, 'odors': 2, 'Chile': 2, 'community': 2, 'Ronald': 2, 'Reagan': 2, 'Woody': 2, 'Woodpecker': 2, 'nose': 2, 'Pit': 2, 'Pendulum': 2, 'cooler': 2, 'cucumber': 2, 'Romantic': 2, 'killer': 2, 'float': 2, 'flavors': 2, 'Roy': 2, 'Ice': 2, 'Rex': 2, 'Claus': 2, 'layers': 2, 'Sierra': 2, 'mountains': 2, '500': 2, 'Connecticut': 2, 'Anglican': 2, 'sort': 2, 'divided': 2, 'pairs': 2, 'lobster': 2, 'revolve': 2, 'dinner': 2, 'Fraze': 2, 'yards': 2, 'headed': 2, 'Thing': 2, 'Hungarian': 2, 'trip': 2, 'abbreviated': 2, 'support': 2, 'boxing': 2, 'faced': 2, 'bounty': 2, 'Eskimo': 2, 'front': 2, 'Museum': 2, 'conditioning': 2, 'heavyweight': 2, 'Bull': 2, 'syrup': 2, 'Lou': 2, 'Gehrig': 2, 'Ouija': 2, 'brown': 2, \"'ll\": 2, 'serving': 2, 'Englishman': 2, 'colonists': 2, 'Flying': 2, 'oz': 2, 'Hills': 2, 'attraction': 2, 'tourists': 2, 'political': 2, 'cologne': 2, 'associated': 2, 'Banana': 2, 'holidays': 2, 'Shevardnadze': 2, 'Native': 2, 'Kafka': 2, 'bra': 2, 'handle': 2, 'Truman': 2, 'separated': 2, 'silent': 2, 'Doonesbury': 2, 'epic': 2, 'Thanksgiving': 2, 'narrates': 2, 'Treasure': 2, 'rise': 2, 'Catholic': 2, 'David': 2, 'battery': 2, 'certain': 2, 'instead': 2, 'gallons': 2, 'since': 2, 'Rights': 2, 'independent': 2, 'Varian': 2, 'Associates': 2, 'sell': 2, 'division': 2, 'Seattle': 2, 'Needle': 2, 'Individuals': 2, 'Education': 2, 'let': 2, 'hat': 2, 'wears': 2, 'Josie': 2, 'Pussycats': 2, 'mouth': 2, 'armed': 2, 'jiggy': 2, 'getting': 2, 'Huston': 2, 'Humphrey': 2, 'Bogart': 2, 'graduate': 2, 'producing': 2, 'acronym': 2, 'cause': 2, 'monitor': 2, 'statement': 2, 'carbon': 2, 'compared': 2, 'Simpson': 2, 'First': 2, 'triangle': 2, 'Korean': 2, 'cotton': 2, 'importer': 2, 'available': 2, 'suffer': 2, 'Arctic': 2, 'development': 2, 'typist': 2, '100': 2, 'pages': 2, 'Medieval': 2, 'mentor': 2, 'Arcadia': 2, 'share': 2, 'ruled': 2, 'Amtrak': 2, 'admit': 2, 'wearing': 2, 'spread': 2, 'Walker': 2, 'beans': 2, 'forced': 2, 'circle': 2, 'vermouth': 2, 'geographical': 2, 'Charley': 2, 'alcoholic': 2, 'juice': 2, 'Ranger': 2, 'Yogi': 2, 'hard': 2, 'annual': 2, 'treatment': 2, 'fur': 2, 'communist': 2, 'Hudson': 2, 'Project': 2, 'Pan': 2, 'Bounty': 2, 'mutiny': 2, 'force': 2, 'afternoon': 2, 'Marley': 2, 'both': 2, 'advertising': 2, 'affected': 2, 'mentioned': 2, 'Monaco': 2, 'Alice': 2, 'Cooper': 2, 'Moscow': 2, 'Times': 2, 'Cuba': 2, 'Allen': 2, 'Review': 2, 'Wonders': 2, 'Warner': 2, 'Bros.': 2, 'Clark': 2, 'stocks': 2, 'hymn': 2, 'bestselling': 2, 'U.S.A': 2, 'heat': 2, 'Baltic': 2, 'costs': 2, 'prepared': 2, 'electricity': 2, 'diary': 2, 'programming': 2, '197': 2, 'invade': 2, 'journalist': 2, 'whose': 2, 'attack': 2, 'Motors': 2, 'Cathedral': 2, 'Gatsby': 2, 'shouldn': 2, 'Ella': 2, 'Gettysburg': 2, 'caught': 2, 'Danube': 2, 'street': 2, 'Ivy': 2, 'Palmer': 2, 'bay': 2, 'codes': 2, 'rings': 2, 'typing': 2, 'practice': 2, 'Now': 2, 'aid': 2, 'Fame': 2, 'Series': 2, '1962': 2, '1929': 2, 'moral': 2, 'Namath': 2, 'teenager': 2, 'Wyoming': 2, 'relative': 2, 'translated': 2, 'calcium': 2, '196': 2, 'lies': 2, 'Bulls': 2, '1990': 2, 'C': 2, 'Tracy': 2, 'Triangle': 2, 'Honor': 2, 'Paris': 2, 'sculpture': 2, 'Describe': 2, 'Commune': 2, '1936': 2, 'rain': 2, '1964': 2, 'von': 2, 'Scottish': 2, 'surrender': 2, 'Victor': 2, 'rear': 2, 'Michener': 2, 'primary': 2, 'bad': 2, 'luck': 2, 'province': 2, 'jogging': 2, 'Amazon': 2, 'Marx': 2, 'Kansas': 2, 'Railway': 2, 'Argentine': 2, 'release': 2, 'steel': 2, 'prayer': 2, 'device': 2, 'officer': 2, 'closing': 2, 'walls': 2, 'Pearl': 2, 'Harbor': 2, 'molecules': 2, 'sodium': 2, 'Western': 2, 'Alexandra': 2, 'graces': 2, 'Lebanon': 2, 'nests': 2, 'VBP': 2, 'floor': 2, 'across': 2, 'killing': 2, 'those': 2, 'helium': 2, 'zones': 2, 'coastline': 2, 'terrorist': 2, 'UN': 2, 'Stalin': 2, 'rank': 2, 'among': 2, 'learn': 2, 'penny': 2, 'Trivial': 2, 'Pursuit': 2, 'Hawaii': 2, 'Communist': 2, 'Party': 2, 'advertise': 2, 'pro': 2, 'U.K.': 2, 'Persian': 2, 'Gulf': 2, 'Side': 2, 'Lady': 2, 'lie': 2, 'kinds': 2, 'athlete': 2, 'putting': 2, 'Singing': 2, 'Nun': 2, 'prevent': 2, 'Stephen': 2, 'blacks': 2, 'giving': 2, 'seat': 2, 'Walter': 2, 'self': 2, 'household': 2, 'Laugh': 2, 'lice': 2, '15th': 2, 'colonies': 2, 'Bette': 2, 'requirement': 2, 'boys': 2, 'sweet': 2, 'Z': 2, 'Global': 2, 'pictures': 2, 'quadruplets': 2, 'donated': 2, 'toys': 2, 'measured': 2, 'Nintendo': 2, 'rent': 2, 'par': 2, '455': 2, 'yard': 2, 'wave': 2, 'commit': 2, 'Fort': 2, 'Knox': 2, 'gestation': 2, 'coconut': 2, 'pineapple': 2, 'credited': 2, 'saying': 2, 'creator': 2, 'stands': 2, 'phenomenon': 2, 'N': 2, 'iron': 2, 'virtual': 2, 'IP': 2, 'starting': 2, 'beginning': 2, 'lawyers': 2, 'Worlds': 2, 'electoral': 2, 'generals': 2, 'greeting': 2, 'Brave': 2, 'sank': 2, 'April': 2, '1989': 2, 'People': 2, 'change': 2, '1933': 2, 'Browns': 2, 'Spangled': 2, 'Banner': 2, 'fax': 2, 'pig': 2, 'Piggy': 2, 'bottles': 2, 'nautical': 2, 'ships': 2, 'subway': 2, 'anniversary': 2, 'Nations': 2, 'imaginary': 2, 'interesting': 2, 'identity': 2, 'Prewitt': 2, 'defined': 2, 'hooligans': 2, 'manager': 2, 'mascot': 2, 'buys': 2, 'tea': 2, 'maker': 2, 'alphabetical': 2, 'earn': 2, 'tourism': 2, 'joined': 2, 'tube': 2, 'Five': 2, 'draw': 2, 'visitors': 2, 'proposition': 2, 'manufacture': 2, 'prize': 2, 'Hermann': 2, 'consciousness': 2, '39': 2, 'Patricia': 2, 'kidnaped': 2, 'cut': 2, 'daminozide': 2, '1957': 2, 'Munich': 2, 'Marco': 2, 'introduce': 2, 'hydrogen': 2, 'published': 2, 'Andrews': 2, 'Delaware': 2, 'sons': 2, 'Nelson': 2, 'increase': 2, 'gain': 2, 'consist': 2, 'Century': 2, 'efficient': 2, 'barbeque': 2, 'ability': 2, 'issue': 2, '80': 2, '1940s': 2, 'vehicles': 2, 'voting': 2, 'regarding': 2, 'Bonnie': 2, 'broke': 2, 'Don': 2, 'Sigmund': 2, 'Freud': 2, 'really': 2, '900': 2, '740': 2, 'TREE': 2, 'Lewis': 2, 'Milky': 2, 'Arch': 2, 'Stevie': 2, 'twice': 2, 'ratified': 2, 'estate': 2, 'Mel': 2, 'Brooks': 2, 'nominated': 2, 'Mont': 2, 'Blanc': 2, 'R.E.M.': 2, 'related': 2, 'rows': 2, 'smallest': 2, 'announced': 2, 'Salonen': 2, '31': 2, 'uniforms': 2, 'kiss': 2, 'numeral': 2, 'Ohio': 2, 'cash': 2, 'corn': 2, 'Francis': 2, 'Stage': 2, 'Smokey': 2, 'steamboat': 2, 'Flintstones': 2, 'Scrooge': 2, 'partner': 2, 'Dickens': 2, 'Carol': 2, 'bombing': 2, 'dogs': 2, 'Bourbon': 2, 'restored': 2, 'Napoleon': 2, 'sauce': 2, 'coastal': 2, 'Dana': 2, 'Two': 2, 'Years': 2, 'abandoned': 2, 'Nero': 2, 'pressure': 2, 'equator': 2, 'Lives': 2, 'Muhammad': 2, 'Ali': 2, 'apartment': 2, 'shall': 2, 'sells': 2, 'plan': 2, 'Jonathan': 2, 'pink': 2, 'Nebraska': 2, 'dozen': 2, 'rating': 2, 'Gay': 2, 'orca': 2, 'fungal': 2, 'infection': 2, 'equation': 2, 'pyramid': 2, 'shampoo': 2, 'fun': 2, 'Steve': 2, 'Robinson': 2, 'Norway': 2, 'underwater': 2, 'Girl': 2, 'personality': 2, 'adopted': 2, 'camel': 2, 'image': 2, 'honorary': 2, 'dreams': 2, 'episode': 2, 'deals': 2, 'him': 2, 'flowers': 2, 'digital': 2, 'easiest': 2, 'grooves': 2, 'dime': 2, 'attempts': 2, 'class': 2, 'professor': 2, 'Randolph': 2, 'Indiglo': 2, 'Brazilian': 2, 'northernmost': 2, '1971': 2, 'stationed': 2, 'strips': 2, 'Joseph': 2, 'crystals': 2, 'exist': 2, 'glory': 2, 'architecture': 2, 'flourish': 2, 'fined': 2, 'Bush': 2, 'cheese': 2, 'might': 2, 'pilots': 2, 'hamburger': 2, '1853': 2, 'Forest': 2, 'Clay': 2, 'clip': 2, 'defeat': 2, 'chess': 2, 'pilot': 2, 'jersey': 2, 'east': 2, 'coast': 2, 'various': 2, 'Operation': 2, 'egg': 2, 'mayonnaise': 2, 'religions': 2, 'tail': 2, 'cocktail': 2, 'endangered': 2, 'heroine': 2, 'Scruples': 2, 'Viking': 2, 'marriage': 2, 'Trail': 2, 'Brandenburg': 2, 'gate': 2, 'erected': 2, 'daughters': 2, 'actors': 2, 'Gerald': 2, 'markets': 2, 'jack': 2, 'agency': 2, 'responsible': 2, 'copies': 2, '1958': 2, 'liberty': 2, 'dates': 2, 'U.S.A.': 2, 'Cool': 2, 'holiday': 2, 'Pride': 2, 'chip': 2, 'value': 2, 'ranch': 2, 'talking': 2, 'background': 2, 'stereo': 2, 'Hoffman': 2, 'pencil': 2, 'J.R.R.': 2, 'Tolkien': 2, 'central': 2, 'Taj': 2, 'Buffalo': 2, 'sundaes': 2, 'meeting': 2, 'artificial': 2, '1797': 2, '185': 2, 'poll': 2, 'holes': 2, 'T': 2, 'spaces': 2, 'mosquito': 2, 'couple': 2, 'divorce': 2, 'deodorant': 2, 'resistance': 2, 'bill': 2, 'Anthony': 2, 'worst': 2, 'fourth': 2, 'Hand': 2, 'singles': 2, 'ratio': 2, 'Vincent': 2, 'Gogh': 2, 'collection': 2, 'marrow': 2, 'Egypt': 2, 'areas': 2, 'breakfast': 2, 'search': 2, 'limit': 2, 'flow': 2, 'Olsen': 2, 'buildings': 2, 'Monet': 2, 'bond': 2, 'raid': 2, 'neon': 2, 'coal': 2, 'adventures': 2, 'objects': 2, '1946': 2, 'VIII': 2, 'Model': 2, 'Daniel': 2, 'doll': 2, 'offices': 2, '1982': 2, 'colleges': 2, 'toes': 2, 'opened': 2, 'heavily': 2, 'caffeinated': 2, 'soup': 2, 'Rhett': 2, 'leaving': 2, 'Hara': 2, 'themselves': 2, 'currency': 2, 'coup': 2, 'hotel': 2, 'burned': 2, 'Hawks': 2, 'brother': 2, 'Fatman': 2, 'eaten': 2, 'cabinet': 2, 'Avery': 2, 'monkey': 2, 'innings': 2, 'subtitled': 2, 'Government': 2, 'Dogtown': 2, 'Jolson': 2, 'singular': 2, 'Rubik': 2, 'Cube': 2, 'colt': 2, 'sailors': 2, 'signature': 2, 'Louie': 2, 'handed': 2, 'Nine': 2, 'softball': 2, 'dextropropoxyphen': 2, 'napsylate': 2, 'hardest': 2, 'Supreme': 2, 'provides': 2, 'coin': 2, 'deal': 2, 'Holy': 2, 'Archie': 2, 'Edinburgh': 2, 'ballet': 2, 'goes': 2, 'within': 2, 'Basketball': 2, 'superstar': 2, 'senator': 2, 'Disney': 2, 'service': 2, 'controls': 2, 'wise': 2, 'Hole': 2, 'territory': 2, 'opener': 2, 'ear': 2, 'hear': 2, 'Sun': 2, 'Last': 2, 'stretch': 2, 'lens': 2, 'Myrtle': 2, 'Rider': 2, 'occurs': 2, 'ripening': 2, 'trade': 2, 'Spider': 2, 'athletes': 2, 'fine': 2, 'Joan': 2, 'says': 2, 'don': 2, 'Pepsi': 2, 'Lucas': 2, 'prompted': 2, 'length': 2, 'broadcasting': 2, 'dam': 2, 'amendment': 2, 'Green': 2, 'There': 2, 'nothing': 2, 'February': 2, 'debate': 2, 'Gourd': 2, 'Trinidad': 2, 'including': 2, 'Louisiana': 2, 'register': 2, 'polio': 2, 'Choo': 2, 'fraction': 2, 'beaver': 2, 'native': 2, 'Simon': 2, 'Bakery': 2, 'shared': 2, 'ethnic': 2, 'Person': 2, 'combat': 2, 'dots': 2, 'surfing': 2, 'hermit': 2, 'crabs': 2, 'varieties': 2, 'printing': 2, 'advertises': 2, 'Bombay': 2, 'Cop': 2, 'T.S.': 2, 'postage': 2, 'Scopes': 2, 'Gore': 2, 'doubles': 2, 'Palace': 2, 'category': 2, 'specializing': 1, 'Either': 1, 'acidic': 1, 'Diana': 1, 'alley': 1, 'PC': 1, 'Beethoven': 1, 'chronic': 1, 'autoimmune': 1, 'attacks': 1, 'sheath': 1, 'nerve': 1, 'gradual': 1, 'Polish': 1, 'celestial': 1, '864': 1, 'heating': 1, 'Cracker': 1, 'tiny': 1, 'factors': 1, 'teen': 1, 'Spartanburg': 1, 'AFTRA': 1, 'Grange': 1, 'Witch': 1, 'Casper': 1, 'girlfriend': 1, 'osteichthyes': 1, 'usenet': 1, 'Grandma': 1, '86ed': 1, 'Hinduism': 1, 'remains': 1, 'expectancy': 1, 'Age': 1, 'bulbs': 1, 'Buzz': 1, 'permanent': 1, 'warned': 1, 'microwaves': 1, 'Phillip': 1, 'Kramer': 1, 'surrendered': 1, 'Saratoga': 1, 'lyricist': 1, 'Sutcliffe': 1, 'Bernoulli': 1, 'Principle': 1, 'airwaves': 1, 'pearls': 1, 'ya': 1, 'lo': 1, 'ove': 1, 'Get': 1, 'naked': 1, 'westernmost': 1, 'bubble': 1, 'wrap': 1, 'mayans': 1, 'Rolls': 1, 'Royce': 1, 'Mayo': 1, 'Clinic': 1, 'longer': 1, 'mouse': 1, 'shooting': 1, 'Styron': 1, 'preacher': 1, '168': 1, 'crown': 1, 'tap': 1, 'LAN': 1, 'activated': 1, 'hook': 1, 'HUB': 1, 'Melbourne': 1, 'Mighty': 1, 'action': 1, 'NECROSIS': 1, 'uprising': 1, 'active': 1, 'Creeps': 1, 'entering': 1, 'strait': 1, 'links': 1, 'Sheila': 1, 'Burnford': 1, 'Journey': 1, 'adults': 1, 'P-32': 1, 'bowler': 1, 'facing': 1, '37803': 1, 'pin': 1, 'JFK': 1, 'Backgammon': 1, 'Sarge': 1, 'Steel': 1, 'metal': 1, 'MORMONS': 1, 'kangaroo': 1, 'Natick': 1, 'distinct': 1, 'physical': 1, 'characterstics': 1, 'Arabian': 1, 'Rams': 1, '239': 1, '48th': 1, 'Lacan': 1, 'promises': 1, 'millions': 1, 'imitations': 1, 'jellies': 1, 'asthma': 1, 'Much': 1, 'Ado': 1, 'Keck': 1, 'telescope': 1, 'Durante': 1, 'orgin': 1, 'xoxoxox': 1, 'squats': 1, 'doubleheader': 1, 'plea': 1, 'Be': 1, 'True': 1, 'Your': 1, 'atlas': 1, 'Micronauts': 1, 'traveling': 1, 'Microverse': 1, 'attendant': 1, 'G2': 1, 'Spectrum': 1, 'Dwarf': 1, 'sharp': 1, 'embedded': 1, 'sailor': 1, 'Mouth': 1, 'Honda': 1, 'separates': 1, 'Naples': 1, 'Algiers': 1, 'Netherlands': 1, 'Maggio': 1, 'except': 1, 'repeats': 1, 'drain': 1, 'barroom': 1, 'judge': 1, 'Pecos': 1, 'brake': 1, 'recruitment': 1, 'punctuation': 1, 'drills': 1, 'grader': 1, 'literal': 1, 'DAY': 1, 'Stradivarius': 1, 'golden': 1, 'arches': 1, 'Biloxi': 1, 'Benelux': 1, 'Apache': 1, 'Wake': 1, 'represent': 1, '90': 1, 'stories': 1, 'contained': 1, 'Edith': 1, 'Wharton': 1, 'theatrical': 1, 'Roaring': 1, 'Forties': 1, 'mathematician': 1, 'Noble': 1, 'Literature': 1, 'Amazing': 1, 'masquerade': 1, 'Pythagoras': 1, '.com': 1, 'rowing': 1, 'machine': 1, 'bills': 1, 'entertainer': 1, 'dental': 1, 'gland': 1, 'regenerate': 1, 'battlefield': 1, 'Arabic': 1, 'Numerals': 1, 'Maid': 1, 'Rites': 1, 'Bend': 1, 'uniform': 1, 'competition': 1, 'bird': 1, 'N.M': 1, 'Porter': 1, 'Gift': 1, 'Magi': 1, 'Johnsons': 1, 'biographer': 1, 'pox': 1, 'populous': 1, '576': 1, 'offers': 1, 'CAD': 1, 'Jesus': 1, 'sisters': 1, 'Byzantine': 1, 'Swing': 1, 'Capp': 1, 'pub': 1, 'castle': 1, 'livestock': 1, 'themes': 1, 'Dawson': 1, 'Creek': 1, 'Felicity': 1, 'Huckleberry': 1, 'Finn': 1, 'snack': 1, 'ridges': 1, 'Heimlich': 1, 'Tulip': 1, 'wop': 1, 'cry': 1, 'Ad': 1, 'Representatives': 1, 'prankster': 1, 'waved': 1, 'train': 1, 'caboose': 1, 'tumbled': 1, 'marble': 1, 'Socratic': 1, 'attire': 1, 'pothooks': 1, 'cowboys': 1, 'Corner': 1, 'Drug': 1, 'Store': 1, 'Waco': 1, '1885': 1, 'Sao': 1, 'Paulo': 1, 'Scandinavian': 1, '173': 1, '732': 1, 'Freidreich': 1, 'Wilhelm': 1, 'Ludwig': 1, 'Leichhardt': 1, 'Prussian': 1, 'K.': 1, 'Smith': 1, 'Wines': 1, 'Barbara': 1, 'cullion': 1, 'skater': 1, 'synonym': 1, 'ruin': 1, 'G.M.T.': 1, 'melancholy': 1, 'Dane': 1, 'grass': 1, 'MSG': 1, 'bread': 1, 'stickers': 1, 'Cisco': 1, 'packages': 1, 'casinos': 1, 'Algeria': 1, 'tender': 1, 'resignation': 1, 'handicraft': 1, 'requires': 1, 'interlace': 1, 'warp': 1, 'weft': 1, 'permutations': 1, 'luxury': 1, 'liners': 1, 'Cap': 1, 'Trafalgar': 1, 'Carmania': 1, 'justice': 1, 'Shostakovich': 1, 'Rostropovich': 1, 'Independent': 1, 'silversmith': 1, 'account': 1, 'khaki': 1, 'chino': 1, 'showed': 1, 'fondness': 1, 'munching': 1, 'pollen': 1, 'watchers': 1, 'Report': 1, 'Eldercare': 1, 'Zorro': 1, 'laundry': 1, 'detergent': 1, 'piano': 1, 'Everyday': 1, 'midi': 1, 'Petersburg': 1, 'Petrograd': 1, 'Favorite': 1, 'Martian': 1, 'hives': 1, 'bogs': 1, 'impact': 1, 'economy': 1, 'improve': 1, 'morale': 1, 'Jews': 1, 'camps': 1, 'jeep': 1, 'collided': 1, 'truck': 1, 'short-': 1, 'underage': 1, '313': 1, 'mount': 1, 'carried': 1, 'multiple': 1, 'births': 1, 'frequent': 1, 'enemies': 1, 'Neither': 1, 'borrower': 1, 'nor': 1, 'lender': 1, 'Fiji': 1, 'picked': 1, 'stalled': 1, 'Wonderbra': 1, 'Fahrenheit': 1, 'centigrade': 1, 'Lime': 1, 'stat': 1, 'quickly': 1, 'seafaring': 1, 'Isle': 1, 'Illinois': 1, 'button': 1, 'gills': 1, 'Gordon': 1, 'recognition': 1, 'services': 1, 'quelling': 1, 'rebellions': 1, 'Herbert': 1, 'Hoover': 1, 'softest': 1, 'barbershop': 1, 'la': 1, 'Concorde': 1, 'burst': 1, 'commercials': 1, 'limelight': 1, 'dumb': 1, 'loveable': 1, 'Gosfield': 1, 'Phil': 1, 'Silvers': 1, 'chartered': 1, 'Vermont': 1, 'telephones': 1, 'easily': 1, 'shirts': 1, 'cheery': 1, 'fellow': 1, 'ZIP': 1, '9971': 1, 'Postal': 1, 'Service': 1, 'priest': 1, 'Marxism': 1, 'Brother': 1, 'watching': 1, 'Count': 1, 'Cinzano': 1, 'trader': 1, 'Heart': 1, 'Darkness': 1, 'kid': 1, 'earned': 1, 'Marquesas': 1, 'Archimedes': 1, 'bordering': 1, 'due': 1, 'Dan': 1, 'Aykroyd': 1, 'comediennes': 1, 'Nora': 1, 'Wiggins': 1, 'Eunice': 1, 'FBI': 1, 'Enemy': 1, 'magnet': 1, 'output': 1, 'narcolepsy': 1, 'useful': 1, 'avoid': 1, 'traffic': 1, 'cone': 1, 'Cawdor': 1, 'Glamis': 1, 'Blair': 1, 'railways': 1, 'Tootsie': 1, 'chickadee': 1, 'Philippine': 1, 'ex': 1, 'treasury': 1, 'executor': 1, 'stored': 1, 'Braun': 1, 'fences': 1, 'neighbors': 1, 'DSL': 1, 'chloroplasts': 1, 'cloth': 1, 'insanity': 1, 'serial': 1, 'tv': 1, 'alternate': 1, 'graphic': 1, 'Hamlet': 1, 'faring': 1, 'Inhumans': 1, 'historically': 1, 'biz': 1, 'Polynesian': 1, 'inhabit': 1, 'KnowPost.com': 1, 'shifting': 1, 'menace': 1, 'Rom': 1, 'airplanes': 1, 'Pheonix': 1, 'enclose': 1, 'Chesapeake': 1, 'hosted': 1, 'strikes': 1, 'Edessa': 1, 'replaced': 1, 'Canary': 1, 'IN': 1, 'sounded': 1, 'So': 1, 'Fine': 1, 'Chiffons': 1, 'www.questions.com': 1, 'open': 1, 'Punchbowl': 1, 'Hill': 1, 'mare': 1, 'nostrum': 1, 'sci': 1, 'fi': 1, 'Second': 1, 'Dicken': 1, 'Gas': 1, 'NASA': 1, 'net': 1, 'champions': 1, 'Uber': 1, 'videotape': 1, 'Folies': 1, 'Bergeres': 1, 'represents': 1, 'shield': 1, 'boat': 1, 'gopher': 1, 'tout': 1, 'chances': 1, 'pregnacy': 1, 'penetrate': 1, 'vagina': 1, '95': 1, 'idealab': 1, 'waste': 1, 'dairy': 1, 'cow': 1, 'cache': 1, 'contibution': 1, 'wedding': 1, 'Fergie': 1, 'commanders': 1, 'Alamein': 1, 'Firehole': 1, 'Fairy': 1, 'bathroom': 1, 'costume': 1, 'decided': 1, 'buses': 1, 'wallpaper': 1, 'fireplug': 1, 'hourly': 1, 'workers': 1, 'deranged': 1, 'Otto': 1, 'Octavius': 1, 'SHIELD': 1, 'fossilizes': 1, 'coprolite': 1, 'Gotham': 1, 'Wide': 1, 'WWW': 1, 'troilism': 1, 'Battery': 1, 'chairbound': 1, 'basophobic': 1, 'Freedy': 1, 'Johnston': 1, 'dingoes': 1, '155': 1, 'Leonardo': 1, 'Vinci': 1, 'Michaelangelo': 1, 'Machiavelli': 1, 'DiMaggio': 1, 'compile': 1, '56': 1, 'streak': 1, 'Stefan': 1, 'Edberg': 1, 'acreage': 1, 'committee': 1, 'Antonio': 1, 'ducats': 1, 'nutrients': 1, 'healthy': 1, 'Linus': 1, 'explorers': 1, 'Bucher': 1, 'hates': 1, 'mankind': 1, 'Burma': 1, 'epicenter': 1, 'cane': 1, 'translation': 1, 'Rupee': 1, 'Depreciates': 1, 'debts': 1, 'Qintex': 1, 'Linux': 1, 'Caroll': 1, 'Baker': 1, 'Grimes': 1, 'Debbie': 1, 'Reynolds': 1, 'erupts': 1, 'Yellowstone': 1, 'Craig': 1, 'Stevens': 1, 'fastener': 1, '1893': 1, 'lifelong': 1, 'snowboard': 1, '200': 1, 'Lucelly': 1, 'Garcia': 1, 'Honduras': 1, 'limited': 1, 'partnership': 1, 'footwear': 1, 'care': 1, 'Burroughs': 1, 'wet': 1, 'warm': 1, 'desire': 1, 'adorns': 1, 'Rwanda': 1, 'windmills': 1, 'concerts': 1, 'fowl': 1, 'grabs': 1, 'spotlight': 1, 'Monkey': 1, 'Normans': 1, 'victory': 1, 'beating': 1, 'task': 1, 'Bouvier': 1, 'propellers': 1, 'helped': 1, 'Untouchables': 1, 'involves': 1, 'Hernando': 1, 'Soto': 1, 'Mars': 1, 'relevant': 1, '118': 1, '126': 1, '134': 1, '142': 1, '158': 1, '167': 1, '177': 1, 'Grinch': 1, 'Stole': 1, 'volume': 1, 'lingo': 1, 'multiplexer': 1, 'pony': 1, 'problem': 1, '1997': 1, 'Was': 1, 'Teenage': 1, 'Werewolf': 1, 'Alvin': 1, 'Stardust': 1, 'legend': 1, 'Amazons': 1, 'Chomper': 1, 'Candice': 1, 'Bergen': 1, 'Jacqueline': 1, 'Bisset': 1, 'remake': 1, '1943': 1, 'Acquaintance': 1, 'ill': 1, 'fated': 1, 'craft': 1, 'captained': 1, 'Ernst': 1, 'Lehmann': 1, 'southeast': 1, 'Wang': 1, 'joining': 1, 'Ping': 1, 'Tak': 1, 'landlocked': 1, 'olives': 1, 'Tony': 1, 'hypertension': 1, 'press': 1, 'dismissed': 1, 'burglary': 1, 'organized': 1, 'veterans': 1, 'Pulaski': 1, '1866': 1, 'Sussex': 1, 'Drive': 1, 'Ottawa': 1, 'Orly': 1, 'deadly': 1, 'sins': 1, 'Remembrance': 1, 'Eggs': 1, 'Benedict': 1, 'Filenes': 1, 'Cid': 1, 'Low': 1, 'Countries': 1, 'clubs': 1, 'symbolize': 1, 'ethnological': 1, 'Barr': 1, 'representative': 1, 'wreaked': 1, 'marching': 1, 'Hallie': 1, 'Woods': 1, 'Sharon': 1, 'Schwarzenegger': 1, 'Smithsonian': 1, 'Le': 1, 'Carre': 1, 'Small': 1, 'flea': 1, 'studios': 1, 'Europeans': 1, 'Oceania': 1, 'Narragansett': 1, 'tribes': 1, 'Rhode': 1, 'turning': 1, 'Gaza': 1, 'Strip': 1, 'Jericho': 1, 'vernal': 1, 'equinox': 1, 'scientific': 1, 'commercially': 1, 'Bayer': 1, 'A.G.': 1, 'Leverkusen': 1, 'pointed': 1, 'handwriting': 1, 'analyst': 1, 'Beanie': 1, 'Bella': 1, 'Abzug': 1, 'sartorial': 1, 'Cartier': 1, 'aviator': 1, 'Santos': 1, 'Dumont': 1, 'Betty': 1, 'Boop': 1, 'cactus': 1, 'Hamilton': 1, 'Wanna': 1, 'Riots': 1, 'Rockefeller': 1, 'JDR3': 1, 'scratch': 1, 'fever': 1, 'stranger': 1, 'conifer': 1, 'transplants': 1, 'Goldilocks': 1, 'Sara': 1, 'Jackal': 1, 'devoured': 1, 'mob': 1, 'starving': 1, 'destructor': 1, 'destroying': 1, 'creativity': 1, 'inkhorn': 1, 'Slam': 1, 'wasn': 1, 'review': 1, 'Nightmare': 1, 'Elm': 1, 'servers': 1, 'Approximately': 1, 'ficus': 1, 'commandant': 1, 'POW': 1, 'Stalag': 1, 'Cornell': 1, 'Lai': 1, 'parasites': 1, 'apartments': 1, 'Brunswick': 1, 'pearl': 1, 'necklaces': 1, 'badly': 1, 'tarnished': 1, 'brass': 1, 'haunted': 1, 'Sinatra': 1, 'loosely': 1, 'classification': 1, '22': 1, '287': 1, 'shoplifts': 1, \"'d\": 1, 'lovely': 1, 'canning': 1, 'summit': 1, 'Eisenhower': 1, 'ecological': 1, 'niche': 1, '191': 1, 'Joel': 1, 'SED': 1, 'Castor': 1, 'Pollux': 1, 'match': 1, 'security': 1, 'infatuation': 1, 'Look': 1, 'viewing': 1, 'Ursula': 1, 'Andress': 1, 'Marbella': 1, 'Fox': 1, 'Principles': 1, 'supports': 1, 'Badaling': 1, 'turret': 1, 'Spade': 1, 'Lawn': 1, 'Tennis': 1, 'Challenge': 1, 'Trophy': 1, 'friction': 1, 'Magoo': 1, 'flog': 1, 'Electric': 1, 'jury': 1, 'Nike': 1, 'intranet': 1, 'newsmen': 1, 'Rhodes': 1, 'scholar': 1, 'rites': 1, 'accompanying': 1, 'circumcision': 1, 'newly': 1, 'Judaism': 1, 'airliners': 1, 'crash': 1, 'gliding': 1, 'elevation': 1, 'Sheboygan': 1, 'Jellicle': 1, 'freckles': 1, 'Zenger': 1, 'deveopment': 1, 'parking': 1, 'lot': 1, 'Madeira': 1, 'polo': 1, 'Paleozoic': 1, 'cooling': 1, 'very': 1, 'Manchester': 1, 'Oop': 1, 'Moo': 1, 'panel': 1, 'Adventures': 1, 'Hood': 1, 'Nicolet': 1, 'Yukon': 1, 'Scarlet': 1, 'Letter': 1, 'Chablis': 1, 'gambling': 1, 'conjugations': 1, 'woke': 1, 'emblazoned': 1, 'Jolly': 1, 'Peller': 1, 'Wendy': 1, 'beef': 1, 'miniature': 1, 'Karnak': 1, 'Bang': 1, 'LP': 1, 'Selleck': 1, 'frames': 1, 'disk': 1, 'bigger': 1, 'thighs': 1, 'Denver': 1, 'establish': 1, 'rust': 1, 'donation': 1, 'entail': 1, 'lions': 1, 'Jungle': 1, 'captive': 1, 'Nautilus': 1, 'predict': 1, 'observing': 1, 'Leave': 1, 'Beaver': 1, 'syringe': 1, 'medicinal': 1, 'bureaucracy': 1, 'plagued': 1, 'paleontologist': 1, 'Run': 1, 'Daylight': 1, 'fountain': 1, 'erupt': 1, 'bank': 1, '7847': 1, '+': 1, '5943': 1, 'Nitrox': 1, 'diving': 1, 'repossession': 1, 'unfamiliar': 1, 'spokespeople': 1, 'clause': 1, 'changed': 1, 'altered': 1, 'amended': 1, 'foot-9': 1, 'initial': 1, 'insisted': 1, 'Clarabell': 1, 'Gin': 1, 'Rummy': 1, 'lakes': 1, 'Becket': 1, 'spouting': 1, 'perfectly': 1, 'natives': 1, 'anus': 1, 'rectum': 1, 'constitutes': 1, 'Etta': 1, 'Butch': 1, 'Cassidey': 1, 'Sundance': 1, 'contributions': 1, 'imposed': 1, 'Blockade': 1, '600': 1, '387': 1, 'achievements': 1, 'Wolfman': 1, 'Quetzalcoatl': 1, 'whole': 1, 'lap': 1, 'P.T.': 1, 'Barnum': 1, 'Thumb': 1, 'sit': 1, 'pyrotechnic': 1, 'murdering': 1, 'whores': 1, 'shan': 1, 'ripping': 1, 'ultraviolet': 1, 'DNA': 1, 'ranking': 1, 'Philip': 1, 'Make': 1, 'peacocks': 1, 'mate': 1, 'adventuring': 1, 'Rann': 1, 'Adam': 1, 'Strange': 1, 'predicted': 1, 'topple': 1, '2010': 1, '2020': 1, 'cd': 1, 'Eminem': 1, 'slim': 1, 'shady': 1, 'oftentimes': 1, 'dropping': 1, 'ancients': 1, 'hiding': 1, 'scars': 1, 'archenemy': 1, 'tower': 1, 'breweries': 1, 'amendements': 1, 'passed': 1, 'distinctive': 1, 'palmiped': 1, 'amphibians': 1, '192': 1, 'extant': 1, 'joke': 1, 'Semper': 1, 'Fidelis': 1, 'celtic': 1, 'sum': 1, 'genetic': 1, 'tackle': 1, 'Eagles': 1, 'XV': 1, 'biblical': 1, 'stones': 1, 'Englishwoman': 1, 'freed': 1, 'slaves': 1, '1847': 1, 'industrialized': 1, 'digitalis': 1, '1830': 1, 'rocket': 1, 'Surveyor': 1, 'Anka': 1, 'articles': 1, 'tokens': 1, 'boats': 1, 'Virtual': 1, 'Desk': 1, 'Reference': 1, 'urged': 1, 'tragic': 1, 'Hockey': 1, 'Stevenson': 1, 'Deacon': 1, 'Brodie': 1, 'cabinetmaker': 1, 'burglar': 1, 'toothbrush': 1, 'Tyrannosaurus': 1, 'hazel': 1, 'dressed': 1, 'affair': 1, 'Winters': 1, 'Baskin': 1, 'Robbins': 1, 'milligrams': 1, 'gram': 1, 'cyclone': 1, '8/28/1941': 1, 'Yoo': 1, 'Hoo': 1, 'settle': 1, 'airforce': 1, 'gallery': 1, 'bith': 1, 'batteries': 1, 'knight': 1, 'Motto': 1, 'AD': 1, '999': 1, 'celebrations': 1, 'fears': 1, 'Ethel': 1, 'weird': 1, 'knife': 1, 'mechanical': 1, 'achieves': 1, 'speeds': 1, 'Dixville': 1, 'Notch': 1, 'SVHS': 1, 'purple': 1, 'Ermal': 1, 'thousands': 1, 'Barton': 1, 'Blackhawk': 1, '1832': 1, 'loud': 1, 'Cabarnet': 1, 'Sauvignon': 1, 'pushed': 1, 'coupled': 1, 'hump': 1, 'megawatts': 1, 'consortium': 1, 'Mission': 1, 'Energy': 1, 'Lauren': 1, 'Bacall': 1, 'sculptress': 1, 'Fantastic': 1, 'Lund': 1, 'skunks': 1, 'MiG': 1, 'lethal': 1, 'injection': 1, 'Dubliners': 1, 'homes': 1, 'juices': 1, 'Punch': 1, 'Chivington': 1, 'Ivan': 1, 'IV': 1, 'expansion': 1, 'scar': 1, 'pitch': 1, 'define': 1, 'prison': 1, 'Ossining': 1, 'Inuit': 1, 'cystic': 1, 'fibrosis': 1, 'hurdle': 1, 'runner': 1, 'steeplechase': 1, 'jump': 1, 'Java': 1, 'Edison': 1, 'BTU': 1, 'operant': 1, 'Wild': 1, 'Pampas': 1, 'cough': 1, 'medication': 1, 'Grammys': 1, 'consecutive': 1, 'Naseem': 1, 'Hamed': 1, 'Queensland': 1, 'Competition': 1, 'Policy': 1, 'frequency': 1, 'VHF': 1, 'amicable': 1, 'Poconos': 1, 'mevacor': 1, 'cousin': 1, 'Theodore': 1, 'Boards': 1, 'combatting': 1, 'sullen': 1, 'untamed': 1, 'elk': 1, 'sinning': 1, 'remote': 1, 'hive': 1, 'bloodhound': 1, 'Hawkins': 1, '1562': 1, 'feminist': 1, 'Sexual': 1, 'Politics': 1, 'quart': 1, 'commanded': 1, 'Orleans': 1, 'milliseconds': 1, 'Hot': 1, 'Wheels': 1, 'Protestant': 1, 'supremacy': 1, 'attracts': 1, 'captain': 1, '1922': 1, '.tbk': 1, 'touchdowns': 1, 'Langston': 1, 'Hughes': 1, 'sung': 1, 'Pajamas': 1, 'continuing': 1, 'dialog': 1, 'contemporary': 1, 'issues': 1, 'readers': 1, 'observances': 1, '1603': 1, 'Metamorphosis': 1, 'awakes': 1, 'escape': 1, 'gravity': 1, 'hungry': 1, 'majority': 1, 'whip': 1, 'MLA': 1, 'bibliographies': 1, 'Ozymandias': 1, 'Bering': 1, 'Strait': 1, 'sonnets': 1, 'composition': 1, 'version': 1, '1849': 1, 'Sutter': 1, 'Mill': 1, 'Hibernia': 1, 'sided': 1, 'werewolf': 1, 'scale': 1, 'earthquakes': 1, 'Homerian': 1, 'chronicles': 1, 'Trojan': 1, 'march': 1, 'Macy': 1, 'Parade': 1, '1812': 1, 'By': 1, 'relationship': 1, 'Church': 1, 'bingo': 1, 'transcript': 1, 'Letterman': 1, 'CTBT': 1, 'coppertop': 1, 'cured': 1, 'cumin': 1, 'Antigua': 1, 'Sunday': 1, 'determined': 1, 'flyer': 1, 'mistakenly': 1, 'Niagra': 1, 'platinum': 1, 'cardinal': 1, 'incubate': 1, '2.5': 1, 'billion': 1, 'PSI': 1, 'Marilyn': 1, 'Monroe': 1, 'silversmiths': 1, 'inaugurated': 1, 'BOC': 1, 'assassin': 1, 'tumbling': 1, 'aborigines': 1, 'prostitute': 1, 'Lemmon': 1, 'pimp': 1, 'pedometer': 1, 'volley': 1, 'popcorn': 1, 'GUM': 1, 'Philadelphia': 1, 'Cleopatra': 1, 'Jay': 1, 'Kay': 1, 'magnets': 1, 'attract': 1, 'vocals': 1, 'SysRq': 1, 'key': 1, '1642': 1, '1649': 1, 'phalanx': 1, 'Furth': 1, 'Rednitz': 1, 'Pegnitz': 1, 'converge': 1, 'habeas': 1, 'canker': 1, 'sores': 1, 'wars': 1, 'conflicts': 1, 'upper': 1, 'corner': 1, 'Budweiser': 1, 'verdandi': 1, 'paying': 1, 'roulette': 1, 'espionage': 1, 'reunited': 1, 'Maltese': 1, 'Falconers': 1, 'Astor': 1, 'Sidney': 1, 'Greenstreet': 1, 'Mae': 1, 'battleship': 1, 'veal': 1, 'roasts': 1, 'chops': 1, 'stem': 1, 'SVGA': 1, 'adapter': 1, 'Wee': 1, 'Willie': 1, 'Winkie': 1, 'reptiles': 1, 'larger': 1, 'Statue': 1, 'Liberty': 1, 'Baretta': 1, 'cockatoo': 1, 'Compaq': 1, 'stronger': 1, 'vitreous': 1, 'Technology': 1, 'cellulose': 1, 'opposition': 1, 'Konrad': 1, 'Adenauer': 1, 'Chancellor': 1, 'O.J.': 1, 'gunboat': 1, 'Sand': 1, 'Pebbles': 1, 'Planet': 1, 'Iran': 1, 'Contra': 1, 'investigation': 1, 'Chilly': 1, 'Willy': 1, 'scalene': 1, 'Budapest': 1, 'Belgrade': 1, 'caldera': 1, 'Kimpo': 1, 'leukemia': 1, 'witness': 1, 'hearings': 1, 'textiles': 1, 'overcome': 1, 'giants': 1, 'twelve': 1, 'Superbowls': 1, 'ers': 1, 'whatever': 1, 'catalogues': 1, 'topophobic': 1, 'deepest': 1, 'Sleepless': 1, 'Benjamin': 1, 'Shalom': 1, 'transport': 1, 'files': 1, 'primate': 1, 'pigment': 1, 'palms': 1, 'Tesla': 1, 'protagonist': 1, 'Dostoevski': 1, 'Idiot': 1, 'Guild': 1, 'syllables': 1, 'hendecasyllabic': 1, 'poetry': 1, 'snoring': 1, 'Camptown': 1, 'Racetrack': 1, 'Gross': 1, 'spaceball': 1, 'Pia': 1, 'Zadora': 1, 'millionaire': 1, 'communications': 1, 'yachts': 1, 'Khyber': 1, 'elders': 1, 'feathered': 1, 'Yugoslavians': 1, 'Vlaja': 1, 'Gaja': 1, 'Raja': 1, 'rail': 1, 'Unsafe': 1, 'Any': 1, 'Speed': 1, 'Siskel': 1, 'flytrap': 1, 'snake': 1, 'EKG': 1, 'senses': 1, 'develops': 1, 'jeans': 1, 'Calvin': 1, 'Klein': 1, 'comfortable': 1, 'sideburns': 1, 'didn': 1, 'until': 1, 'industrial': 1, '26': 1, 'Kenyan': 1, 'safari': 1, 'vending': 1, 'Iberia': 1, 'Pilot': 1, 'spice': 1, 'pay': 1, 'Johnnie': 1, 'Label': 1, 'aged': 1, 'ancient': 1, 'refuse': 1, 'trading': 1, 'Arts': 1, 'dialogue': 1, 'Dudley': 1, 'Right': 1, 'micro': 1, 'mixing': 1, 'gin': 1, 'Ybarra': 1, 'council': 1, 'Montenegro': 1, 'Sally': 1, 'Dyke': 1, 'Hustle': 1, 'Internet2': 1, 'pomegranate': 1, 'owe': 1, 'fix': 1, 'squeaky': 1, 'floors': 1, 'woo': 1, 'Wu': 1, 'dialect': 1, 'waterways': 1, '1.76': 1, 'elect': 1, 'bell': 1, 'Mountain': 1, 'mass': 1, 'exposed': 1, 'granite': 1, 'Sunflowers': 1, 'despondent': 1, 'Freddie': 1, 'Prinze': 1, 'anymore': 1, 'Shawn': 1, 'retirement': 1, 'T.V.': 1, 'Terrific': 1, 'Adventours': 1, 'Tours': 1, 'FCC': 1, 'Newton': 1, 'Minow': 1, 'declare': 1, '1961': 1, 'Candlemas': 1, 'document': 1, 'copy': 1, 'placed': 1, 'burial': 1, 'boilermaker': 1, 'Pickering': 1, 'Nathan': 1, 'Hamill': 1, 'prequel': 1, 'nitrogen': 1, 'proud': 1, 'Volcano': 1, 'Sinemet': 1, 'Hub': 1, 'Plc': 1, 'Olympia': 1, 'overlook': 1, 'Rev.': 1, 'Falwell': 1, 'ancestral': 1, 'overlooking': 1, 'Hyde': 1, 'Manhatten': 1, 'crocodile': 1, 'swallow': 1, 'menstruation': 1, 'Waugh': 1, 'Handful': 1, 'Dust': 1, 'mad': 1, 'pianos': 1, 'motorcycles': 1, 'records': 1, 'rainfall': 1, 'Exxon': 1, 'polyorchid': 1, 'fat': 1, 'staff': 1, 'Prayer': 1, 'nitrates': 1, 'environment': 1, 'quilting': 1, 'chop': 1, 'suey': 1, 'Terrence': 1, 'Malick': 1, 'spy': 1, 'correspondent': 1, 'Reuter': 1, 'Manuel': 1, 'Noriega': 1, 'ousted': 1, 'authorities': 1, 'Universe': 1, 'conditions': 1, 'niece': 1, 'nephew': 1, 'extinct': 1, 'One': 1, 'pow': 1, 'kisser': 1, 'modestly': 1, 'Janelle': 1, 'Ouarterly': 1, 'Doublespeak': 1, 'inoperative': 1, 'Friz': 1, 'Freleng': 1, 'ranks': 1, 'dioxide': 1, 'Grapes': 1, 'Wrath': 1, 'dangling': 1, 'participle': 1, 'manufacturers': 1, 'Lights': 1, 'Bookshop': 1, 'sewer': 1, 'commissioner': 1, 'Provo': 1, 'components': 1, 'polyester': 1, 'premier': 1, 'insects': 1, 'spiracles': 1, 'Garmat': 1, 'unleashed': 1, 'Celestials': 1, 'extension': 1, '.dbf': 1, 'disposable': 1, 'razor': 1, 'cents': 1, 'estuary': 1, 'Continental': 1, 'Divide': 1, 'mustard': 1, 'hurley': 1, 'flogged': 1, 'Inga': 1, 'Nielsen': 1, 'Iraqi': 1, 'Prix': 1, 'driving': 1, 'ankle': 1, 'sprain': 1, 'goalie': 1, 'permitted': 1, '3rd': 1, 'falls': 1, 'asleep': 1, 'catsup': 1, 'talk': 1, 'lends': 1, 'sneezing': 1, 'laureate': 1, 'expelled': 1, 'Timor': 1, 'astronomer': 1, 'Caine': 1, 'Mutiny': 1, 'shine': 1, 'Meyer': 1, 'Wolfsheim': 1, 'fixed': 1, 'pH': 1, 'wick': 1, 'resurrectionist': 1, 'sharks': 1, 'stinger': 1, 'tweezers': 1, 'Believe': 1, 'Magic': 1, 'needs': 1, 'Galapagos': 1, 'belong': 1, 'bandleader': 1, 'cowrote': 1, 'A_Tisket': 1, 'Tasket': 1, 'haboob': 1, 'blows': 1, 'wrestling': 1, 'Hulk': 1, 'Urals': 1, 'diminutive': 1, 'gymnast': 1, 'stores': 1, 'nightclubs': 1, 'Sweet': 1, 'Pea': 1, 'garment': 1, 'Bradley': 1, 'Voorhees': 1, 'proliferation': 1, 'Tucson': 1, 'anything': 1, 'zip': 1, 'founders': 1, 'Artists': 1, 'Dumbo': 1, 'insured': 1, 'Heineken': 1, 'advertised': 1, 'multimedia': 1, 'Hardy': 1, 'raised': 1, 'ruckus': 1, 'Woman': 1, 'Pocahontas': 1, 'peace': 1, 'global': 1, 'depression': 1, 'Ghana': 1, 'bullheads': 1, 'WWI': 1, 'Doughboys': 1, 'unknown': 1, 'Aesop': 1, 'fable': 1, 'swift': 1, 'Slow': 1, 'steady': 1, 'bound': 1, 'Makepeace': 1, 'Thackeray': 1, 'Stanley': 1, 'Kubrick': 1, 'Tirana': 1, 'shoreline': 1, 'Badge': 1, 'Courage': 1, 'coastlines': 1, 'Biscay': 1, 'Aladdin': 1, 'Leo': 1, 'Tolstoy': 1, 'titanium': 1, 'Olivia': 1, 'Havilland': 1, 'penalty': 1, 'Avenue': 1, 'Reading': 1, 'Railroad': 1, 'touch': 1, 'Blatty': 1, 'recounts': 1, 'horrors': 1, 'Regan': 1, 'MacNeil': 1, 'devil': 1, 'beholder': 1, 'Yaroslavl': 1, 'destroyers': 1, 'Maddox': 1, 'Turner': 1, 'Joy': 1, 'Patti': 1, 'Page': 1, 'Bermuda': 1, 'headaches': 1, 'parachute': 1, 'Toulmin': 1, 'logic': 1, 'canine': 1, 'Pere': 1, 'Lachaise': 1, 'cemetery': 1, 'bronze': 1, 'Fourth': 1, 'Position': 1, 'Front': 1, 'LCD': 1, 'Yemen': 1, 'reunified': 1, 'why': 1, 'feather': 1, 'macaroni': 1, 'villi': 1, 'intestine': 1, 'tropical': 1, 'distributions': 1, 'bodies': 1, 'visited': 1, 'directly': 1, 'Detroit': 1, 'wonders': 1, 'V-8': 1, 'Juice': 1, 'slogan': 1, 'tastebud': 1, 'Kind': 1, 'virgin': 1, 'nun': 1, 'Oh': 1, 'Challengers': 1, 'Unknown': 1, 'exclusive': 1, 'Copacabana': 1, 'Ipanema': 1, 'Circus': 1, 'Billie': 1, 'Marion': 1, 'moxie': 1, 'chickenpoxs': 1, 'Mortgage': 1, 'Lifter': 1, 'ace': 1, 'Manfred': 1, 'Richthofen': 1, 'Louse': 1, 'seeking': 1, 'missile': 1, 'Sidewinder': 1, 'ejaculate': 1, 'operation': 1, 'Imperial': 1, 'Forces': 1, 'traversed': 1, 'Bellworts': 1, 'exile': 1, 'poop': 1, 'walking': 1, 'marked': 1, 'Redford': 1, 'directorial': 1, 'debut': 1, 'Corbett': 1, '1892': 1, 'Names': 1, 'A.': 1, 'locations': 1, 'calls': 1, 'pointsettia': 1, 'breaking': 1, 'mirror': 1, 'forfeited': 1, 'spacewalk': 1, 'Win': 1, 'Rah': 1, 'muscle': 1, 'provide': 1, 'intake': 1, 'Edmonton': 1, 'longtime': 1, 'several': 1, 'educational': 1, 'resources': 1, 'parents': 1, 'teachers': 1, 'serves': 1, 'BPH': 1, '280': 1, 'crew': 1, 'automation': 1, 'Thursday': 1, 'massage': 1, 'Sweden': 1, 'Finland': 1, 'convert': 1, 'revolutionary': 1, 'Castro': 1, '1979': 1, 'Tzimisce': 1, 'Blythe': 1, 'european': 1, 'tied': 1, 'ruble': 1, 'subaru': 1, 'bands': 1, 'instruments': 1, 'electronic': 1, 'visual': 1, 'displays': 1, 'corresponding': 1, 'signals': 1, 'Brian': 1, 'Boru': 1, '11th': 1, 'Usenet': 1, 'Britney': 1, 'Spears': 1, 'Marijuana': 1, 'Nipsy': 1, 'Russell': 1, 'Allan': 1, 'Somme': 1, 'Ayer': 1, 'cubic': 1, 'punishment': 1, 'Retrograde': 1, 'Writer': 1, 'Egyptians': 1, 'shave': 1, 'eyebrows': 1, 'entered': 1, 'meerkat': 1, 'Top': 1, 'neighborhood': 1, 'Plain': 1, 'Dealer': 1, 'required': 1, '1879': 1, '1880': 1, '1881': 1, 'Houdini': 1, 'attorney': 1, 'ordered': 1, 'Alcatraz': 1, 'exterminate': 1, 'attacked': 1, 'Douglas': 1, 'McArthur': 1, 'recalled': 1, 'Youngman': 1, 'condiment': 1, 'Dutch': 1, 'dip': 1, 'fries': 1, 'Scouts': 1, 'Woodward': 1, 'Brethren': 1, 'fluorine': 1, 'magnesium': 1, 'Hemisphere': 1, 'boycott': 1, 'Twain': 1, 'Forests': 1, 'Kosovo': 1, 'boiled': 1, 'Mormon': 1, 'Liverpool': 1, 'stringed': 1, 'fires': 1, 'bolt': 1, 'televised': 1, 'specimen': 1, 'basidiomycetes': 1, 'Barbary': 1, 'wasps': 1, 'conformist': 1, 'abstract': 1, 'Dripper': 1, 'elevators': 1, 'Building': 1, 'Antichrist': 1, 'dare': 1, 'knock': 1, 'shoulder': 1, 'clearer': 1, 'acting': 1, 'Lunt': 1, 'Fontanne': 1, 'reflectors': 1, 'washed': 1, 'vodka': 1, 'straight': 1, 'poker': 1, 'hence': 1, 'roosters': 1, 'punch': 1, 'drunk': 1, 'pugilist': 1, 'Cauliflower': 1, 'McPugg': 1, 'vegetation': 1, 'crabgrass': 1, 'advanced': 1, 'Vietnamese': 1, 'delegate': 1, 'quotes': 1, 'fatal': 1, 'anybody': 1, 'Sabrina': 1, 'rockin': 1, 'Vichy': 1, 'Betsy': 1, 'Ross': 1, 'Nile': 1, 'Turks': 1, 'Libya': 1, 'cockroaches': 1, 'shillings': 1, 'guinea': 1, 'itch': 1, 'poison': 1, 'ivy': 1, 'Square': 1, 'sourness': 1, 'slum': 1, 'Darius': 1, 'Mac': 1, 'Outstanding': 1, 'Committee': 1, 'fairy': 1, 'Snow': 1, 'farthings': 1, 'Death': 1, 'Salesman': 1, '1985': 1, 'bureau': 1, 'Cruise': 1, 'Kathie': 1, 'Gifford': 1, 'nanometer': 1, 'internal': 1, 'combustion': 1, 'lines': 1, 'footballs': 1, 'supplement': 1, '47': 1, 'Apple': 1, 'Wheatfield': 1, 'Crows': 1, 'Germanic': 1, 'Tell': 1, 'Mandrake': 1, 'Saudi': 1, 'Arabia': 1, 'Iraq': 1, 'Natalie': 1, 'Wood': 1, 'Audrey': 1, 'Hepburn': 1, 'Fair': 1, 'farther': 1, 'opposed': 1, 'further': 1, 'Arkansas': 1, 'doughnut': 1, 'Cambodia': 1, 'Aztec': 1, 'Ingmar': 1, 'Bergman': 1, 'Rain': 1, 'Blood': 1, 'Fire': 1, 'extinction': 1, 'graveyard': 1, 'pets': 1, 'Nicolo': 1, 'Paganini': 1, 'famously': 1, 'warn': 1, 'Wiz': 1, 'cult': 1, 'Marcus': 1, 'Garvey': 1, 'Rosa': 1, 'Parks': 1, 'Become': 1, 'refusing': 1, 'bus': 1, 'terror': 1, 'Horton': 1, 'Madre': 1, 'limits': 1, 'defense': 1, 'eliminates': 1, 'germs': 1, 'mold': 1, 'mildew': 1, 'garden': 1, 'kissed': 1, 'positions': 1, 'succession': 1, 'sub': 1, 'Saharan': 1, 'CCT': 1, 'diagram': 1, 'deserts': 1, 'Coney': 1, 'boardwalk': 1, 'deconstructionism': 1, 'protanopia': 1, 'Rowan': 1, 'Burr': 1, 'vocalist': 1, 'Hansel': 1, 'Gretel': 1, 'block': 1, 'Kingdom': 1, 'brimstone': 1, 'monk': 1, 'gained': 1, 'Florence': 1, 'burnt': 1, 'stake': 1, 'zoonose': 1, 'Revolution': 1, 'cheetahs': 1, 'creating': 1, 'scandal': 1, 'daring': 1, 'gown': 1, 'Neoclassical': 1, 'Romanticism': 1, 'watts': 1, 'kilowatt': 1, 'Dolly': 1, 'Parton': 1, 'rarely': 1, 'charcter': 1, 'chiefly': 1, 'enormous': 1, 'admitted': 1, 'Winslow': 1, 'Homer': 1, '1872': 1, 'Snap': 1, 'Whip': 1, 'thee': 1, 'complemented': 1, 'potatoes': 1, 'peas': 1, 'echidna': 1, 'Schoolhouse': 1, 'Future': 1, 'Shock': 1, 'freeway': 1, 'construction': 1, 'Capital': 1, 'chromosome': 1, 'cameras': 1, 'Prussia': 1, 'centers': 1, 'stolen': 1, 'treatments': 1, 'chance': 1, 'conceiving': 1, 'hundred': 1, 'billionth': 1, 'crayon': 1, 'Crayola': 1, 'Desert': 1, 'organs': 1, 'Fodor': 1, 'pies': 1, 'wound': 1, 'manufacturing': 1, 'throwing': 1, 'lottery': 1, 'Buffett': 1, 'concert': 1, 'Camden': 1, 'Milt': 1, 'Harper': 1, 'modem': 1, 'access': 1, '64': 1, 'Volkswagen': 1, 'portal': 1, 'CE': 1, 'particularly': 1, 'electrical': 1, 'purchased': 1, 'bails': 1, 'cricket': 1, 'wicket': 1, 'Doegs': 1, 'visiting': 1, 'Duvalier': 1, 'principles': 1, 'learning': 1, 'Research': 1, 'Learning': 1, 'IRL': 1, 'advertizing': 1, 'Frito': 1, 'Lay': 1, 'Moorish': 1, 'patents': 1, 'toast': 1, 'alloy': 1, 'copper': 1, 'tin': 1, 'piracy': 1, 'hostage': 1, 'taking': 1, 'Justin': 1, 'valuable': 1, 'here': 1, 'hijacking': 1, 'sword': 1, 'Corporal': 1, 'cunnilingus': 1, 'schematics': 1, 'windshield': 1, 'wiper': 1, 'mechanism': 1, 'shrubs': 1, 'Bradbury': 1, 'Chronicles': 1, 'pollution': 1, 'Bert': 1, 'Loomis': 1, 'Hazmat': 1, 'astronomical': 1, 'Jan.': 1, 'Quebec': 1, 'alternator': 1, 'Wayne': 1, 'Gretzky': 1, 'jerk': 1, 'Fray': 1, 'Bentos': 1, 'Mackinaw': 1, 'est': 1, 'ce': 1, 'pas': 1, 'turnkey': 1, 'cullions': 1, 'chromatology': 1, 'Guadalcanal': 1, 'wheat': 1, 'silk': 1, 'screening': 1, 'radioactive': 1, 'Nevil': 1, 'Shute': 1, 'doomed': 1, 'survivors': 1, 'abbreviate': 1, 'cc': 1, 'Miller': 1, 'stuck': 1, 'tongue': 1, 'friendly': 1, 'Teflon': 1, 'pan': 1, 'inducted': 1, 'Swimming': 1, 'routinely': 1, 'dem': 1, 'bums': 1, 'frustrated': 1, 'e.g.': 1, 'tire': 1, 'spin': 1, 'slows': 1, 'Dennis': 1, 'Menace': 1, 'stripe': 1, 'Coho': 1, 'salmon': 1, 'powered': 1, 'Norwegian': 1, 'M': 1, 'calculate': 1, 'enthalpy': 1, 'reaction': 1, 'snakes': 1, 'rugs': 1, 'calluses': 1, 'mailman': 1, 'Beasley': 1, 'preface': 1, 'foreword': 1, 'temple': 1, 'Laos': 1, 'Farmer': 1, 'Almanac': 1, 'adding': 1, 'Lactobacillus': 1, 'bulgaricus': 1, 'Twin': 1, 'Cities': 1, 'pressured': 1, 'appointing': 1, 'B': 1, 'assassination': 1, 'UK': 1, 'pulls': 1, 'strings': 1, 'speaks': 1, 'Holden': 1, 'Caulfield': 1, 'rich': 1, 'quick': 1, 'Pepper': 1, 'scorpion': 1, 'caul': 1, 'thefts': 1, 'sailing': 1, 'Canal': 1, 'Christina': 1, 'Austerlitz': 1, 'stops': 1, 'adjournment': 1, '25th': 1, 'session': 1, 'Assembly': 1, 'goldenseal': 1, 'tarantula': 1, 'Mountains': 1, 'voices': 1, 'lustrum': 1, '36893': 1, 'ignores': 1, 'friends': 1, 'Med': 1, 'molybdenum': 1, 'porphyria': 1, 'bat': 1, 'fellatio': 1, 'facts': 1, 'dogsledding': 1, 'Merrick': 1, 'noble': 1, 'ogre': 1, 'sagebrush': 1, 'territories': 1, 'Irkutsk': 1, 'Yakutsk': 1, 'Kamchatka': 1, 'Chiang': 1, 'Kai': 1, 'shek': 1, 'sexiest': 1, 'Hajo': 1, 'a.m.': 1, 'emperors': 1, 'problems': 1, 'Notre': 1, 'Dame': 1, 'exports': 1, 'pasta': 1, 'merchandise': 1, 'sales': 1, 'z': 1, 'reputed': 1, 'mythology': 1, 'Gaulle': 1, 'fingernails': 1, 'Ishmael': 1, 'Moby': 1, 'jobs': 1, 'earns': 1, 'Hell': 1, 'Kitchen': 1, 'aids': 1, 'riding': 1, 'eats': 1, 'sleeps': 1, 'underground': 1, 'cookie': 1, 'abolished': 1, 'importance': 1, 'Magellan': 1, 'Gran': 1, 'Bernardo': 1, 'arab': 1, 'strap': 1, 'facility': 1, 'ballcock': 1, 'overflow': 1, 'bras': 1, 'Italians': 1, 'Bavaria': 1, 'submarines': 1, 'Peugeot': 1, 'blondes': 1, 'discontent': 1, 'caps': 1, 'G7': 1, 'Omni': 1, 'ultimate': 1, 'unanswerable': 1, 'Hesse': 1, 'procedure': 1, 'drilling': 1, 'skull': 1, 'acheive': 1, 'higher': 1, 'ion': 1, 'Perfect': 1, 'Fool': 1, 'bullseye': 1, 'darts': 1, 'millimeters': 1, 'Aldous': 1, 'Huxley': 1, 'Halloween': 1, 'aortic': 1, 'abdominal': 1, 'aneurysm': 1, 'surroundings': 1, 'Ninety': 1, 'Theses': 1, 'stratocaster': 1, 'Element': 1, 'screensaver': 1, 'funnel': 1, 'Rossetti': 1, 'Beata': 1, 'Beatrix': 1, 'thin': 1, 'weapons': 1, 'warfare': 1, 'mourning': 1, 'unsuccessful': 1, 'overthrow': 1, 'Bavarian': 1, 'condensed': 1, 'relax': 1, 'clear': 1, 'mathematical': 1, 'factor': 1, 'authors': 1, 'goals': 1, 'scored': 1, 'Polo': 1, 'Kubla': 1, 'Khan': 1, 'tenths': 1, 'duke': 1, 'NASDAQ': 1, 'marvelous': 1, 'spokesman': 1, 'Canon': 1, 'Major': 1, 'Megan': 1, 'expedition': 1, 'climbing': 1, 'Wilkes': 1, 'plantation': 1, 'encyclopedia': 1, 'crooner': 1, 'Sisters': 1, 'Pistol': 1, 'Packin': 1, 'Mama': 1, 'cricketer': 1, '1898': 1, 'Knight': 1, 'Ridder': 1, 'roommates': 1, 'cigar': 1, 'chewing': 1, 'observed': 1, 'feel': 1, 'Iberian': 1, 'peninsula': 1, 'Penn': 1, 'Landing': 1, 'banks': 1, 'Ozzie': 1, 'Harriet': 1, 'JESSICA': 1, 'mystical': 1, 'ravens': 1, 'Odin': 1, 'biceps': 1, 'saliva': 1, 'kidnaping': 1, 'termed': 1, 'Crime': 1, 'immigration': 1, 'laws': 1, 'Vince': 1, 'Lombardi': 1, 'coaching': 1, 'studio': 1, 'Bateau': 1, 'Lavoir': 1, 'Montmartre': 1, 'silkworm': 1, 'moth': 1, 'domestication': 1, 'else': 1, 'swastika': 1, 'stood': 1, 'paintball': 1, 'clot': 1, 'distilling': 1, 'height': 1, 'Eyes': 1, 'secondary': 1, 'Mammoth': 1, 'Cave': 1, 'collectible': 1, 'Donald': 1, 'Duck': 1, 'Boat': 1, 'acres': 1, 'suspension': 1, 'anthem': 1, 'nonconsecutive': 1, 'cartoondom': 1, 'climb': 1, 'Nepal': 1, 'denied': 1, 'snatches': 1, 'jerks': 1, 'barrier': 1, 'Czech': 1, 'Wallbanger': 1, 'Spartacus': 1, 'gladiator': 1, 'qualifications': 1, 'individuals': 1, 'donating': 1, 'Nicois': 1, '1937': 1, 'Garrett': 1, 'Morgan': 1, 'fascinated': 1, 'experimenting': 1, 'neurasthenia': 1, 'Dialing': 1, 'Janet': 1, 'Gregorian': 1, 'row': 1, 'Carroll': 1, 'Humpty': 1, 'Dumpty': 1, 'deltiologist': 1, 'collect': 1, 'firewall': 1, 'Galaxy': 1, 'revolution': 1, 'skyline': 1, 'Gateway': 1, '1990s': 1, 'droppings': 1, 'Angelus': 1, 'Best': 1, 'Actor': 1, 'slavery': 1, 'properly': 1, 'Ukrainians': 1, 'builder': 1, 'captured': 1, 'Syrian': 1, 'mounted': 1, 'guerrilla': 1, 'Jesse': 1, 'Coleman': 1, 'Younger': 1, 'ridden': 1, 'sequencing': 1, 'Beverly': 1, 'Hillbillies': 1, 'Daisy': 1, 'Moses': 1, 'assent': 1, 'emblem': 1, 'nematode': 1, 'Cartesian': 1, 'Diver': 1, 'pusher': 1, 'poorly': 1, 'economists': 1, 'chancery': 1, 'buxom': 1, 'blonde': 1, 'magazines': 1, 'Let': 1, 'goddamn': 1, 'airborne': 1, 'Boston': 1, 'Kreme': 1, 'glacier': 1, 'U.S': 1, 'ATM': 1, 'Malta': 1, 'Tunnel': 1, 'practical': 1, 'marketed': 1, 'socioeconomic': 1, 'ranges': 1, 'continental': 1, 'Bic': 1, 'flame': 1, 'clockwise': 1, 'counterclockwise': 1, 'Graffiti': 1, 'Zimbabwe': 1, 'Abigail': 1, 'Arcane': 1, 'villainous': 1, 'opponent': 1, 'Swamp': 1, 'Cromwell': 1, 'pepper': 1, 'whiskers': 1, 'journalism': 1, 'Universal': 1, 'Import': 1, 'Export': 1, 'agricultural': 1, 'belly': 1, 'buttons': 1, 'Thompson': 1, 'flood': 1, 'Luis': 1, 'Rey': 1, 'Marino': 1, 'prenatal': 1, 'engineer': 1, 'Harold': 1, 'Stassen': 1, 'Esa': 1, 'Pekka': 1, 'Belmont': 1, 'Stakes': 1, 'Brenner': 1, 'chief': 1, 'Powhatan': 1, 'Rolfe': 1, 'somene': 1, 'honey': 1, 'HIGHEST': 1, 'KDGE': 1, 'Burkina': 1, 'Faso': 1, 'cookies': 1, 'similar': 1, 'Lyrics': 1, 'Server': 1, 'pneumonia': 1, 'afflict': 1, 'Prophet': 1, 'Medina': 1, 'mainland': 1, 'surpassing': 1, 'blew': 1, 'Lakehurst': 1, 'clay': 1, 'mixture': 1, 'china': 1, 'ports': 1, 'chickens': 1, 'chicks': 1, 'Docklands': 1, 'Light': 1, 'constructed': 1, 'safest': 1, 'pedestrians': 1, 'Assisi': 1, 'driven': 1, 'Bandit': 1, 'Collector': 1, 'Tampa': 1, 'Fulton': 1, 'stove': 1, 'ground': 1, 'Am': 1, 'Flight': 1, '103': 1, 'Lockerbie': 1, 'portly': 1, 'criminologist': 1, 'Carl': 1, 'Hyatt': 1, 'Checkmate': 1, 'Ed': 1, 'allegedly': 1, 'obscene': 1, 'gesture': 1, 'consider': 1, 'blunder': 1, 'previous': 1, 'Commonwealth': 1, 'mustachioed': 1, 'Frankie': 1, 'sixties': 1, 'throne': 1, 'abdication': 1, 'estimated': 1, 'whitetail': 1, 'soy': 1, 'electronics': 1, 'Before': 1, 'Mast': 1, 'seafarers': 1, 'succeeded': 1, 'Nikita': 1, 'recomended': 1, 'switch': 1, 'crib': 1, 'bed': 1, 'lying': 1, 'dollars': 1, 'Frommer': 1, 'employed': 1, '72': 1, 'baseemen': 1, 'incorporate': 1, 'Ball': 1, 'Booth': 1, 'Catherine': 1, 'hits': 1, 'foul': 1, 'filling': 1, 'tonsils': 1, 'Bud': 1, 'Melman': 1, 'Toast': 1, 'Stick': 1, 'Mpilo': 1, 'oyster': 1, 'phobophobe': 1, 'stings': 1, 'Gillette': 1, 'thermal': 1, 'equilibrium': 1, 'belt': 1, 'Cookbook': 1, '198': 1, 'Liz': 1, 'Chandler': 1, 'Guiteau': 1, 'childhood': 1, 'Foreman': 1, 'victim': 1, 'copier': 1, 'Doodyville': 1, 'Orphans': 1, 'Fund': 1, 'bachelor': 1, 'bedroom': 1, 'Sistine': 1, 'Chapel': 1, 'popularly': 1, 'pallbearer': 1, 'tsetse': 1, 'Peruvian': 1, 'mummified': 1, 'Pizarro': 1, 'cecum': 1, 'hardware': 1, 'weakness': 1, 'enforce': 1, 'youngsters': 1, 'please': 1, 'vessels': 1, 'melt': 1, 'quicker': 1, '401': 1, 'Livingstone': 1, 'Seagull': 1, 'browser': 1, 'Mosaic': 1, 'includes': 1, '1896': 1, 'italian': 1, 'traditional': 1, 'No.1': 1, 'apparel': 1, 'Horsemen': 1, 'Apocalypse': 1, 'feeling': 1, 'experienced': 1, 'something': 1, 'laser': 1, 'mid': 1, '1900s': 1, 'Catch-22': 1, 'radiographer': 1, 'conditioner': 1, 'efficiency': 1, 'Science': 1, 'Maximo': 1, 'Picts': 1, 'processing': 1, 'FUBU': 1, 'childbirth': 1, 'zone': 1, 'archery': 1, 'hearts': 1, 'octopus': 1, 'Meanie': 1, 'Andorra': 1, 'nestled': 1, 'traditions': 1, 'Elizabethian': 1, 'Giza': 1, 'S&P': 1, 'prevents': 1, 'eczema': 1, 'seborrhea': 1, 'psoriasis': 1, 'Aristotle': 1, 'Onassis': 1, 'yacht': 1, 'trace': 1, 'roots': 1, 'Ringo': 1, 'Stagecoach': 1, 'things': 1, 'Cozumel': 1, 'teenagers': 1, 'violent': 1, 'McQueen': 1, 'Edward': 1, 'G.': 1, 'Cincinnati': 1, 'lasts': 1, 'tells': 1, 'betrayed': 1, 'thrillers': 1, 'Cortez': 1, '69': 1, 'Econoline': 1, 'van': 1, 'F25': 1, 'V1': 1, 'hypnotherapy': 1, 'inspiration': 1, 'schoolteacher': 1, 'Poets': 1, 'surfboard': 1, 'well': 1, 'Hans': 1, 'Henderson': 1, 'spanish': 1, 'spine': 1, 'alleged': 1, 'Shroud': 1, 'Turin': 1, 'irate': 1, 'oxide': 1, 'protects': 1, 'Comics': 1, 'realm': 1, 'Hooked': 1, 'Feeling': 1, 'Ally': 1, 'Mcbeal': 1, 'Jaco': 1, 'Pastorius': 1, 'Raging': 1, 'health': 1, 'nutrition': 1, 'Erica': 1, 'Jong': 1, 'Isadora': 1, 'Wing': 1, 'maids': 1, 'milking': 1, 'funny': 1, 'creative': 1, 'genius': 1, 'Everything': 1, 'hustles': 1, 'waits': 1, 'zoological': 1, 'ruminant': 1, 'cherry': 1, 'audio': 1, 'delicacy': 1, 'indelicately': 1, 'pickled': 1, 'roe': 1, 'Sandra': 1, 'Bullock': 1, 'ejaculation': 1, 'nuts': 1, 'marzipan': 1, 'travelling': 1, 'u': 1, 'Wilbur': 1, 'Reed': 1, 'owning': 1, 'Sondheim': 1, 'ballad': 1, 'Well': 1, 'maybe': 1, 'flab': 1, 'chin': 1, 'Yesterdays': 1, 'bow': 1, 'goulash': 1, 'feat': 1, 'U-2': 1, 'quickest': 1, 'nail': 1, 'polish': 1, 'youngest': 1, 'recognize': 1, 'anorexia': 1, 'translate': 1, 'gandy': 1, 'dancer': 1, 'Elvis': 1, 'Presley': 1, 'Sultan': 1, 'Ted': 1, 'edge': 1, 'Vladimir': 1, 'Nabokov': 1, 'Professor': 1, 'Humbert': 1, 'chemiosmotic': 1, 'Bernadette': 1, 'Peters': 1, 'specifically': 1, 'Noah': 1, 'Ark': 1, 'Antonia': 1, 'Shimerda': 1, 'farm': 1, 'powder': 1, 'lotion': 1, 'smell': 1, 'Langerhans': 1, 'zipper': 1, 'temperance': 1, 'advocate': 1, 'wielded': 1, 'hatchet': 1, 'saloons': 1, 'assassinations': 1, '1865': 1, 'Hammer': 1, 'Studios': 1, 'thirds': 1, 'midwest': 1, 'slang': 1, 'darn': 1, 'tootin': 1, 'Dylan': 1, 'aquatic': 1, 'scenes': 1, 'Springs': 1, 'crossed': 1, 'slits': 1, 'castles': 1, 'accommodate': 1, 'photograph': 1, 'Quirk': 1, 'Theresa': 1, 'At': 1, 'village': 1, 'Mancha': 1, 'Depression': 1, 'Funk': 1, 'Lata': 1, 'aurora': 1, 'motorcycle': 1, 'folk': 1, 'Chapman': 1, 'legendary': 1, 'reputation': 1, 'stealing': 1, 'jokes': 1, 'touching': 1, 'ambassadorial': 1, 'relations': 1, 'apostle': 1, 'Caldwell': 1, 'VDRL': 1, 'solar': 1, 'experiment': 1, 'Darwin': 1, 'Basilica': 1, 'AFS': 1, 'mosquitoes': 1, 'fair': 1, 'projects': 1, '8th': 1, 'Wassermann': 1, 'develop': 1, 'specific': 1, 'Swampy': 1, 'Mankiewicz': 1, 'entries': 1, '1669': 1, 'blend': 1, 'herbs': 1, 'spices': 1, 'basic': 1, 'strokes': 1, 'criticism': 1, 'Neal': 1, 'grandeur': 1, 'Sonny': 1, 'Liston': 1, 'succeed': 1, 'champion': 1, 'policeman': 1, 'edition': 1, 'Morris': 1, 'bishop': 1, 'becomes': 1, 'Hawkeye': 1, 'Stockyards': 1, 'Gothic': 1, 'Soldiers': 1, 'battles': 1, 'nevermind': 1, 'dye': 1, 'compass': 1, 'prefix': 1, 'surnames': 1, 'breeding': 1, 'edentulous': 1, 'smile': 1, 'hormone': 1, 'Chiricahua': 1, 'ingredients': 1, 'consumption': 1, 'Tyler': 1, 'regular': 1, 'Tadeus': 1, 'Wladyslaw': 1, 'Konopka': 1, 'Vasco': 1, 'Gama': 1, 'Hampshire': 1, 'hamlet': 1, 'rises': 1, 'elections': 1, 'biochemists': 1, 'Warlock': 1, 'forehead': 1, 'ham': 1, '1789': 1, 'Far': 1, 'Madding': 1, 'Crowd': 1, 'Cribbage': 1, 'isolationist': 1, 'Theo': 1, 'Rousseau': 1, 'Fontaine': 1, 'eighth': 1, 'Nones': 1, 'C.': 1, 'Calhoun': 1, 'arometherapy': 1, 'occurred': 1, 'Bebrenia': 1, 'Amazonis': 1, 'Bagdad': 1, 'bends': 1, 'Fordham': 1, 'Waynesburg': 1, '12601': 1, 'Skrunch': 1, 'Butter': 1, 'Oompas': 1, 'pass': 1, 'Copyright': 1, 'lemurs': 1, 'Matterhorn': 1, 'Isis': 1, 'goddess': 1, 'Grace': 1, 'Metalious': 1, 'Suzy': 1, 'Parker': 1, 'Islam': 1, 'resulting': 1, 'hiemal': 1, 'activity': 1, 'normally': 1, 'quality': 1, 'drinks': 1, 'stone': 1, 'rolling': 1, 'west': 1, 'completed': 1, 'quarterbacks': 1, 'shortstop': 1, 'belonged': 1, 'tattoo': 1, 'wrist': 1, 'reading': 1, 'Forever': 1, 'climbed': 1, 'Mt.': 1, 'young': 1, 'flights': 1, 'PhotoShop': 1, 'Gompers': 1, 'Mayan': 1, 'Allies': 1, 'Avalanche': 1, 'plural': 1, 'Elongated': 1, 'afoot': 1, 'heavier': 1, 'U.S.S.R.': 1, 'appointed': 1, 'chair': 1, 'Reserve': 1, 'sled': 1, 'Iditarod': 1, 'cannon': 1, 'saltpeter': 1, 'birthdate': 1, 'BMW': 1, 'waterfall': 1, 'hyperlink': 1, 'ribbon': 1, 'Video': 1, 'Aeul': 1, 'Jell': 1, 'refers': 1, 'differences': 1, 'Methodist': 1, '139': 1, 'papal': 1, 'kite': 1, 'Doxat': 1, 'Stirred': 1, 'Not': 1, 'Shaken': 1, 'corridors': 1, 'Pentagon': 1, 'rare': 1, 'symptoms': 1, 'involuntary': 1, 'movements': 1, 'tics': 1, 'swearing': 1, 'incoherent': 1, 'vocalizations': 1, 'grunts': 1, 'shouts': 1, 'replies': 1, 'Leia': 1, 'confession': 1, 'Strikes': 1, 'Back': 1, 'Madilyn': 1, 'Kahn': 1, 'Wilder': 1, 'manifest': 1, 'latent': 1, 'theories': 1, 'taller': 1, 'ran': 1, 'Lenny': 1, 'Bruce': 1, 'arrested': 1, 'gangsters': 1, 'Clyde': 1, 'Koran': 1, 'Sinclair': 1, 'Main': 1, 'Fe': 1, 'traded': 1, 'Hermitage': 1, 'materials': 1, 'decompose': 1, 'Visine': 1, 'Lagos': 1, 'permanently': 1, 'frozen': 1, 'Cleaveland': 1, 'Cavaliers': 1, 'Pines': 1, 'cholera': 1, 'brunettes': 1, 'scares': 1, 'credits': 1, 'Also': 1, 'Known': 1, '-lantern': 1, 'Elroy': 1, 'Hirsch': 1, 'tabulates': 1, 'ballots': 1, 'victor': 1, 'doorstep': 1, 'Gasoline': 1, 'Katie': 1, 'Dartmouth': 1, 'Woodstock': 1, 'NAFTA': 1, 'propaganda': 1, 'Water': 1, 'Carrier': 1, 'zodiacal': 1, 'wash': 1, 'contagious': 1, 'governmental': 1, 'dealing': 1, 'racism': 1, 'gametophytic': 1, 'tissue': 1, 'Boris': 1, 'Pasternak': 1, 'Kim': 1, 'Philby': 1, 'Chief': 1, 'Aztecs': 1, 'Gods': 1, 'editor': 1, 'cake': 1, 'steam': 1, 'rainstorm': 1, 'bulls': 1, 'Pamplona': 1, 'suspect': 1, 'Clue': 1, 'Percival': 1, 'Lovell': 1, 'claws': 1, 'pistol': 1, 'ailment': 1, 'Urgent': 1, 'Fury': 1, 'advised': 1, 'listeners': 1, 'Chevrolet': 1, 'excluded': 1, 'ANZUS': 1, 'alliance': 1, 'L.L.': 1, 'malls': 1, 'decorations': 1, 'assigned': 1, 'incompetent': 1, 'shores': 1, 'Gitchee': 1, 'Gumee': 1, 'philosophy': 1, 'plans': 1, 'forerunner': 1, 'folklore': 1, 'Admiral': 1, 'Seas': 1, 'Viceroy': 1, 'Governor': 1, 'granted': 1, '10-': 1, 'profits': 1, 'voyage': 1, 'noise': 1, 'Dondi': 1, 'adoptive': 1, 'grandfather': 1, 'Abbey': 1, 'Rubin': 1, 'Hayden': 1, 'feeding': 1, 'pigeons': 1, 'Piazza': 1, 'colony': 1, 'Greenland': 1, '985': 1, 'equivalence': 1, 'Bloom': 1, 'resident': 1, 'wreaks': 1, 'destroyed': 1, 'x': 1, 'Kinks': 1, 'hiking': 1, 'Iraqis': 1, 'Faber': 1, 'Mongol': 1, 'lucky': 1, 'enough': 1, 'sprayed': 1, 'Corpus': 1, 'Christi': 1, 'skiing': 1, 'Calgary': 1, 'clitoris': 1, 'costliest': 1, 'disaster': 1, 'industry': 1, 'Bilbo': 1, 'Baggins': 1, 'Sebastian': 1, 'Mahal': 1, 'Sabres': 1, 'chick': 1, 'breathe': 1, 'nearest': 1, 'hang': 1, 'Mona': 1, 'Lisa': 1, 'Crimean': 1, 'Churchill': 1, 'intelligence': 1, 'stained': 1, 'window': 1, 'Gustav': 1, 'V': 1, '195': 1, 'transistor': 1, 'Charge': 1, 'magnetar': 1, 'lion': 1, 'brew': 1, 'Christine': 1, 'possessed': 1, 'magenta': 1, 'Ligurian': 1, 'RCD': 1, 'equity': 1, 'securities': 1, 'Lutine': 1, 'Bell': 1, 'announce': 1, 'Eli': 1, 'Lilly': 1, 'jealousy': 1, 'referring': 1, 'it-': 1, 'racehorse': 1, 'Associated': 1, 'Press': 1, '20th': 1, 'panties': 1, 'Titans': 1, 'adopt': 1, 'AIM-54C': 1, 'threat': 1, 'Goldie': 1, 'Hawn': 1, 'boyfriend': 1, 'filmmakers': 1, 'collabrative': 1, 'pregnancies': 1, 'rathaus': 1, 'Frankfurt': 1, 'fell': 1, 'Elizabeth': 1, 'Malawi': 1, 'sprocket': 1, '35': 1, 'millimeter': 1, 'spelled': 1, 'Y': 1, 'Finnish': 1, 'psychology': 1, 'Tiny': 1, 'Tim': 1, 'alone': 1, 'Astroturf': 1, 'creams': 1, 'seaweed': 1, 'Scrabble': 1, 'Crossword': 1, 'Game': 1, 'Declaration': 1, 'slinky': 1, 'bite': 1, 'draws': 1, 'CC': 1, 'Fickle': 1, 'Fate': 1, 'congressional': 1, 'delegation': 1, 'June': 1, 'Tutankhamun': 1, 'exhibit': 1, 'moving': 1, 'transported': 1, 'Deere': 1, 'tractors': 1, 'bounded': 1, 'Coral': 1, 'Tasman': 1, 'shipment': 1, 'proper': 1, 'respones': 1, 'Say': 1, 'goodnight': 1, 'Kwai': 1, 'pet': 1, 'invaded': 1, 'Poland': 1, 'leprosy': 1, 'licensed': 1, 'geese': 1, 'U.S.-based': 1, 'Jenna': 1, 'eligible': 1, 'Roller': 1, 'Ian': 1, 'Fleming': 1, 'seed': 1, 'piles': 1, 'canonize': 1, 'Post': 1, '1895': 1, 'H.G.': 1, 'Wells': 1, 'Chronic': 1, 'Argonauts': 1, 'tips': 1, 'fireplace': 1, 'geological': 1, 'neurons': 1, 'divides': 1, 'Eastern': 1, 'Shores': 1, 'Louise': 1, 'Fletcher': 1, 'builds': 1, 'odor': 1, 'cows': 1, 'MTV': 1, 'salesman': 1, 'UOL': 1, 'Marciano': 1, 'pelvic': 1, 'restore': 1, 'distribute': 1, 'humanitarian': 1, 'relief': 1, 'Somalia': 1, 'monarchs': 1, 'crowned': 1, 'LMDS': 1, 'finish': 1, '1926': 1, 'Luke': 1, 'Julie': 1, 'Poppins': 1, 'Motown': 1, 'Records': 1, 'Drinks': 1, 'Abbie': 1, 'dose': 1, 'Perry': 1, 'dying': 1, 'His': 1, 'Voice': 1, 'fade': 1, 'hijack': 1, 'aol.com': 1, 'yahoo.com': 1, 'Priestley': 1, 'erase': 1, 'nylon': 1, 'stockings': 1, 'sale': 1, 'pocket': 1, 'billiards': 1, 'hocks': 1, 'personal': 1, 'Minnesota': 1, 'hemisphere': 1, 'nonaggression': 1, 'pact': 1, 'tenpin': 1, 'brilliant': 1, 'economist': 1, 'creation': 1, 'Helps': 1, 'hurt': 1, 'hurting': 1, 'Warren': 1, 'Spahn': 1, '20': 1, 'curies': 1, 'seventeen': 1, 'defensive': 1, 'Diplomacy': 1, 'challenged': 1, 'explore': 1, 'Frontier': 1, 'tools': 1, 'crewel': 1, '1915': 1, 'gross': 1, 'Redskin': 1, 'fan': 1, 'lemon': 1, 'automobiles': 1, 'Zatanna': 1, 'appropriate': 1, 'Yom': 1, 'Kippur': 1, 'sunk': 1, 'mine': 1, 'Havana': 1, 'harbor': 1, 'Lust': 1, 'data': 1, 'forests': 1, 'Feynman': 1, 'Physics': 1, 'repeating': 1, 'voter': 1, 'builders': 1, 'mainly': 1, 'Capone': 1, 'prism': 1, 'transmitted': 1, 'Anopheles': 1, 'gymnophobia': 1, 'Chaplin': 1, 'Modern': 1, 'Dictator': 1, 'Titus': 1, 'Metropolis': 1, 'expect': 1, 'monthly': 1, 'publication': 1, 'Bigfoot': 1, 'News': 1, 'judiciary': 1, 'Music': 1, 'Iceland': 1, 'ready': 1, 'paracetamol': 1, 'touched': 1, 'wind': 1, 'quiz': 1, 'Vera': 1, 'Lynn': 1, 'We': 1, 'Meet': 1, 'Again': 1, 'exclusively': 1, 'Inoco': 1, 'laptop': 1, 'Blaise': 1, 'Pascal': 1, 'returned': 1, 'fraudulent': 1, 'operations': 1, 'pacer': 1, 'compete': 1, 'alcohol': 1, 'Gimli': 1, 'Bobby': 1, 'Fischer': 1, 'Astaire': 1, 'Angela': 1, 'bicornate': 1, 'Led': 1, 'Zeppelin': 1, '187s': 1, 'mining': 1, 'cranes': 1, 'tampon': 1, 'AOL': 1, 'users': 1, 'raced': 1, 'Tour': 1, 'haven': 1, 'plugged': 1, 'Coffee': 1, 'firemen': 1, 'migrates': 1, 'farthest': 1, 'parts': 1, 'psorisis': 1, 'disappear': 1, 'photographer': 1, 'Yousuf': 1, 'Karsh': 1, 'shiest': 1, 'safety': 1, 'Oklahoma': 1, 'savings': 1, 'mature': 1, 'Air': 1, 'Force': 1, 'Citizen': 1, 'Kane': 1, 'Swiss': 1, 'orbit': 1, 'Palpatine': 1, 'preferably': 1, 'bells': 1, 'radius': 1, 'ellipse': 1, 'Congo': 1, 'Westview': 1, 'Funky': 1, 'Winkerbean': 1, 'rounded': 1, 'matchbook': 1, 'recruited': 1, 'Saddam': 1, 'Hussein': 1, 'therapy': 1, 'elicit': 1, 'primal': 1, 'scream': 1, 'photosynthesis': 1, 'Sleeping': 1, 'Excite': 1, 'crust': 1, 'Manson': 1, 'Anne': 1, 'Boleyn': 1, 'provided': 1, 'listen': 1, 'Paraguay': 1, 'vacations': 1, 'Kemper': 1, 'calleda': 1, '1928': 1, 'affectionate': 1, 'Erich': 1, 'boob': 1, 'bomb': 1, 'prehistoric': 1, 'erotic': 1, 'apart': 1, 'n': 1, 'Boulevard': 1, '1959': 1, 'Bulge': 1, 'Famine': 1, 'Guernsey': 1, 'Sark': 1, 'Herm': 1, 'Lagoon': 1, 'Pirate': 1, 'Heaven': 1, 'speaking': 1, 'airman': 1, 'Goering': 1, 'Zolotow': 1, 'Shooting': 1, 'Soft': 1, 'Self': 1, 'Portrait': 1, 'Grilled': 1, 'Bacon': 1, 'spiritual': 1, 'cultural': 1, 'brothel': 1, 'chosen': 1, 'Joint': 1, 'Chiefs': 1, 'Staff': 1, 'boxcars': 1, 'flush': 1, 'Gibson': 1, 'learned': 1, 'saxophone': 1, 'speak': 1, 'Sons': 1, 'Lovers': 1, 'Mills': 1, 'Cheerios': 1, 'paths': 1, 'enhance': 1, 'athletic': 1, 'sporting': 1, 'collapsed': 1, 'coney': 1, 'Wembley': 1, 'dish': 1, 'pigs': 1, 'intestines': 1, 'engineering': 1, 'civilization': 1, 'nearsightedness': 1, 'Renaud': 1, 'headquartered': 1, 'contestant': 1, 'picking': 1, 'shower': 1, 'Stonehenge': 1, 'Rockettes': 1, 'Biggest': 1, 'Autry': 1, 'Please': 1, 'regards': 1, 'Dale': 1, 'Economy': 1, 'popularized': 1, 'Brillo': 1, 'pad': 1, 'boxes': 1, 'multicultural': 1, 'multilingual': 1, 'Belle': 1, 'Beast': 1, 'Film': 1, 'heir': 1, 'raising': 1, 'wreckage': 1, 'Andrea': 1, 'Doria': 1, 'Mideast': 1, 'slotbacks': 1, 'tailbacks': 1, 'touchbacks': 1, 'student': 1, 'Amherst': 1, 'Sicilian': 1, 'justify': 1, 'emergency': 1, 'decrees': 1, 'imprisoning': 1, 'opponents': 1, 'vesting': 1, 'snowboarding': 1, 'Argentina': 1, 'leftovers': 1, 'marl': 1, 'mineral': 1, 'pursued': 1, 'Tweety': 1, 'Pie': 1, 'currently': 1, 'inescapable': 1, 'purveyor': 1, 'heaviest': 1, 'fiddlers': 1, 'Cole': 1, 'Chilean': 1, 'd': 1, 'etat': 1, '84': 1, 'Judith': 1, 'Rossner': 1, 'Diane': 1, 'Keaton': 1, 'Eduard': 1, 'primitives': 1, 'rural': 1, 'goldfish': 1, 'dimly': 1, 'lit': 1, 'unique': 1, 'Canadians': 1, 'emmigrate': 1, 'curl': 1, 'tee': 1, 'Masters': 1, 'vichyssoise': 1, 'shake': 1, 'friendliness': 1, 'Olive': 1, 'Oyl': 1, 'Viagra': 1, 'node': 1, 'Woodrow': 1, 'Wilson': 1, 'dimension': 1, 'viscosity': 1, 'oldtime': 1, 'Guide': 1, 'Jeff': 1, 'Greenfield': 1, 'subversive': 1, 'B.Y.O.B.': 1, 'berry': 1, 'blackberry': 1, 'raspberry': 1, 'strawberry': 1, 'extensively': 1, 'grown': 1, 'appoint': 1, 'menu': 1, 'item': 1, 'spicey': 1, 'Tex': 1, 'arriving': 1, 'MGM': 1, 'treated': 1, 'Occam': 1, 'Razor': 1, 'puzzle': 1, '1913': 1, 'brightest': 1, 'visible': 1, 'ART': 1, 'JPEG': 1, 'Bitmap': 1, 'news': 1, 'typically': 1, '55': 1, 'hairs': 1, 'chihuahuas': 1, 'neurosurgeon': 1, 'Bjorn': 1, 'Borg': 1, 'forehand': 1, 'Even': 1, 'akita': 1, 'G': 1, 'False': 1, 'delicate': 1, 'tasting': 1, 'onion': 1, 'constitute': 1, 'trigonometry': 1, 'topic': 1, 'outline': 1, 'Reflections': 1, 'Holland': 1, 'Mackenzie': 1, 'Standard': 1, 'Industrial': 1, 'Classification': 1, 'SIC': 1, 'Antilles': 1, '33': 1, 'Rock': 1, 'approaches': 1, 'systems': 1, 'analysis': 1, 'Salk': 1, 'Head': 1, 'Start': 1, 'postal': 1, 'ouzo': 1, 'Milo': 1, 'knighted': 1, 'novels': 1, 'section': 1, 'finger': 1, 'joint': 1, 'hobby': 1, 'A&W': 1, 'camels': 1, 'humps': 1, 'surge': 1, 'Pompeii': 1, 'dial': 1, 'tragedy': 1, 'Freddy': 1, 'Freeman': 1, 'Cinderslut': 1, 'skein': 1, 'wool': 1, 'capitalizes': 1, 'pronoun': 1, 'TO': 1, 'VB': 1, 'POS': 1, 'reference': 1, 'Biblical': 1, 'quotation': 1, 'together': 1, 'colored': 1, 'squares': 1, 'Anything': 1, 'Goes': 1, 'IT': 1, 'User': 1, 'Satisfaction': 1, 'Level': 1, 'Joyce': 1, 'Ulysses': 1, 'Musician': 1, 'HDLC': 1, 'rugby': 1, 'bull': 1, 'Masons': 1, 'Rosanne': 1, 'Rosanna': 1, '48': 1, 'conterminous': 1, 'Dorsets': 1, 'Lincolns': 1, 'Oxfords': 1, 'Southdowns': 1, 'Transparent': 1, 'Salvador': 1, 'Dali': 1, 'worlds': 1, 'supplier': 1, 'cannabis': 1, 'amaretto': 1, 'biscuits': 1, 'Estonia': 1, 'bottled': 1, 'jeroboams': 1, 'stethoscope': 1, 'disks': 1, 'Crokinole': 1, 'Smartnet': 1, 'Georgetown': 1, 'Hoya': 1, 'cables': 1, 'corgi': 1, 'Inch': 1, 'Nails': 1, 'Mandy': 1, 'approval': 1, 'Pelt': 1, 'psychiatric': 1, 'sessions': 1, 'husbands': 1, 'Hilton': 1, 'Wilding': 1, 'binomial': 1, 'coefficients': 1, 'Shirtwaist': 1, 'describes': 1, 'usage': 1, 'Elysium': 1, 'impress': 1, 'guy': 1, 'Ezra': 1, 'Taft': 1, 'Benson': 1, 'install': 1, 'tile': 1, 'showers': 1, 'Sicily': 1, 'drafted': 1, 'logarithmic': 1, 'scales': 1, 'slide': 1, 'weekend': 1, 'Monterey': 1, 'Jazz': 1, 'hairdryer': 1, 'Bullwinkle': 1, 'Pluribus': 1, 'Unum': 1, 'ante': 1, 'mortem': 1, 'Tel': 1, 'Aviv': 1, 'Memphis': 1, 'mailing': 1, 'lists': 1, 'pens': 1, 'Roe': 1, 'Wade': 1, 'decision': 1, 'wolverine': 1, 'habits': 1, 'foreclosure': 1, 'Yale': 1, 'Lock': 1, '1976': 1, 'Deep': 1, 'Throat': 1, 'clone': 1, 'larynx': 1, 'USSR': 1, 'dissolved': 1, 'Kythnos': 1, 'Siphnos': 1, 'Seriphos': 1, 'Mykonos': 1, 'Baffin': 1, 'Frobisher': 1, 'looking': 1, 'rarest': 1, 'encounters': 1, 'auberge': 1, 'tabs': 1, 'Third': 1, 'Eye': 1, 'Blind': 1, 'watchman': 1, 'Frank': 1, 'Wills': 1, 'racoon': 1, 'trinitrotoluene': 1, 'Nordic': 1, 'Montana': 1, '1954': 1, 'exactly': 1, 'anteater': 1, 'Seine': 1, 'Konigsberg': 1, 'behavior': 1, 'violates': 1, 'accepted': 1, 'standards': 1, 'morality': 1, 'pins': 1, 'skittles': 1, 'centurion': 1, 'arms': 1, 'Harlow': 1, '1932': 1, 'artists': 1, 'tend': 1, 'portraits': 1, 'Fairground': 1, 'Valley': 1, 'Keller': 1, 'flightless': 1, 'cuckoo': 1, 'Bunyan': 1, 'ox': 1, 'Child': 1, 'suburban': 1, 'Feminine': 1, 'Mystique': 1, '5.9': 1, 'virus': 1, 'Frederick': 1, 'ingredient': 1, 'yogurt': 1, 'Renaissance': 1, 'Kyriakos': 1, 'Theotokopoulos': 1, '219': 1, 'writers': 1, 'Smothers': 1, 'Comedy': 1, 'Hour': 1, 'Nicklaus': 1, 'Professional': 1, 'Golfers': 1, 'tour': 1, 'Midsummer': 1, 'Dream': 1, 'Fang': 1, 'Tooth': 1, 'Pookie': 1, 'Cisalpine': 1, 'passing': 1, 'contents': 1, 'cuckquean': 1, 'BIOS': 1, 'residence': 1, 'kings': 1, 'Yiddish': 1, 'Theater': 1, 'barnstorming': 1, 'recently': 1, 'revive': 1, 'nebbish': 1, 'Neil': 1, 'Grenada': 1, 'commodity': 1, 'Wittenberg': 1, 'twenty': 1, 'twirl': 1, 'patrons': 1, 'Stonewall': 1, 'Greenwich': 1, 'Village': 1, 'balance': 1, 'snap': 1, 'crackle': 1, 'webpage': 1, 'Associaton': 1, 'Havlicek': 1, 'stay': 1, 'Only': 1, 'Twice': 1, 'Tiffany': 1, 'cookers': 1, 'kitchen': 1, 'Ozzy': 1, 'Osbourne': 1, 'LOL': 1, 'Voyager': 1, 'wished': 1, 'looked': 1, 'respond': 1, 'millenium': 1, 'Giant': 1, 'Steps': 1, 'wiener': 1, 'schnitzel': 1, 'Brigham': 1, 'Young': 1, 'traveled': 1, 'Rodeo': 1, 'Cowboys': 1, 'gringo': 1, '1919': 1, 'occurrence': 1, 'unarmed': 1, 'protestors': 1, 'Mile': 1, 'Knicks': 1, 'obelisk': 1, 'Getting': 1, 'Married': 1, 'Today': 1, 'jets': 1, 'vapor': 1, 'trail': 1, 'Jennifer': 1, 'Merrie': 1, 'Melodies': 1, 'Walt': 1, 'none': 1, 'employees': 1, 'vermicilli': 1, 'rigati': 1, 'zitoni': 1, 'tubetti': 1, 'Emma': 1, 'Peel': 1, 'Caesar': 1, 'magnate': 1, 'initials': 1, 'sleeve': 1, 'Padres': 1, 'producers': 1, 'promoters': 1, 'landing': 1, 'diamonds': 1, 'rabbit': 1, 'resembled': 1, 'jackass': 1, 'Corvette': 1, 'Tornado': 1, 'void': 1, 'pulse': 1, 'Esquire': 1, 'Ash': 1, '2001': 1, 'Odyssey': 1, 'leaky': 1, 'valve': 1, 'enigmatic': 1, 'acquitted': 1, 'treason': 1, 'plot': 1, 'Apartheid': 1, 'Deadwood': 1, 'ukulele': 1, 'Valentine': 1, 'anti': 1, 'D.H.': 1, 'Tenderness': 1, 'C.C.': 1, 'Magee': 1, 'Ferry': 1, 'Napolean': 1, 'Jena': 1, 'Auerstadt': 1, 'Cody': 1, 'Abominable': 1, 'Snowman': 1, 'wander': 1, 'kill': 1, 'Community': 1, 'Chest': 1, 'Drink': 1, 'thine': 1, 'Apollo': 1, 'minded': 1, 'Armstrong': 1, 'saute': 1, 'Petrified': 1, 'Eleven': 1, 'obtained': 1, 'recruits': 1, 'oilseeds': 1, 'thru': 1, 'genetics': 1, 'mushroom': 1, 'optical': 1, 'weakest': 1, 'Yes': 1, 'Can': 1, 'presided': 1, 'hackers': 1, 'Cuckoo': 1, 'Egg': 1, 'Tracking': 1, 'Spy': 1, 'Through': 1, 'Maze': 1, 'Espionage': 1, 'myth': 1, 'experience': 1, '123': 1, 'Calcutta': 1, 'Dubai': 1, 'concrete': 1, 'Hundred': 1, 'Secret': 1, 'Mitty': 1, 'Zionism': 1, 'meta': 1, 'dragonflies': 1, 'Inuits': 1, 'quantum': 1, 'leaps': 1, 'simpler': 1, 'fascist': 1, 'Snoopy': 1, 'enemy': 1, 'Knute': 1, 'Rockne': 1, 'text': 1, 'massive': 1, 'complex': 1, 'extends': 1, 'Alabama': 1, 'Hooters': 1, '1930s': 1, 'angles': 1, 'isosceles': 1, 'rejection': 1, 'beats': 1, 'Barney': 1, 'Rubble': 1, 'drops': 1, 'drought': 1, 'attendance': 1, 'Supper': 1, 'Poems': 1, 'fools': 1, 'teats': 1, 'Lifesaver': 1, 'petroleum': 1, 'solve': 1, 'Webster': 1, 'myself': 1, 'manicure': 1, 'Hope': 1, 'hostages': 1, 'Entebbe': 1, 'rid': 1, 'woodpeckers': 1, 'earthworms': 1, 'pasture': 1, 'Tempelhol': 1, 'besides': 1, 'lifting': 1, 'indicate': 1, '007': 1, 'feud': 1, '1891': 1, 'sand': 1, 'dunes': 1, 'psychologically': 1, 'storms': 1, 'Cherokee': 1, 'roles': 1, 'Streetcar': 1, 'Named': 1, 'Desire': 1, 'lent': 1, 'dot': 1, 'i': 1, 'iris': 1, 'shark': 1, 'reliable': 1, 'download': 1, 'Heretic': 1, 'travelers': 1, 'Goldfinger': 1, 'Denmark': 1, 'pit': 1, 'Monument': 1, 'Circle': 1, 'monument': 1, 'spectacle': 1, 'telecast': 1, 'coined': 1, 'cyberspace': 1, 'Neuromancer': 1, 'Statistical': 1, 'Abstract': 1, 'shopping': 1, 'mall': 1, 'Guam': 1, 'fool': 1, 'oath': 1, 'paradise': 1, 'kickoff': 1, 'climbs': 1, 'Bunker': 1, 'accompanied': 1, 'Ghost': 1, 'missions': 1, 'taught': 1, 'Matt': 1, 'Murdock': 1, 'extraordinary': 1, 'abilities': 1, '175': 1, 'tons': 1, 'happy': 1, 'Portuguese': 1, 'hope': 1, 'bocci': 1, 'attorneys': 1, 'Defense': 1, 'Free': 1, 'publisher': 1, 'camcorders': 1, 'utilities': 1, 'Casey': 1, 'boss': 1, 'tent': 1, 'figs': 1, 'ripe': 1, 'chronicled': 1, 'Katy': 1, 'Holstrum': 1, 'Congressman': 1, 'Glen': 1, 'Morley': 1, '45Mhz': 1, 'processor': 1, 'acetylsalicylic': 1, 'evaporate': 1, 'Pudding': 1, 'mark': 1, 'describing': 1, 'Godiva': 1, 'chocolates': 1, 'horoscope': 1, 'RCA': 1, 'Coronado': 1, 'bless': 1, 'sneeze': 1, 'exercises': 1, '1927': 1, 'revival': 1, 'bestowed': 1, 'commentary': 1, 'Puerto': 1, 'Rico': 1, 'vowel': 1, 'profit': 1, '836': 1, 'commerce': 1, 'truly': 1, 'tape': 1, 'understand': 1, 'entertainment': 1, 'Collins': 1, 'grades': 1, 'closed': 1, 'courier': 1, 'Hohenzollerns': 1, 'Uruguay': 1, 'Edo': 1, 'mixable': 1, 'anyone': 1, 'Around': 1, 'Loop': 1, 'dumplings': 1, 'BladeRunner': 1, 'Clear': 1, 'cleaner': 1, 'limbo': 1, 'thumb': 1, 'BUD': 1, 'Maiden': 1, 'acceptance': 1, 'speech': 1, 'yous': 1, 'galaxy': 1, 'formation': 1, 'ninjitsu': 1, 'kung': 1, 'fu': 1, 'Craps': 1, 'swapped': 1, 'families': 1, 'Period': 1, 'Ukraine': 1, 'onetime': 1, 'socialism': 1, 'highways': 1, 'Parma': 1, 'goodness': 1, 'Living': 1, 'Room': 1, 'connected': 1, 'seriously': 1, 'footed': 1, 'Musca': 1, 'domestica': 1, 'enters': 1, 'villain': 1, 'afraid': 1, 'carelessness': 1, 'carefreeness': 1, 'Scientists': 1, 'Stock': 1, 'Exchange': 1, 'poodle': 1, 'domesticated': 1, 'ferret': 1, 'biorhythm': 1, 'robbers': 1, '7th': 1, 'inning': 1, 'successfully': 1, 'liver': 1, 'shelf': 1, 'beside': 1, 'crouching': 1, '1886': 1, 'Tub': 1, 'Immaculate': 1, 'Conception': 1, 'tiles': 1, 'Shuttle': 1, 'jar': 1, 'dispatched': 1, 'cruiser': 1, 'carry': 1, 'Leslie': 1, 'Hornby': 1, 'injectors': 1, 'Silence': 1, 'Lambs': 1, 'compound': 1, 'resource': 1, 'departments': 1, 'ribavirin': 1, 'flatfish': 1, 'Bridges': 1, 'Upstairs': 1, 'Downstairs': 1, 'Pesth': 1, 'Buda': 1, 'merged': 1, 'eastern': 1, 'sprouted': 1, 'glowsticks': 1, 'repealed': 1, 'predominant': 1, 'Tab': 1, '1st': 1, 'sense': 1, 'tide': 1, 'ebb': 1, 'dipsomaniac': 1, 'crave': 1, 'tequila': 1, 'galliano': 1, 'Scott': 1, '194': 1, 'delivered': 1, 'newscast': 1, 'hijacked': 1, 'Nuremberg': 1, 'hebephrenia': 1, 'bullfighting': 1, 'article': 1, 'dice': 1, 'mendelevium': 1, 'frog': 1, 'dig': 1, \"'em\": 1, 'appropriates': 1, 'butcher': 1, 'handful': 1, 'eating': 1, 'utensils': 1, 'handicapped': 1, 'referees': 1, 'sought': 1, 'AAA': 1, 'liability': 1, 'skim': 1, 'annotated': 1, 'Coulee': 1, 'Dam': 1, 'Belize': 1, 'slow': 1, 'aging': 1, 'computers': 1, 'FDR': 1, 'MVP': 1, 'means': 1, 'Godfather': 1, 'tornado': 1, 'Packers': 1, 'philosophized': 1, 'stokes': 1, 'Janis': 1, 'Joplin': 1, 'McLean': 1, 'laments': 1, 'Buddy': 1, 'Holly': 1, 'Slightly': 1, 'ahead': 1, '86': 1, 'Socrates': 1, 'Rayburn': 1, 'walked': 1, 'OZ': 1, 'Explorer': 1, 'D.B.': 1, 'registers': 1, 'trademarks': 1, 'derived': 1, 'biritch': 1, 'Whist': 1, 'Leos': 1, 'mythical': 1, 'hourglass': 1, 'scythe': 1, 'ticker': 1, '1870': 1, 'Tufts': 1, 'sequel': 1, 'Pink': 1, 'Panther': 1, 'Falklands': 1, 'hairless': 1, 'touring': 1, 'Porgy': 1, 'Bess': 1, 'Chicken': 1, 'Pox': 1, 'corners': 1, 'spritsail': 1, 'harmful': 1, 'spray': 1, '43rd': 1, 'VW': 1, 'Beetle': 1, 'changes': 1, 'supercontinent': 1, 'Pangaea': 1, 'break': 1, 'walks': 1, 'multitalented': 1, 'award': 1, 'failed': 1, 'moderated': 1, 'concoct': 1, 'methods': 1, 'regulate': 1, 'monopolies': 1, 'suffrage': 1, 'replied': 1, 'begun': 1, 'terrorized': 1, 'Stalker': 1, 'Ms.': 1, 'expectant': 1, 'motor': 1, 'snoogans': 1, 'possum': 1, 'Everybody': 1, 'Comes': 1, 'Rick': 1, 'comprises': 1, 'Highlands': 1, 'Lowlands': 1, 'Uplands': 1, 'cook': 1, 'Rawhide': 1, 'pitched': 1, 'Power': 1, 'Bars': 1, 'honors': 1, 'Stones': 1, 'Natchitoches': 1, 'Mia': 1, 'Farrow': 1, 'reports': 1, 'Morning': 1, '46': 1, '227': 1, 'reviews': 1, 'Turbulent': 1, 'Souls': 1, 'evidence': 1, 'Cohan': 1, 'Dandy': 1, 'stern': 1, 'fiber': 1, 'stricken': 1, '528': 1, 'doctorate': 1, 'hike': 1, 'blinking': 1, 'burns': 1, 'vitamin': 1, 'B12': 1, 'rabies': 1, 'Milton': 1, 'Obote': 1, 'Waverly': 1, 'assign': 1, 'agents': 1, 'geckos': 1, 'variations': 1, 'Canfield': 1, 'Klondike': 1, 'hamburgers': 1, 'steakburgers': 1, 'poing': 1, 'powers': 1, 'weaknesses': 1, 'Lantern': 1, 'impenetrable': 1, 'fortifications': 1, 'frontier': 1, 'satirized': 1, 'countinghouse': 1, 'counting': 1, 'Nasty': 1, 'infomatics': 1, 'mountainous': 1, 'Lhasa': 1, 'Apso': 1, 'pain': 1, 'abuse': 1, 'choose': 1, 'witnesses': 1, 'execution': 1, 'forward': 1, 'thinking': 1, 'insert': 1, 'bagels': 1, 'boost': 1, 'predators': 1, 'Antarctica': 1, 'genie': 1, 'conjured': 1, 'Nancy': 1, 'Chuck': 1, 'mill': 1, 'shadows': 1, 'McCall': 1, 'prisoners': 1, 'competitor': 1, 'Trans': 1, 'plagues': 1, 'InterLata': 1, 'possible': 1, 'bid': 1, 'Contract': 1, 'spears': 1, 'Kenya': 1, 'surgeon': 1, 'performed': 1, 'Hurricane': 1, 'Annie': 1, 'neurotic': 1, 'Duane': 1, 'Song': 1, 'Solomon': 1, 'Squares': 1, 'bullets': 1, 'poetic': 1, 'blank': 1, 'verse': 1, 'Pro': 1, 'wrestler': 1, 'cwt': 1, 'elephants': 1, 'rabbits': 1, 'act': 1, 'Suzette': 1, 'assume': 1, 'Strasbourg': 1, 'oxidation': 1, 'worm': 1, '1980s': 1, 'Pilgrim': 1, 'survivor': 1, 'Dresden': 1, 'firestorm': 1, 'polka': 1, 'beaches': 1, 'quantity': 1, 'Martha': 1, 'distinction': 1, 'Thurgood': 1, 'Marshall': 1, 'proof': 1, 'houseplants': 1, 'metabolize': 1, 'carcinogens': 1, 'Dipper': 1, 'Gestapo': 1, 'attends': 1, 'Pencey': 1, 'Prep': 1, 'wingspan': 1, 'condor': 1, 'spamming': 1, 'hen': 1, 'lay': 1, 'sent': 1, 'brief': 1, 'message': 1, 'conquered': 1, 'oddsmaker': 1, 'Snyder': 1, 'compounds': 1, 'categorized': 1, 'bourgeoisie': 1, 'aimed': 1, 'audience': 1, 'reproduce': 1, 'legally': 1, 'testament': 1, 'kilamanjaro': 1, 'apple': 1, 'URL': 1, 'extensions': 1, 'Pictures': 1, 'forged': 1, 'Cliff': 1, 'Robertson': 1, 'check': 1, 'Some': 1, 'newspapers': 1, 'dispose': 1, 'garbage': 1, 'bald': 1, 'Presidents': 1, 'Zapper': 1, 'dwarf': 1, 'Frosted': 1, 'Flakes': 1, 'Gleason': 1, 'Bendix': 1, 'flat': 1, 'explosion': 1, 'sphere': 1, 'frogs': 1, 'Dennison': 1, 'doesn': 1, 'www.answers.com': 1, 'Eckley': 1, 'duck': 1, 'Kindergarden': 1, 'Eliot': 1, 'false': 1, 'funded': 1, 'Indies': 1, 'fertile': 1, 'sardonyx': 1, 'dramatized': 1, 'tyvek': 1, 'Baryshnikov': 1, 'danced': 1, 'millionth': 1, 'verdict': 1, '1925': 1, 'T.': 1, 'Devo': 1, 'biologist': 1, 'Mack': 1, 'Sennett': 1, 'ticklish': 1, 'bytes': 1, 'terabyte': 1, 'guys': 1, 'whoever': 1, 'finds': 1, 'wins': 1, 'backup': 1, '23': 1, '924': 1, 'rebounds': 1, 'horseshoes': 1, 'bring': 1, 'Ruby': 1, 'M3': 1, 'growth': 1, 'agencies': 1, 'employment': 1, 'verification': 1, 'organic': 1, 'dwellers': 1, 'slane': 1, 'Rita': 1, 'Hayworth': 1, 'Doyle': 1, 'Honeymooners': 1, 'Television': 1, 'promising': 1, 'Kong': 1, 'Jim': 1, 'Bohannon': 1, 'Talk': 1, 'steepest': 1, 'streets': 1, 'quarters': 1, 'furlongs': 1, 'quarter': 1, 'recetrack': 1, 'laid': 1, 'baseman': 1, 'Supergirl': 1, 'penguins': 1, 'Pump': 1, 'entirely': 1, 'idle': 1, 'Romania': 1, 'Fig': 1, 'Newtons': 1, 'filthiest': 1, 'alive': 1, 'monarchy': 1, 'ladybugs': 1, 'actual': 1, 'Fourteenth': 1, 'Rubens': 1, 'Dyck': 1, 'Bruegel': 1, 'citizens': 1, 'Gina': 1, 'donor': 1, 'slime': 1, 'Sunnyside': 1, 'special': 1, 'prosecutor': 1, 'later': 1, 'Karenna': 1, 'constitution': 1, 'relatives': 1, 'Tears': 1, 'Calder': 1, 'Curious': 1, 'curious': 1, 'luggage': 1, 'flier': 1, 'Puccini': 1, 'Boheme': 1, 'indoor': 1, 'Inferno': 1, '111': 1, 'McDonald': 1, 'Wordsworth': 1, 'Budweis': 1, 'capitalism': 1, 'according': 1, 'Max': 1, 'Weber': 1, 'topped': 1, 'Bombshell': 1, '1873': 1, 'drunken': 1, 'drivers': 1, 'mayfly': 1, 'Globe': 1, 'Theatre': 1, 'burn': 1, 'sponsor': 1, 'transistors': 1, 'sacred': 1, 'Breony': 1, 'grocer': 1, 'fingertips': 1, 'S.O.S.': 1, 'Rococo': 1, 'answers.com': 1, 'albums': 1, 'Maldive': 1, 'piece': 1, 'chessboard': 1, 'Confucius': 1, 'Jett': 1, 'monsters': 1, 'Electoral': 1, 'examples': 1, 'stamps': 1, 'OJ': 1, 'pleasure': 1, 'endurance': 1, 'conscious': 1, 'Colonel': 1, 'Edwin': 1, 'Drake': 1, 'drill': 1, 'Paine': 1, 'stomach': 1, 'Alpert': 1, 'Moss': 1, 'Ancient': 1, 'osmosis': 1, 'snafu': 1, 'throw': 1, 'housewarming': 1, 'risk': 1, 'venture': 1, 'bet': 1, 'nihilist': 1, 'Belgium': 1, '1815': 1, 'dirty': 1, '1699': 1, '172': 1, 'McCain': 1, 'Rifleman': 1, 'channel': 1, 'ESPN': 1, 'mandibulofacial': 1, 'dysostosis': 1, 'amen': 1, 'Philebus': 1, 'Certified': 1, 'Nurse': 1, 'Midwife': 1, 'Missionary': 1, 'Researches': 1, '1857': 1, 'Tailors': 1, 'dummy': 1, 'degree': 1, 'Northwestern': 1, 'attic': 1, 'rice': 1, 'importers': 1, 'caber': 1, 'tossing': 1, 'Beany': 1, 'Cecil': 1, 'sailed': 1, 'sued': 1, 'Dannon': 1, 'yougurt': 1, 'Ron': 1, 'Raider': 1, 'promotion': 1, 'Ismail': 1, 'Farouk': 1, 'Body': 1, 'Asiento': 1, 'posh': 1, 'businessman': 1, 'Humor': 1, 'Cream': 1, 'Muslim': 1, 'doctors': 1, 'diagnose': 1, 'blasted': 1, 'valley': 1, 'Mojave': 1, 'fatalism': 1, 'determinism': 1, 'true': 1, 'hepcats': 1, 'isthmus': 1, 'gambler': 1, 'wants': 1, 'gets': 1, 'eels': 1, 'caucasian': 1, 'Enola': 1, 'log': 1, 'antidisestablishmentarianism': 1, 'identify': 1, 'botanical': 1, 'marvel': 1, 'Nebuchadnezzar': 1, 'Israeli': 1, 'lobsters': 1, 'shiver': 1, 'SAP': 1, 'geographic': 1, 'submerged': 1, 'fringe': 1, 'bikini': 1, 'bathing': 1, 'unusual': 1, '1951': 1, 'NBC': 1, 'Ernie': 1, 'Amelia': 1, 'Earhart': 1, 'disappeared': 1, 'Mauritania': 1, 'tribe': 1, 'Troop': 1, 'perpetually': 1, 'Leon': 1, 'Uris': 1, 'capture': 1, 'microprocessors': 1, 'microcontrollers': 1, 'deadrise': 1, 'commonplace': 1, 'rhomboideus': 1, 'minor': 1, 'panoramic': 1, 'sugar': 1, 'ones': 1, 'scene': 1, 'Ku': 1, 'Klux': 1, 'Klan': 1, 'taxed': 1, 'Leno': 1, 'Rosemary': 1, 'LaBianca': 1, 'Scientology': 1, 'bucks': 1, 'Ty': 1, 'Cobb': 1, 'pines': 1, 'Leif': 1, 'Ericson': 1, 'Parthenon': 1, 'zebras': 1, 'geoscientist': 1, 'darning': 1, 'needles': 1, 'stingers': 1, 'Imam': 1, 'Hussain': 1, 'Shia': 1, 'virtues': 1, 'Sen.': 1, 'Everett': 1, 'Dirkson': 1, '70': 1, 'Li': 1, 'l': 1, 'Abner': 1, 'Nina': 1, 'warmup': 1, 'pitches': 1, 'reliever': 1, 'Englishmen': 1, 'dunk': 1, 'circulation': 1, 'anesthetic': 1, 'allow': 1, 'cop': 1, 'Kindergarten': 1, 'Macarthur': 1, '1767': 1, '1834': 1, 'Sellers': 1, 'Bowls': 1, 'Cowardly': 1, 'Wizard': 1, 'Oz': 1, 'yo': 1, 'yos': 1, 'seccession': 1, 'snarly': 1, 'Shelleen': 1, 'Loco': 1, 'Motion': 1, 'Spock': 1, 'lump': 1, 'Galloping': 1, 'Gourmet': 1, 'veronica': 1, 'developmental': 1, 'stages': 1, 'Bligh': 1, '1960s': 1, '1970s': 1, '50s': 1, 'polis': 1, 'Minneapolis': 1, 'Dita': 1, 'Beard': 1, 'Of': 1, 'yearly': 1, 'Jinnah': 1, 'pecan': 1, 'peanut': 1, 'individual': 1, 'tested': 1, 'entire': 1, 'circumnavigator': 1, 'globe': 1, 'translations': 1, 'phrase': 1, 'Thank': 1, 'Eagle': 1, 'syndrome': 1, 'styloid': 1, 'worms': 1, 'derogatory': 1, 'applied': 1, 'painters': 1, 'Sisley': 1, 'Pissarro': 1, 'Renoir': 1, 'multicolored': 1, 'cube': 1, '42.3': 1, 'quintillion': 1, 'potential': 1, 'combinations': 1, 'managing': 1, 'Apricot': 1, 'RAM': 1, 'Stein': 1, 'Eriksen': 1, 'manatees': 1, 'Shape': 1, 'Up': 1, 'Horlick': 1, 'Hunter': 1, 'Tylo': 1, 'Jacques': 1, 'Cousteau': 1, 'Bank': 1, 'Senator': 1, 'touted': 1, 'Representative': 1, 'Abolitionists': 1, 'Kashmir': 1, 'invasion': 1, 'SCSI': 1, 'values': 1, 'denote': 1, 'Boomer': 1, 'Edition': 1, 'compiled': 1, 'eclairs': 1, 'veins': 1, 'circulatory': 1, 'vamp': 1, 'm': 1, 'jealous': 1, 'bible': 1, 'Sky': 1, 'experts': 1, 'Chernobyl': 1, 'accident': 1, 'loved': 1, 'regulation': 1, 'Maris': 1, '61': 1, 'Cash': 1, 'offered': 1, 'Preservation': 1, 'Favoured': 1, 'Struggle': 1, 'tornadoes': 1, 'Ocho': 1, 'Rios': 1, 'castellated': 1, 'Kremlin': 1, 'snowballs': 1, 'rodder': 1, 'dissented': 1, '1941': 1, 'declaration': 1, 'homeostasis': 1, 'existence': 1, 'Zebulon': 1, 'Pike': 1, 'statues': 1, 'Mauis': 1, 'NY': 1, 'Shakespearean': 1, 'Shylock': 1, 'cognac': 1, 'Calypso': 1, 'thrilled': 1, 'buds': 1, 'Snickers': 1, 'Musketeers': 1, 'impulse': 1, 'hardening': 1, 'equipment': 1, 'Uncle': 1, 'Duke': 1, 'Honey': 1, 'yohimbine': 1, 'tenants': 1, 'adjoining': 1, 'cabinets': 1, 'sprawling': 1, 'airports': 1, 'comparisons': 1, 'prices': 1, 'fabric': 1, 'Kilvington': 1, '18': 1, '327': 1, 'shelves': 1, 'Grimace': 1, 'Mayor': 1, 'McCheese': 1, 'revolutionaries': 1, 'Franz': 1, 'Country': 1, 'Doctor': 1, 'trying': 1, 'powdered': 1, 'mix': 1, 'Sawyer': 1, 'aunt': 1, 'pita': 1, 'Majal': 1, 'Indianapolis': 1, 'corporate': 1, 'Chance': 1, 'Animals': 1, 'Hispaniola': 1, 'alveoli': 1, 'Simple': 1, 'fishing': 1, 'pail': 1, 'bovine': 1, 'accused': 1, 'Trial': 1, 'Janurary': 1, 'culture': 1})\n",
      "['What', 'is', 'the', 'most', 'famous', 'German', 'word', 'in', 'English', 'language', '?', 'Where', 'are', 'leading', 'medical', 'groups', 'specializing', 'lung', 'diseases', 'actor', 'said', 'A', 'Day', 'at', 'Races', ':', '`', 'Either', 'he', \"'s\", 'dead', 'or', 'my', 'watch', 'has', 'stopped', \"''\", 'Which', 'area', 'produces', 'least', 'acidic', 'coffee', 'When', 'did', 'Princess', 'Diana', 'and', 'Prince', 'Charles', 'get', 'married', 'Asian', 'city', 'boasts', 'world', 'biggest', 'bowling', 'alley', 'a', 'handheld', 'PC', 'On', 'average', ',', 'how', 'many', 'miles', 'there', 'to', 'moon', 'was', 'Beethoven', 'born', 'name', 'of', 'chronic', 'neurological', 'autoimmune', 'disease', 'which', 'attacks', 'protein', 'sheath', 'that', 'surrounds', 'nerve', 'cells', 'causing', 'gradual', 'loss', 'movement', 'body', 'North', 'American', 'Polish', 'population', 'celestial', 'diameter', '864', '000', 'Mount', 'St.', 'Helen', 'last', 'have', 'significant', 'eruption', 'George', 'Washington', 'How', 'do', 'you', 'determine', 'heating', 'requirements', 'for', 'your', 'home', 'fear', 'strong', 'light', 'dog', 'on', 'Cracker', 'Jack', 'box', 'Colin', 'Powell', 'what', 'Gulliver', 'find', 'race', 'tiny', 'people', 'factors', 'high', 'teen', 'pregnancy', 'rate', 'Spartanburg', 'South', 'Carolina', 'two', 'largest', 'birds', 'earth', 'game', 'Garry', 'Kasparov', 'plays', 'mayor', 'made', 'so', 'TV', 'appearances', 'asked', 'join', 'AFTRA', '1984', 'Who', 'Red', 'Grange', 'Good', 'Little', 'Witch', 'Casper', 'girlfriend', 'osteichthyes', 'usenet', 'Internet', 'say', 'Grandma', 'Irish', 'term', '86ed', 'come', 'from', 'goat', 'Name', 'one', 'major', 'gods', 'Hinduism', '.', 'does', 'hurricane', 'form', 'dinosaur', 'remains', 'been', 'found', 'can', 'I', 'look', 'perpetual', 'calendar', 'life', 'expectancy', 'during', 'Stone', 'Age', 'bulbs', 'go', 'out', 'November', 'birthstone', 'Buzz', 'Aldrin', 'want', 'build', 'permanent', 'manned', 'space', 'station', 'Richard', 'Nixon', 'classical', 'Spanish', 'writer', 'warned', 'All', 'glitters', 'not', 'gold', 'microwaves', 'work', 'happened', 'Phillip', 'Taylor', 'Kramer', 'British', 'general', 'surrendered', 'colonial', 'army', 'Saratoga', 'Why', 'rooftops', 'Canada', 'green', 'first', 'poet', 'win', 'Nobel', 'Prize', 'literature', '1948', 'Rotary', 'engines', 'were', 'manufactured', 'by', 'company', 'lyricist', 'who', 'composer', 'between', 'Gilbert', 'Sullivan', 'infamous', 'pseudonym', 'Peter', 'Sutcliffe', 'Bernoulli', 'Principle', 'graced', 'airwaves', 'with', 'such', 'pearls', 'as', 'Do', 'ya', 'lo', '-', 'o', 'ove', 'me', 'Get', 'naked', 'baby', '!', 'country', 'contains', 'westernmost', 'point', 'America', 'Secretary', 'State', 'administration', 'Nostradamus', 'In', 'war', 'submarine', 'used', 'bubble', 'wrap', 'makes', 'sperm', 'ball', 'played', 'mayans', 'served', 'Logan', 'International', 'Airport', 'Soviet', 'leader', 'owned', 'Rolls', 'Royce', 'Mayo', 'Clinic', 'longer', 'an', 'elephant', 'mouse', 'shooting', 'stars', 'William', 'Styron', 'book', 'about', 'black', 'preacher', 'leads', 'slave', 'revolt', 'much', 'minimum', 'wage', '1991', 'New', 'York', 'City', 'landmark', '168', 'steps', 'its', 'crown', 'letter', 'appears', 'cold', 'water', 'tap', 'Spain', 'LAN', 'card', 'activated', 'it', 'hook', 'up', 'another', 'computer', 'without', 'using', 'HUB', 'seven', 'digits', 'follow', 'code', 'number', 'long', 'distance', 'information', 'developed', 'potlatch', 'clouds', 'Melbourne', 'Mighty', 'Mouse', 'always', 'sing', 'went', 'into', 'action', 'NECROSIS', 'mean', 'Russian', 'ambassador', 'Hungary', '1956', 'uprising', 'active', 'volcano', 'located', 'popular', 'song', 'Creeps', 'sang', 'marijuana', 'entering', 'United', 'States', 'strait', 'links', 'Mediterranean', 'Sea', 'Atlantic', 'Ocean', 'three', 'animals', 'Sheila', 'Burnford', 'The', 'Incredible', 'Journey', 'soft', 'drink', 'adults', 'method', 'called', 'fusion', 'create', 'half', 'P-32', 'Judy', 'Garland', 'date', 'birth', 'side', 'should', 'bowler', 'facing', '37803', 'split', 'hit', '3', 'pin', 'left', 'right', 'salt', 'oceans', 'killed', 'JFK', 'large', 'U.S.', 'had', 'highest', 'murder', '1988', 'band', '1960', 'points', 'Backgammon', 'board', 'Sarge', 'Steel', 'his', 'metal', 'hand', 'color', 'ash', 'World', 'War', 'start', 'MORMONS', 'believe', 'Christ', 'meat', 'business', 'because', 'became', 'known', 'underworld', 'selling', 'them', 'kangaroo', 'Natick', 'distinct', 'physical', 'characterstics', 'Arabian', 'horse', 'horsepower', 'shuttle', 'boosters', 'events', 'make', 'decathlon', 'owns', 'Louis', 'Rams', 'chefs', 'call', 'Master', 'Spice', 'second', 'weapon', 'stop', 'junk', 'snail', 'mail', 'richest', 'person', 'Italian', 'restaurant', 'be', '239', 'West', '48th', 'Street', 'Lacan', 'Orange', 'Bowl', 'Maurizio', 'Pellegrin', 'now', 'live', 'wrote', '...', 'promises', 'keep', 'before', 'sleep', 'designer', 'shoe', 'spawned', 'millions', 'plastic', 'imitations', 'jellies', \"'\", '28', 'tenses', 'causes', 'asthma', 'Much', 'Ado', 'About', 'Nothing', 'Keck', 'telescope', 'Jimmy', 'Durante', 'theme', 'orgin', 'xoxoxox', 'player', 'squats', 'times', 'baseball', 'doubleheader', 'musical', 'plea', 'Be', 'True', 'Your', 'School', 'monster', 'Spielberg', 'film', 'Jaws', 'atlas', 'map', 'online', 'no', 'charge', 'member', 'Micronauts', 'spent', '1', 'years', 'traveling', 'Microverse', 'Marvel', 'comics', 'kind', 'education', 'need', 'become', 'flight', 'attendant', 'closest', 'G2', 'Spectrum', 'Yellow', 'Dwarf', 'Earth', 'broken', 'glass', 'sharp', 'rocks', 'embedded', 'cement', 'top', 'wall', 'team', 'owner', 'sailor', 'Mouth', 'Honda', 'employ', 'sea', 'separates', 'Naples', 'Algiers', 'Ventura', 'County', 'police', 'department', 'seized', 'county', 'amount', 'cocaine', 'ever', 'meant', 'capital', 'market', 'would', 'white', 'cent', 'stamp', 'worth', 'Thomas', 'Jefferson', 'current', 'these', 'countries', 'France', 'Italy', 'Greece', 'Austria', 'Germany', 'Switzerland', 'Netherlands', 'mountain', 'portrayed', 'Maggio', 'From', 'Here', 'Eternity', '5', 'words', 'use', 'all', 'letters', 'alphabet', 'except', 'Q', 'repeats', 'names', 'seas', 'ocean', 'they', 'drain', 'perfect', 'score', 'gymnastics', 'exercise', 'know', 'if', 'am', 'pregnant', 'dew', 'barroom', 'judge', 'himself', 'Law', 'Pecos', 'Jake', 'brake', 'old', 'when', 'died', 'recruitment', 'interview', 'technique', 'spelling', 'punctuation', 'drills', '6th', 'grader', 'literal', 'meaning', 'D', 'DAY', 'Stradivarius', 'violins', 'Define', 'cosmology', 'fast', 'food', 'chain', 'golden', 'arches', 'inventor', 'silly', 'putty', 'Biloxi', 'Mississippi', 'Big', 'Bear', 'qigong', 'pronounced', 'Benelux', 'command', 'inside', 'US', 'Apache', 'helicopter', 'Kings', 'Canyon', 'flag', 'flies', 'over', 'Wake', 'Island', 'figures', 'represent', 'Easter', 'four', 'elements', '90', 'percent', 'human', 'stories', 'contained', 'Edith', 'Wharton', 'Old', 'theatrical', 'district', 'dubbed', 'Roaring', 'Forties', 'mathematician', 'won', 'Noble', 'Literature', '1950', 'Amazing', 'Three', 'masquerade', 'agent', 'orange', 'Pythagoras', 'full', '.com', 'benefits', 'rowing', 'machine', 'cable', 'network', 'bills', 'itself', 'family', 'entertainer', 'whisky', 'keeps', 'dental', 'root', 'canal', 'only', 'gland', 'humans', 'regenerate', 'president', 'government', 'civil', 'battlefield', 'Arabic', 'Numerals', '10', 'Maid', 'Rites', 'year', 'movie', 'Ten', 'Commandments', 'released', 'blue', 'balls', 'Bend', 'President', 'wear', 'Nazi', 'uniform', 'origin', 'Army', 'sergeant', 'stripes', 'bog', 'format', 'VHS', 'main', 'competition', 'California', 'state', 'bird', 'everything', 'shot', 'Lee', 'Harvey', 'Oswald', 'approximate', 'Las', 'Vegas', 'N.M', 'Sydney', 'Porter', 'writing', 'Gift', 'Magi', 'Iron', 'Man', 'armor', 'Samuel', 'Johnsons', 'friend', 'biographer', 'vaccine', 'chicken', 'pox', 'populous', 'covers', '49', '576', 'square', 'software', 'offers', 'inventors', 'CAD', 'like', 'design', 'someone', 'federal', 'Bible', 'tell', 'Jesus', 'brothers', 'sisters', 'leaders', 'Byzantine', 'empire', 'Hitler', 'chancellor', 'airplane', 'Ernest', 'Hemingway', 'eyes', 'King', 'Swing', 'runs', 'Andy', 'Capp', 'favorite', 'pub', 'famed', 'London', 'criminal', 'court', 'once', 'feudal', 'castle', 'Stanford', 'University', 'common', 'livestock', 'greatest', 'variety', 'breeds', 'sings', 'themes', 'Dawson', 'Creek', 'Felicity', 'Huckleberry', 'Finn', 'snack', 'ridges', 'CNN', 'stand', 'Heimlich', 'maneuver', 'peak', 'lived', 'under', 'six', 'flags', 'her', 'real', 'tall', 'animated', 'Frankenstein', 'Tulip', 'Festival', 'Michigan', 'Randy', 'Craft', 'lawyer', 'wop', 'Latin', 'battle', 'cry', 'Ad', 'arma', 'ad', 'contemptible', 'scoundrel', 'stole', 'cork', 'lunch', 'web', 'address', 'list', 'e', 'addresses', 'members', 'House', 'Representatives', 'Democratic', 'prankster', 'waved', 'train', 'while', 'spoke', 'caboose', 'tumbled', 'marble', 'Socratic', 'hot', 'oven', 'making', 'Peachy', 'Oat', 'Muffins', 'part', 'their', 'attire', 'pothooks', 'cowboys', 'Pittsburgh', 'appeared', 'Corner', 'Drug', 'Store', 'Waco', 'Texas', '1885', 'Sao', 'Paulo', 'Brazil', 'oldest', 'Americas', 'Scandinavian', '173', '732', 'Freidreich', 'Wilhelm', 'Ludwig', 'Leichhardt', 'Prussian', 'explorer', 'school', 'Hank', 'Aaron', 'Stewart', 'Howard', 'K.', 'Smith', 'borders', 'others', 'French', 'seaport', 'claims', 'Home', 'Wines', 'Barbara', 'Jordan', 'John', 'F.', 'Kennedy', 'sister', 'cullion', 'female', 'figure', 'skater', 'answers', 'questions', 'synonym', 'aspartame', 'ruin', 'G.M.T.', 'fictional', 'character', 'melancholy', 'Dane', 'grass', 'MSG', 'bread', 'feature', 'stickers', 'Cisco', 'Kid', 'ends', 'packages', 'casinos', 'NJ', 'official', 'Algeria', 'tender', 'resignation', 'handicraft', 'requires', 'interlace', 'warp', 'weft', 'permutations', 'c', 'saw', 'naval', 'luxury', 'liners', 'Cap', 'Trafalgar', 'Carmania', 'cars', 'rotary', 'Benny', 'Carter', 'also', 'supreme', 'justice', 'invented', 'Shostakovich', 'write', 'Rostropovich', 'Independent', 'silversmith', 'account', 'percentage', 'silver', 'production', 'difference', 'khaki', 'chino', 'showed', 'fondness', 'munching', 'bee', 'pollen', 'bars', 'television', 'watchers', 'sites', 'linked', 'Report', 'Genesis', 'Eldercare', 'classic', 'books', '5th', 'graders', 'read', 'Zorro', 'ride', 'laundry', 'detergent', 'Rule', 'Act', 'piano', 'music', 'Jamiroquai', 'Everyday', 'midi', 'Petersburg', 'Petrograd', 'title', 'role', 'My', 'Favorite', 'Martian', 'cities', 'Utah', 'hives', 'cranberry', 'bogs', 'economic', 'impact', 'unemployment', 'economy', 'best', 'ways', 'improve', 'employee', 'morale', 'low', 'cost', 'Jews', 'executed', 'concentration', 'camps', 'WWII', 'December', '1945', 'jeep', 'collided', 'truck', 'short-', 'effects', 'underage', 'drinking', 'organism', 'lives', 'dropped', '313', 'feet', '1980', 'correct', 'way', 'mount', 'roll', 'toilet', 'paper', 'candle', 'we', 'blow', 'woman', 'carried', 'multiple', 'births', 'twins', 'triplets', 'etc', 'Yankee', 'frequent', 'enemies', 'swap', 'math', 'penned', 'Neither', 'borrower', 'nor', 'lender', 'fresh', 'islands', 'Fiji', 'Babe', 'Ruth', 'oil', 'almost', 'picked', 'stalled', 'car', 'Japanese', 'new', 'international', 'Wonderbra', 'Mao', 'different', 'types', 'Fahrenheit', 'equivalent', 'zero', 'degrees', 'centigrade', 'opposite', 'faces', 'die', 'add', 'created', 'Harry', 'Lime', 'stat', 'expression', 'quickly', 'Six', 'fought', 'seafaring', 'southern', 'tip', 'tallest', 'building', 'Isle', 'Illinois', 'north', 'plant', 'button', 'cap', 'cup', 'gills', 'ring', 'college', 'league', 'Chinese', 'emperor', 'present', 'soldier', 'Gordon', 'recognition', 'services', 'quelling', 'rebellions', 'nickname', 'companies', 'Dow', 'Jones', 'Herbert', 'Hoover', 'softest', 'Oscars', 'Academy', 'Awards', '1999', 'latitude', 'longitude', 'El', 'Paso', 'red', 'barbershop', 'pole', 'arch', 'see', 'Place', 'de', 'la', 'Concorde', 'burst', 'through', 'screen', 'Lite', 'beer', 'commercials', 'designed', 'Bridge', 'put', 'James', 'limelight', 'time', 'take', 'type', 'screenplay', 'dumb', 'but', 'loveable', 'Maurice', 'Gosfield', 'play', 'Phil', 'Silvers', 'Show', 'town', 'chartered', 'Vermont', 'telephones', 'easily', 'remove', 'wine', 'stains', 't', 'shirts', 'cheery', 'fellow', 'got', 'ZIP', '9971', 'Postal', 'Service', '1963', 'future', 'dictator', 'training', 'priest', 'turned', 'Marxism', 'novel', 'Brother', 'watching', 'Count', 'Cinzano', 'powerful', 'trader', 'Conrad', 'Heart', 'Darkness', 'will', 'kid', 'eat', 'middle', 'save', 'chocolate', 'outside', 'star', 'down', '$', 'contract', 'felt', \"n't\", 'earned', '19th', 'century', 'painter', 'Marquesas', 'Islands', 'Archimedes', 'bordering', 'due', 'Costa', 'Rica', 'day', 'week', 'Dan', 'Aykroyd', 'Belushi', 'quit', 'Saturday', 'Night', 'Live', 'comediennes', 'characters', 'include', 'former', 'Nora', 'Desmond', 'secretary', 'Mrs.', 'Wiggins', 'housewife', 'named', 'Eunice', 'gave', 'Arthur', 'round', 'table', 'Tokyo', 'level', 'FBI', 'Public', 'Enemy', 'No', 'speed', 'magnet', 'spins', 'generator', 'affect', 'output', 'temperature', 'baking', 'video', 'alternative', 'narcolepsy', 'useful', 'site', 'measures', 'Clinton', 'avoid', 'draft', 'money', 'Marcos', 'steal', 'road', 'traffic', 'cone', 'Cawdor', 'Castle', 'Glamis', 'Blair', 'railways', 'Monopoly', 'Herb', 'Tootsie', 'next', 'door', 'habitat', 'chickadee', 'Philippine', 'ex', 'treasury', 'PLO', 'executor', 'eggs', 'supposed', 'stored', 'small', 'end', 'surname', 'Braun', 'fences', 'good', 'neighbors', 'DSL', 'chloroplasts', 'terry', 'cloth', 'insanity', 'planet', 'Ewoks', 'men', 'appear', 'serial', 'tv', 'Muppets', 'alternate', 'definition', 'graphic', 'details', 'Hamlet', 'faring', 'brought', 'Inhumans', 'important', 'nation', 'historically', 'following', 'celebrities', 'started', 'show', 'biz', 'career', 'disc', 'jockey', 'Polynesian', 'inhabit', 'Zealand', 'Answers.com', 'KnowPost.com', 'shape', 'shifting', 'menace', 'Rom', 'fight', 'fuel', 'airplanes', 'Pheonix', 'Club', 'states', 'enclose', 'Chesapeake', 'Bay', 'hosted', 'Winter', 'Olympics', 'Asia', 'per', 'capita', 'income', 'Colombia', 'happens', 'lightning', 'strikes', 'effect', 'LSD', 'brain', 'Edessa', 'nowadays', 'often', 'replaced', 'case', 'Americans', 'Disabilities', '199', 'creatures', 'Canary', 'Nazis', 'occupy', 'CD', 'NNS', 'IN', 'NNP', 'Harrison', 'tune', 'sounded', 'too', 'He', 'So', 'Fine', 'Chiffons', 'www.questions.com', 'going', 'open', 'Boxing', 'celebrated', 'detective', 'Punchbowl', 'Hill', '11', 'children', 'Romans', 'mare', 'nostrum', 'Billy', 'sci', 'fi', 'trilogy', 'Foundation', 'Empire', 'Second', 'Dicken', 'Gas', 'Gang', 'NASA', 'net', 'sees', 'women', 'amateur', 'champions', 'receive', 'Uber', 'Cup', 'buy', 'movies', 'videotape', 'Folies', 'Bergeres', 'constellation', 'represents', 'hunter', 'club', 'shield', 'boat', 'gopher', 'wood', 'represented', 'Steven', 'pitcher', 'rubber', 'tout', 'suite', 'chances', 'pregnacy', 'penis', 'penetrate', 'vagina', 'games', 'Windows', '95', '98', 'idealab', 'waste', 'dairy', 'cow', 'produce', 'clean', 'cache', 'database', 'birthday', 'allowed', 'claim', 'contibution', 'tax', 'purposes', 'royal', 'wedding', 'Andrew', 'Fergie', 'place', 'commanders', 'directed', 'forces', 'Battle', 'Alamein', 'park', 'Firehole', 'River', 'Fairy', 'Falls', 'Russians', 'landed', 'typical', 'bathroom', 'costume', 'decided', 'Michael', 'Jackson', 'glove', 'buses', 'far', 'away', 'wallpaper', 'fireplug', 'hourly', 'workers', 'swimmer', 'medals', '1972', 'deranged', 'super', 'Otto', 'Octavius', 'uses', 'F', 'Jackie', 'SHIELD', 'fossilizes', 'coprolite', 'governor', 'Virginia', 'sometimes', 'Gotham', 'hate', 'Wide', 'Web', 'WWW', 'troilism', 'island', 'Battery', 'chairbound', 'basophobic', 'broadcast', 'singer', 'Freedy', 'Johnston', 'dingoes', '155', 'Leonardo', 'da', 'Vinci', 'Michaelangelo', 'Machiavelli', 'working', 'Joe', 'DiMaggio', 'compile', '56', 'hitting', 'streak', 'tennis', 'Stefan', 'Edberg', 'acreage', 'Chappellet', 'vineyard', 'chairman', 'Senate', 'select', 'committee', 'tried', 'bottom', 'Watergate', 'store', 'Shakespeare', 'Antonio', 'borrow', '0', 'ducats', 'nutrients', 'healthy', 'bones', 'teeth', 'Lucy', 'Linus', 'Peanut', 'comic', 'strip', 'Einstein', 'IQ', 'explorers', 'followed', 'Columbus', 'ship', 'Lloyd', 'M.', 'Bucher', 'inches', 'Stuart', 'eye', 'hates', 'mankind', 'Burma', 'epicenter', 'Australia', 'candy', 'cane', 'Christmas', 'muscles', 'adult', 'walk', 'translation', 'caliente', 'Greek', 'God', 'Rupee', 'Depreciates', 'debts', 'Qintex', 'group', 'leave', 'Gandhi', 'airport', 'Los', 'Angeles', 'Linux', 'Caroll', 'Baker', 'Tammy', 'Grimes', 'Debbie', 'Reynolds', 'erupts', 'every', 'hour', 'Yellowstone', 'National', 'Park', 'Craig', 'Stevens', 'fastener', 'Whitcomb', 'Judson', 'patent', '1893', 'lifelong', 'evil', 'comes', 'alphabetically', 'snowboard', 'less', 'than', '200', 'properties', 'Lucelly', 'Garcia', 'Columbia', 'Honduras', 'abbreviation', 'limited', 'partnership', 'doctor', 'synonymous', 'footwear', 'foot', 'care', 'rivers', 'Europe', 'weight', 'teaspoon', 'matter', 'hole', 'university', 'library', 'Southern', 'after', 'Edgar', 'Rice', 'Burroughs', 'being', 'element', 'strontium', 'purified', 'poem', 'starts', 'love', 'lips', \"'re\", 'wet', 'warm', 'desire', 'nationality', 'Pope', 'Paul', 'II', 'adorns', 'Rwanda', 'months', 'windmills', 'concerts', 'held', 'this', 'Homelite', 'Inc.', 'page', 'fowl', 'grabs', 'spotlight', 'Year', 'Monkey', 'hazmat', 'nicknamed', 'infectious', 'kills', 'worldwide', 'led', 'Normans', 'victory', 'Hastings', 'Indian', 'prime', 'minister', 'beating', '1977', 'election', 'task', 'Bouvier', 'breed', 'perform', 'propellers', 'helped', 'power', 'plane', 'Wright', 'flew', 'history', 'Untouchables', 'Kevin', 'Costner', 'involves', 'Sioux', 'Indians', 'river', 'Hernando', 'Soto', 'European', 'Mars', 'highway', 'death', 'toll', '1969', 'sport', 'numbers', 'relevant', '118', '126', '134', '142', '15', '158', '167', '177', '19', 'Grinch', 'Stole', '2th', 'volume', 'biography', 'Abraham', 'Lincoln', 'ages', 'lingo', 'bottle', 'Victorian', 'novelist', 'post', 'office', 'multiplexer', 'pony', 'daughter', 'smoking', 'problem', '1997', 'Was', 'Teenage', 'Werewolf', 'Alvin', 'Stardust', 'colorful', 'region', 'legend', 'Amazons', 'near', 'export', 'hero', 'some', 'fans', 'Chomper', '1981', 'Candice', 'Bergen', 'Jacqueline', 'Bisset', 'remake', '1943', 'Acquaintance', 'Windsor', 'seventh', 'cousins', 'removed', 'claimed', 'toy', 'ill', 'fated', 'craft', 'captained', 'Ernst', 'Lehmann', 'air', 'southeast', 'Wang', 'joining', 'Ping', 'Tak', 'landlocked', 'olives', 'martinis', '1920s', 'cowboy', 'rode', 'Tony', 'Wonder', 'Horse', 'spoken', 'condition', 'hypertension', 'likes', 'fire', 'presidential', 'press', 'dismissed', 'third', 'burglary', 'attempt', 'professional', 'sports', 'originated', 'Sam', 'Casablanca', 'Helens', 'foods', 'contain', 'organized', 'Confederate', 'veterans', 'social', 'Pulaski', 'Tennessee', '1866', 'Cleveland', '24', 'Sussex', 'Drive', 'Ottawa', 'Orly', 'serve', 'deadly', 'sins', 'Remembrance', 'recipe', 'Eggs', 'Benedict', 'Filenes', 'Cid', 'Low', 'Countries', 'maximum', 'clubs', 'golfer', 'may', 'symbolize', 'truth', 'caused', 'Titanic', 'sink', 'ethnological', 'museum', 'man', 'drugs', 'treat', 'tuberculosis', 'Bob', 'Barr', 'representative', 'Georgia', 'Civil', 'wreaked', 'havoc', 'south', 'marching', 'football', 'bowl', 'queen', 'Hallie', 'Woods', 'starred', 'Sharon', 'Arnold', 'Schwarzenegger', 'Smithsonian', 'Institute', 'setting', 'Le', 'Carre', 'Small', 'Town', 'wings', 'flea', 'land', 'films', 'studios', 'Europeans', 'Oceania', 'Narragansett', 'other', 'tribes', 'Rhode', 'Dr.', 'Israel', 'begin', 'turning', 'Gaza', 'Strip', 'Jericho', 'season', 'begins', 'vernal', 'equinox', 'Butterfield', '8', 'style', 'Chicago', 'pizza', 'scientific', 'calculator', 'introduced', 'commercially', 'Bayer', 'A.G.', 'Leverkusen', '1899', 'pointed', 'handwriting', 'analyst', 'sizes', 'colors', 'Beanie', 'Baby', 'Bella', 'Abzug', 'sartorial', 'trademark', 'Cartier', 'invent', 'aviator', 'Santos', 'Dumont', '1940', 'Betty', 'Boop', 'cactus', 'wide', 'Ray', 'playing', 'instrument', 'vice', 'Alexander', 'Hamilton', 'duel', 'Wanna', 'Go', 'Riots', 'occur', 'expensive', 'Rockefeller', 'JDR3', 'cat', 'scratch', 'fever', 'ask', 'total', 'stranger', 'conifer', 'organ', 'transplants', 'more', 'successful', 'today', 'bears', 'Goldilocks', 'Sara', 'Jane', 'Moore', 'try', 'assassinate', 'Jackal', 'Williams', 'devoured', 'mob', 'starving', 'declared', 'jean', 'destructor', 'It', 'destroying', 'creativity', 'must', 'inkhorn', 'terms', 'Star', 'Wars', 'satellites', 'Grand', 'Slam', 'golf', 'tournament', 'wasn', 'review', 'Nightmare', 'Elm', 'journal', 'populated', 'servers', 'Approximately', 'weigh', 'fruit', 'ficus', 'director', 'commandant', 'POW', 'camp', '1953', 'Stalag', '17', 'legged', 'creature', 'Cornell', 'study', 'companion', 'convicted', 'Lai', 'Massacre', 'African', 'Melissa', 'widely', 'detect', 'defects', 'parasites', 'apartments', 'Saint', 'Brunswick', 'lengths', 'pearl', 'necklaces', 'badly', 'tarnished', 'brass', 'haunted', 'houses', 'Islamic', 'counterpart', 'Cross', 'Sinatra', 'dooby', 'Silent', 'fifth', 'loosely', 'based', 'DC', 'Justice', 'League', 'classification', 'lady', 'bug', 'lack', 'reach', 'dialing', '22', '287', 'shoplifts', 'thought', \"'d\", 'never', 'lovely', 'tree', 'incident', 'canning', 'summit', 'conference', 'Eisenhower', 'Khrushchev', 'ecological', 'niche', 'line', 'grow', 'yet', 'actress', 'appearance', 'stage', 'age', 'five', '191', 'model', 'Joel', 'blood', 'SED', 'Castor', 'Pollux', 'Jude', 'acted', 'match', 'security', 'Berlin', 'Wall', 'infatuation', 'Look', 'Love', 'viewing', 'Ursula', 'Andress', 'Marbella', 'western', 'Silver', 'Fox', 'Valdez', 'Principles', 'supports', 'Badaling', 'turret', 'Spade', 'Lawn', 'Tennis', 'Challenge', 'Trophy', 'usually', 'friction', 'motion', 'Mr.', 'Magoo', 'flog', 'General', 'Electric', 'January', '12', 'jury', 'cases', 'mission', 'Nike', 'intranet', 'newsmen', 'Rhodes', 'scholar', 'rites', 'accompanying', 'circumcision', 'newly', 'child', 'Judaism', 'III', 'airliners', 'crash', 'vs.', 'gliding', 'lowest', 'elevation', '6', 'Triple', 'Crown', 'winners', 'lake', 'Sheboygan', 'Jellicle', 'Cat', 'products', 'tiger', 'symbol', '2', 'formula', 'Coca', 'Cola', 'freckles', 'Howdy', 'Doody', 'face', 'Zenger', 'deveopment', 'newspaper', 'labor', 'seen', 'parking', 'lot', 'Madeira', 'Sinn', 'Fein', 'recommended', 'male', 'bees', 'horses', 'polo', 'Paleozoic', 'era', 'Nino', 'results', 'cooling', 'temperatures', 'very', 'dry', 'weather', 'Manchester', 'spend', 'players', '1993', 'Alley', 'Oop', 'Moo', 'Line', 'panel', 'object', 'oriented', 'betting', 'racing', 'Adventures', 'Robin', 'Hood', 'Jean', 'Nicolet', 'Yukon', 'empty', 'Enterprise', 'Trek', 'Scarlet', 'Letter', 'Chablis', 'gambling', 'conjugations', 'wake', 'woke', 'emblazoned', 'Jolly', 'Roger', 'travel', 'outer', 'Clara', 'Peller', 'paid', 'Wendy', 'beef', 'range', 'marks', 'border', 'art', 'growing', 'miniature', 'trees', 'flows', 'past', 'Temple', 'Karnak', 'energy', 'Bang', 'revolutions', 'standard', 'LP', 'minutes', 'Tom', 'Selleck', 'frames', 'disk', 'camera', 'shoot', 'gender', 'bigger', 'thighs', 'Denver', 'million', 'gallon', 'spill', 'Congress', 'Vienna', 'establish', 'rust', 'clothing', 'donation', 'process', 'entail', 'lions', 'Jungle', 'Jules', 'Verne', 'features', 'scientists', 'captive', 'Nautilus', 'predict', 'observing', 'Leave', 'Beaver', 'syringe', 'medicinal', 'damage', 'bureaucracy', 'plagued', 'Africa', 'salary', 'paleontologist', 'outcome', 'Yalta', 'Conference', 'coach', 'story', 'told', 'Run', 'Daylight', 'fountain', 'erupt', 'Rascals', 'bank', '7847', '+', '5943', 'equal', 'Nitrox', 'diving', 'legal', 'rights', 'automobile', 'repossession', 'unfamiliar', 'celebrity', 'spokespeople', 'promote', 'product', 'clause', 'Constitution', 'changed', 'altered', 'amended', '4', 'foot-9', 'performer', 'Oscar', 'sex', 'Mikhail', 'Gorbachev', 'initial', 'clown', 'early', 'insisted', 'Clarabell', 'Brunei', 'cards', 'dealt', 'each', 'Gin', 'Rummy', 'lakes', 'cathedral', 'Becket', 'murdered', 'spouting', 'perfectly', 'languages', 'natives', 'Afghanistan', 'anus', 'rectum', 'businesses', 'constitutes', 'Etta', 'Butch', 'Cassidey', 'Sundance', 'exchange', 'Australian', 'thank', 'notes', 'ticket', 'fare', 'Cairo', 'Barbados', 'boy', 'Christopher', 'Marlowe', 'literary', 'contributions', 'seizure', 'eleven', 'Simpsons', 'imposed', 'Blockade', '600', '387', 'achievements', 'Wolfman', 'jail', 'just', 'Taiwanese', 'correctly', 'pronounce', 'Quetzalcoatl', 'cigarette', 'whole', 'monarch', 'lap', 'P.T.', 'Barnum', 'Thumb', 'sit', 'medium', 'Hamblen', 'considered', 'singing', 'pyrotechnic', 'display', 'murdering', 'whores', 'shan', 'ripping', 'field', 'dentist', 'Beers', 'rules', 'Vietnam', 'ultraviolet', 'DNA', 'skin', 'ranking', 'suit', 'bridge', 'room', 'W.C.', 'Fields', 'private', 'Philip', 'Make', 'peacocks', 'mate', 'adventuring', 'Rann', 'Adam', 'Strange', 'profession', 'predicted', 'topple', '2010', '2020', 'cd', 'Eminem', 'slim', 'shady', 'Is', 'Give', 'reason', 'oftentimes', 'dropping', 'odors', 'ancients', 'great', 'yellow', 'hiding', 'facial', 'scars', 'Chile', 'Sherlock', 'Holmes', 'archenemy', 'function', 'community', 'tower', 'breweries', 'ten', 'amendements', 'passed', 'distinctive', 'palmiped', 'universe', 'amphibians', 'began', 'East', 'contest', '192', 'extant', 'hockey', 'Ronald', 'Reagan', 'joke', 'Woody', 'Woodpecker', 'cartoon', 'Semper', 'Fidelis', 'celtic', 'sum', 'genetic', 'material', 'given', 'nose', 'tackle', 'Eagles', 'Super', 'XV', 'biblical', 'stones', 'Pit', 'Pendulum', 'cooler', 'cucumber', 'Spumante', 'Pollock', 'Romantic', 'Englishwoman', 'founded', 'freed', 'slaves', '1847', 'chapter', 'verses', 'killer', 'industrialized', 'digitalis', '1830', 'rocket', 'launched', 'Surveyor', 'spacecraft', 'stock', 'Way', 'written', 'Anka', 'articles', 'tokens', 'boats', 'float', 'flavors', 'ice', 'cream', 'Johnson', 'Virtual', 'Desk', 'Reference', 'urged', 'us', 'Come', 'where', 'flavor', 'tragic', 'Roy', 'Rogers', 'schools', 'included', 'Florida', 'Ice', 'Hockey', 'program', 'Robert', 'Stevenson', 'inspired', 'Deacon', 'Brodie', 'cabinetmaker', 'burglar', 'night', 'toothbrush', 'Tyrannosaurus', 'Rex', 'witch', 'hazel', 'dressed', 'Santa', 'Claus', 'affair', 'Shelley', 'Winters', '1994', 'Baskin', '&', 'Robbins', 'offer', 'milligrams', 'gram', 'cyclone', 'Jersey', '8/28/1941', 'layers', 'Yoo', 'Hoo', 'settle', 'Sierra', 'Nevada', 'mountains', '500', 'airforce', 'troops', 'Queen', 'Pacific', 'gallery', 'bith', 'Superman', 'organization', 'Security', 'Council', 'batteries', 'Connecticut', 'Dikembe', 'Mutombo', 'basketball', 'knight', 'websites', 'Motto', 'Maryland', 'ago', 'Anglican', 'church', 'Vatican', 'AD', '999', 'sort', 'celebrations', 'fears', 'divided', 'system', 'pairs', 'legs', 'lobster', 'Ethel', 'revolve', 'around', 'weird', 'dinner', 'fish', 'knife', 'originate', 'non', 'mechanical', 'achieves', 'speeds', 'Dixville', 'Notch', 'SVHS', 'Ireland', 'then', 'return', 'purple', 'Ermal', 'Fraze', 'thousands', 'students', 'attend', 'Massachusetts', 'Barton', 'Blackhawk', '1832', 'loud', 'thunder', 'Cabarnet', 'Sauvignon', 'CPR', 'pushed', 'coupled', 'hump', 'yards', 'megawatts', 'project', 'Indonesia', 'built', 'consortium', 'headed', 'Mission', 'Energy', 'Lauren', 'Bacall', 'husband', 'blind', 'sculptress', 'Fantastic', 'Four', 'Thing', 'Lund', 'Hungarian', 'Lindbergh', 'skunks', 'MiG', 'chemicals', 'lethal', 'injection', 'Dubliners', 'own', 'homes', 'complete', 'trip', 'abbreviated', 'Bureau', 'Investigation', 'juices', 'Hawaiian', 'Punch', 'Chivington', 'Ivan', 'IV', 'support', 'expansion', 'boxing', 'nine', 'scar', 'faced', 'bounty', 'direction', 'pitchers', 'pitch', 'toward', 'define', 'prison', 'Ossining', 'Inuit', 'Eskimo', 'cystic', 'fibrosis', 'hurdle', 'front', 'runner', 'steeplechase', 'jump', 'Java', 'Edison', 'Museum', 'BTU', 'conditioning', 'operant', 'heavyweight', 'boxer', 'Wild', 'Bull', 'Pampas', 'engine', 'cough', 'syrup', 'drive', 'medication', 'Grammys', '1983', 'consecutive', 'Lou', 'Gehrig', 'Naseem', 'Hamed', 'Queensland', 'Competition', 'Policy', 'frequency', 'VHF', 'amicable', 'Poconos', 'mevacor', 'close', 'cousin', 'Franklin', 'D.', 'Theodore', 'Roosevelt', 'Ouija', 'Boards', 'combatting', ';', 'think', 'brown', 'god', 'sullen', 'untamed', \"'ll\", 'milk', 'elk', 'For', 'San', 'Diego', 'serving', 'apples', 'sinning', 'set', 'remote', 'control', 'reign', 'hive', 'bloodhound', 'Englishman', 'Hawkins', 'colonists', '1562', 'feminist', 'Sexual', 'Politics', 'Flying', 'species', 'caffeine', '16', 'oz', 'Black', 'Hills', 'Dakota', 'tourist', 'attraction', 'big', 'quart', 'commanded', 'Orleans', 'milliseconds', 'Hot', 'Wheels', 'produced', 'Protestant', 'against', 'supremacy', 'attracts', 'tourists', 'Reims', 'captain', 'political', 'party', '1922', 'run', '.tbk', 'file', 'holds', 'NFL', 'record', 'touchdowns', 'cologne', 'associated', 'Langston', 'Hughes', 'sung', 'Banana', 'Pajamas', 'Playboy', 'continuing', 'dialog', 'contemporary', 'issues', 'readers', 'holidays', 'observances', 'Shevardnadze', '1603', 'Native', 'Kafka', 'Metamorphosis', 'awakes', 'morning', 'England', 'carries', 'telephone', '27', 'cross', 'heart', 'bra', 'escape', 'gravity', '%', 'handle', 'hungry', 'majority', 'whip', 'Truman', 'MLA', 'bibliographies', 'source', 'Ozymandias', 'separated', 'Bering', 'Strait', 'sonnets', 'chemical', 'composition', 'Barbie', 'silent', 'sound', 'version', '1849', 'discovered', 'Sutter', 'Mill', 'Hibernia', 'any', 'sided', 'Doonesbury', 'likely', 'turn', 'werewolf', 'scale', 'earthquakes', 'Homerian', 'epic', 'chronicles', 'Trojan', 'march', 'Macy', 'Thanksgiving', 'Parade', '1812', 'Mozambique', 'narrates', 'Treasure', 'By', 'gas', 'rise', '2000', 'historical', 'relationship', 'Catholic', 'Church', 'bingo', 'send', 'transcript', 'Hollywood', 'David', 'Letterman', 'CTBT', 'coppertop', 'battery', 'bibliography', 'cured', 'cumin', 'certain', 'Antigua', 'Sunday', 'determined', 'flyer', 'mistakenly', 'instead', 'gallons', 'Niagra', 'platinum', 'cardinal', 'incubate', '2.5', 'billion', 'little', 'since', '1935', 'Darth', 'Vader', 'son', 'PSI', 'Fifth', 'Amendment', 'Bill', 'Rights', 'Marilyn', 'Monroe', 'independent', 'silversmiths', 'inaugurated', 'Radio', 'Varian', 'Associates', 'sell', 'vacuum', 'division', 'BOC', 'opens', 'assassin', 'tumbling', 'Seattle', 'Space', 'Needle', 'aborigines', 'featured', 'Shirley', 'MacLaine', 'prostitute', 'Lemmon', 'pimp', 'pedometer', 'measure', 'Individuals', 'Education', 'let', 'volley', 'popcorn', 'pop', 'GUM', 'Philadelphia', 'visit', 'Cleopatra', 'hat', 'Jay', 'Kay', 'wears', 'magnets', 'attract', 'Charlie', 'Angels', 'vocals', 'Josie', 'Pussycats', 'purpose', 'SysRq', 'key', 'keyboard', '1642', '1649', 'phalanx', 'Furth', 'Rednitz', 'Pegnitz', 'converge', 'habeas', 'corpus', 'canker', 'sores', 'mouth', 'wars', 'armed', 'conflicts', 'continent', 'upper', 'corner', 'Budweiser', 'label', 'jiggy', 'getting', 'verdandi', 'paying', 'odds', 'roulette', '1942', 'espionage', 'reunited', 'Huston', 'Maltese', 'Falconers', 'Humphrey', 'Bogart', 'Mary', 'Astor', 'Sidney', 'Greenstreet', 'Hillary', 'graduate', 'producing', 'acronym', 'Mae', 'battleship', 'veal', 'roasts', 'chops', 'cause', 'stem', 'monitor', 'SVGA', 'adapter', 'Wee', 'Willie', 'Winkie', 'reptiles', 'larger', 'size', 'Statue', 'Liberty', 'Baretta', 'cockatoo', 'Compaq', 'statement', 'stronger', 'vitreous', 'carbon', 'Technology', 'compared', 'cellulose', 'opposition', 'Konrad', 'Adenauer', 'Chancellor', 'O.J.', 'Simpson', 'Rose', 'Navy', 'gunboat', 'Sand', 'Pebbles', 'Planet', 'First', 'Iran', 'Contra', 'investigation', 'Chilly', 'Willy', 'sides', 'scalene', 'triangle', 'album', 'Budapest', 'Belgrade', 'caldera', 'Korean', 'Kimpo', 'Nicholas', 'Cage', 'leukemia', 'witness', 'hearings', 'cotton', 'textiles', 'importer', 'overcome', 'giants', 'twelve', 'Travels', 'Superbowls', 'ers', 'whatever', 'available', 'catalogues', 'DT', 'NN', 'topophobic', 'suffer', 'deepest', 'Arctic', 'lead', 'Sleepless', 'Benjamin', 'development', 'Shalom', 'transport', 'files', 'primate', 'pigment', 'palms', 'hands', 'takes', 'typist', '100', 'pages', 'Tesla', 'protagonist', 'Dostoevski', 'Idiot', 'Medieval', 'Guild', 'syllables', 'hendecasyllabic', 'poetry', 'done', 'snoring', 'Camptown', 'Racetrack', 'Gross', 'email', 'spaceball', 'Pia', 'Zadora', 'millionaire', 'mentor', 'Arcadia', 'communications', 'yachts', 'share', 'Khyber', 'Pass', 'struck', 'society', 'ruled', 'elders', 'feathered', 'Yugoslavians', 'Vlaja', 'Gaja', 'Raja', 'busiest', 'Amtrak', 'rail', 'stations', 'Unsafe', 'Any', 'Speed', 'Gene', 'Siskel', 'Venus', 'flytrap', 'snake', 'EKG', 'senses', 'develops', 'brand', 'jeans', 'Calvin', 'Klein', 'admit', 'comfortable', 'wearing', '=', 'mc2', 'sideburns', 'didn', 'rule', 'spread', 'until', 'industrial', '26', '1978', 'Kenyan', 'safari', 'center', 'vending', 'machines', 'dollar', 'Iberia', 'Pilot', 'spice', 'pay', 'Johnnie', 'Walker', 'Label', 'aged', 'beans', 'ancient', 'refuse', 'policy', 'forced', 'China', 'nations', 'trading', 'citizen', 'awarded', 'Albert', 'Medal', 'Society', 'Arts', 'dialogue', 'circle', 'cartoons', 'Larry', 'job', 'Dudley', 'Right', 'micro', 'mixing', 'gin', 'vermouth', 'snow', 'fired', 'Maria', 'Ybarra', 'position', 'council', 'McCarren', 'Montenegro', 'geographical', 'father', 'Sally', 'Dick', 'Van', 'Dyke', 'Charley', 'Hustle', 'Internet2', 'deadliest', 'Brown', 'Days', 'Steinbeck', 'alcoholic', 'pomegranate', 'juice', 'To', 'Microsoft', 'owe', 'success', 'fix', 'squeaky', 'floors', 'woo', 'Wu', 'dialect', 'waterways', '1.76', 'order', 'Ranger', 'Yogi', 'elect', 'bell', 'Mountain', 'mass', 'exposed', 'granite', 'hard', 'hearing', 'artist', 'painted', 'Sunflowers', 'despondent', 'Freddie', 'Prinze', 'anymore', 'Shawn', 'retirement', 'T.V.', 'could', 'Terrific', 'Adventours', 'Tours', 'FCC', 'Newton', 'Minow', 'declare', 'May', '9', '1961', 'Candlemas', 'document', 'copy', 'placed', 'head', 'upon', 'burial', 'annual', 'boilermaker', 'Pickering', 'Nathan', 'Hamill', 'prequel', 'nitrogen', 'proud', 'Volcano', 'drug', 'Sinemet', 'treatment', 'Olympic', 'motto', 'hair', 'fur', 'Hub', 'Plc', 'communist', 'Mexico', 'Olympia', 'overlook', 'Rev.', 'Jerry', 'Falwell', 'buried', 'ancestral', 'overlooking', 'Hudson', 'Hyde', 'worked', 'Manhatten', 'Project', 'crocodile', 'swallow', 'Pan', 'menstruation', 'Bounty', 'mutiny', 'Waugh', 'Handful', 'Dust', 'mad', 'force', 'afternoon', 'Marley', 'manufacturer', 'both', 'pianos', 'motorcycles', 'meters', 'mile', 'records', 'rainfall', 'advertising', 'banned', 'radio', 'Alaska', 'affected', 'Exxon', 'polyorchid', 'fat', 'Francisco', 'staff', 'mentioned', 'Lord', 'Prayer', 'Monaco', 'nitrates', 'environment', 'quilting', 'Alice', 'Cooper', 'chop', 'suey', 'Terrence', 'Malick', 'spy', 'Moscow', 'correspondent', 'Reuter', 'Times', 'Manuel', 'Noriega', 'ousted', 'Panama', 'authorities', 'magic', 'Universe', 'living', 'conditions', 'Cuba', 'birthplace', 'Allen', 'Poe', 'niece', 'nephew', 'operating', 'IBM', 'compatible', 'extinct', 'One', 'days', 'pow', 'kisser', 'modestly', 'late', 'Janelle', 'Ouarterly', 'Review', 'Doublespeak', 'inoperative', 'IOC', '7', 'Wonders', 'Whom', 'Friz', 'Freleng', 'Warner', 'Bros.', 'ranks', 'Clark', 'dioxide', 'natural', 'stocks', 'presidents', 'science', 'hymn', 'Grapes', 'Wrath', 'dangling', 'participle', 'manufacturers', 'bestselling', 'modern', 'co', 'founder', 'Lights', 'Bookshop', 'sewer', 'commissioner', 'Provo', 'components', 'polyester', 'premier', 'insects', 'spiracles', 'Garmat', 'U.S.A', 'cancer', 'king', 'signed', 'Magna', 'Carta', 'heat', 'sun', 'races', 'unleashed', 'Celestials', 'extension', '.dbf', 'Baltic', 'disposable', 'razor', 'costs', 'cents', 'plants', 'estuary', 'Continental', 'Divide', 'enter', 'prepared', 'mustard', 'originally', 'hurley', 'electricity', 'kept', 'diary', 'flogged', 'programming', 'Inga', 'Nielsen', '197', 'Iraqi', 'invade', 'Kuwait', 'becoming', 'journalist', 'Prix', 'driving', 'championship', 'ankle', 'sprain', 'soccer', 'goalie', 'permitted', '3rd', 'falls', 'asleep', 'whose', 'catsup', 'talk', 'host', 'lends', 'longest', 'sneezing', 'attack', 'Motors', 'laureate', 'expelled', 'Philippines', 'Timor', 'astronomer', 'architect', 'Cathedral', 'website', 'Pulitzer', 'Caine', 'Mutiny', 'shine', 'short', 'event', 'Meyer', 'Wolfsheim', 'fixed', 'Great', 'Gatsby', 'pH', 'wick', 'resurrectionist', 'sharks', 'shouldn', 'stinger', 'tweezers', 'question', 'You', 'Believe', 'Magic', 'needs', 'Galapagos', 'belong', 'bandleader', 'Ella', 'Fitzgerald', 'whom', 'she', 'cowrote', 'A_Tisket', 'Tasket', 'Gettysburg', 'caught', 'haboob', 'blows', 'wrestling', 'Hulk', 'Urals', 'diminutive', 'gymnast', 'Danube', 'street', 'stores', 'nightclubs', 'Sweet', 'Pea', 'Popeye', 'Ivy', 'Palmer', 'Stadium', 'bay', 'Angel', 'garment', 'Bradley', 'Voorhees', 'nuclear', 'proliferation', 'Tucson', 'taste', 'anything', 'zip', 'codes', 'founders', 'Artists', 'Dumbo', 'ears', 'insured', 'astronauts', 'Heineken', 'advertised', 'rings', 'multimedia', 'Hardy', 'typing', 'practice', 'sentence', 'Now', 'aid', 'rock', 'raised', 'ruckus', 'White', 'Woman', 'Hall', 'Fame', 'Series', 'Games', 'Yankees', '1962', 'Pocahontas', 'along', 'peace', 'global', 'depression', '1929', 'Ghana', 'bullheads', 'soldiers', 'WWI', 'Doughboys', 'unknown', 'author', 'opera', 'Aesop', 'fable', 'moral', 'swift', 'Slow', 'steady', 'bound', 'Namath', 'Makepeace', 'Thackeray', 'Stanley', 'Kubrick', 'teenager', 'intercourse', 'Tirana', 'Lakes', 'shoreline', 'Badge', 'Courage', 'Ben', 'coastlines', 'Biscay', 'Wyoming', 'Aladdin', 'relative', 'Leo', 'Tolstoy', 'translated', 'Peace', 'eight', 'calcium', 'daily', 'titanium', 'Olivia', 'De', 'Havilland', '196', 'Summer', 'penalty', 'lies', 'Avenue', 'Reading', 'Railroad', 'Bulls', 'touch', 'Me', 'Blatty', 'recounts', 'horrors', 'Regan', 'MacNeil', 'possession', 'devil', 'Beauty', 'beholder', 'Yaroslavl', 'Dracula', 'statistics', '1965', '1990', 'Honecker', 'gulf', 'destroyers', 'Maddox', 'C', 'Turner', 'Joy', 'Patti', 'Page', 'dancing', 'Tracy', 'mystery', 'Bermuda', 'Triangle', 'headaches', 'parachute', 'Honor', 'Toulmin', 'logic', 'canine', 'Pere', 'Lachaise', 'cemetery', 'Paris', 'Degas', 'bronze', 'sculpture', 'Fourth', 'Position', 'Front', 'Describe', 'Long', 'March', 'LCD', 'Yemen', 'reunified', 'Doodle', 'why', 'stick', 'feather', 'macaroni', 'Mother', 'villi', 'intestine', 'Commune', 'hold', 'rest', '1936', '1939', 'tropical', 'rain', 'forest', 'distributions', 'bodies', 'visited', 'directly', 'Detroit', 'catch', 'wonders', 'V-8', 'Juice', 'slogan', 'tastebud', '1964', 'Kind', 'virgin', 'nun', 'Oh', 'Boy', 'Challengers', 'Unknown', 'exclusive', 'Copacabana', 'Beach', 'Ipanema', 'Family', 'Circus', 'Billie', 'Marion', 'Davies', 'moxie', 'chickenpoxs', 'Mortgage', 'Lifter', 'flying', 'ace', 'Manfred', 'von', 'Richthofen', 'Scottish', 'Louse', 'animal', 'seeking', 'missile', 'Sidewinder', 'ejaculate', 'lost', 'voice', 'operation', '1966', 'Imperial', 'Forces', 'surrender', 'traversed', 'railroad', 'Bellworts', 'Victor', 'Hugo', 'exile', 'Nadia', 'Comaneci', 'poop', 'leg', 'move', 'walking', 'rear', 'ton', 'marked', 'Redford', 'directorial', 'debut', 'religion', 'L.', 'J.', 'Corbett', '1892', 'Names', 'A.', 'Michener', 'locations', '1968', 'primary', 'rainbow', 'Kentucky', 'calls', 'Center', 'pointsettia', 'bad', 'luck', 'breaking', 'mirror', 'forfeited', 'astronaut', 'spacewalk', 'Win', 'Rah', 'muscle', 'provide', 'intake', 'gay', 'province', 'Edmonton', 'longtime', 'jogging', 'Central', 'assassinated', 'Amazon', 'several', 'Marx', 'Brothers', 'educational', 'resources', 'parents', 'teachers', 'serves', 'Salt', 'Lake', 'BPH', '280', 'buffalo', 'crew', 'Kansas', 'Railway', 'automation', 'Thursday', 'give', 'massage', 'Sweden', 'Finland', 'convert', 'pounds', 'Argentine', 'revolutionary', 'Castro', 'Bolivia', '1979', 'Tzimisce', 'release', 'Phoenix', 'Blythe', 'european', 'tied', 'ruble', 'subaru', 'steel', 'bands', 'instruments', 'national', 'prayer', 'electronic', 'device', 'visual', 'displays', 'corresponding', 'electric', 'signals', 'Brian', 'Boru', '11th', 'Usenet', 'Britney', 'Spears', 'Marijuana', 'Nipsy', 'Russell', 'Allan', 'Somme', 'Ayer', 'cubic', 'punishment', 'Retrograde', 'Rolling', 'Writer', 'Egyptians', 'shave', 'eyebrows', 'entered', 'Independence', 'meerkat', 'officer', 'Top', 'neighborhood', 'Plain', 'Dealer', 'Court', 'required', '1879', '1880', '1881', 'Moon', 'sign', 'callosum', 'Houdini', 'Martin', 'Luther', 'Jr.', 'attorney', 'ordered', 'closing', 'Alcatraz', 'exterminate', 'walls', 'Will', 'remain', 'winter', 'Pearl', 'Harbor', 'attacked', 'Douglas', 'McArthur', 'recalled', 'comedian', 'Henry', 'Youngman', 'condiment', 'Dutch', 'dip', 'fries', 'Scouts', 'Missouri', 'Woodward', 'describe', 'Brethren', 'molecules', 'fluorine', 'sodium', 'magnesium', 'Time', 'Western', 'Hemisphere', 'boycott', 'Mark', 'Twain', 'Forests', 'Kosovo', 'Bond', 'boiled', 'Mormon', 'wives', 'Liverpool', 'Thatcher', 'stringed', 'fires', 'bolt', 'televised', 'Alexandra', 'sensitive', 'Michelangelo', 'specimen', 'basidiomycetes', 'graces', 'Lebanon', 'involved', 'Barbary', 'wasps', 'nests', 'VBP', 'elected', 'folic', 'acid', 'conformist', 'abstract', 'Dripper', 'Drew', 'Barrymore', 'elevators', 'floor', 'Building', 'Antichrist', 'dare', 'knock', 'off', 'shoulder', 'help', 'clearer', 'across', 'USA', 'wife', 'acting', 'Lunt', 'Fontanne', 'killing', 'those', 'reflectors', 'washed', 'vodka', 'straight', 'poker', 'helium', 'hence', 'roosters', 'clock', 'punch', 'drunk', 'pugilist', 'Cauliflower', 'McPugg', 'vegetation', 'zones', 'crabgrass', 'advanced', 'coastline', 'Vietnamese', 'terrorist', 'UN', 'delegate', 'quotes', 'fatal', 'anybody', '21', 'Sabrina', 'rockin', 'commonly', 'Stalin', 'Vichy', 'Betsy', 'Ross', 'rank', 'among', 'soda', 'Nile', 'Turks', 'Libya', 'cockroaches', 'learn', 'shillings', 'guinea', 'itch', 'poison', 'ivy', 'Square', 'Japan', 'fly', 'Everest', 'sourness', 'If', 've', 'slum', 'Darius', 'calories', 'Mac', 'Outstanding', 'Award', 'Committee', 'fairy', 'tale', 'Snow', 'Seven', 'Dwarfs', 'penny', 'farthings', 'Trivial', 'Pursuit', 'Death', 'Salesman', 'original', '1985', 'Hawaii', 'bureau', 'Communist', 'Party', 'Cruise', 'Kathie', 'Gifford', 'advertise', 'nanometer', 'internal', 'combustion', 'lines', 'pro', 'footballs', 'supplement', 'U.K.', '47', 'Apple', 'Computer', 'painting', 'Wheatfield', 'Crows', 'Germanic', 'Tell', 'teams', 'Mandrake', 'desert', 'Saudi', 'Arabia', 'Iraq', 'Persian', 'Gulf', 'Whose', 'Natalie', 'Wood', 'Side', 'Story', 'Audrey', 'Hepburn', 'Fair', 'Lady', 'farther', 'opposed', 'further', 'location', 'raise', 'lie', 'Arkansas', 'doughnut', 'kinds', 'Cambodia', '16th', 'Aztec', 'athlete', 'putting', 'Ingmar', 'Bergman', 'Singing', 'Rain', 'Nun', 'Blood', 'Fire', 'doing', 'prevent', 'extinction', 'Stephen', 'graveyard', 'pets', 'blacks', 'Nicolo', 'Paganini', 'famously', 'warn', 'coming', 'price', 'rum', 'giving', 'Wiz', 'Caribbean', 'cult', 'Marcus', 'Garvey', 'Teddy', 'bear', 'Rosa', 'Parks', 'Become', 'refusing', 'seat', 'bus', 'terror', 'Johnny', 'Horton', 'Walter', 'Madre', 'limits', 'self', 'defense', 'ads', 'eliminates', 'household', 'germs', 'mold', 'mildew', 'Jewish', 'cats', 'flower', 'garden', 'kissed', 'positions', 'succession', 'presidency', 'sub', 'Saharan', 'CCT', 'diagram', 'deserts', 'Coney', 'boardwalk', 'deconstructionism', 'happen', 'protanopia', 'Rowan', 'Laugh', 'lice', 'Burr', 'vocalist', 'same', 'Hansel', 'Gretel', 'block', 'Kingdom', '15th', 'brimstone', 'monk', 'gained', 'Florence', 'ended', 'burnt', 'stake', 'zoonose', 'colonies', 'Revolution', 'cheetahs', 'Bette', 'Davis', 'creating', 'scandal', 'daring', 'gown', 'requirement', 'Neoclassical', 'Romanticism', 'watts', 'kilowatt', 'Dolly', 'Parton', 'rarely', 'charcter', 'chiefly', 'enormous', 'admitted', 'Union', 'boys', 'Winslow', 'Homer', '1872', 'Snap', 'Whip', 'thee', 'complemented', 'sweet', 'potatoes', 'peas', 'Z', 'echidna', 'Madonna', 'Global', 'Schoolhouse', 'Future', 'Shock', 'freeway', 'construction', 'Rubber', 'Capital', 'chromosome', 'cameras', 'pictures', 'Prussia', 'centers', 'stolen', 'treatments', 'chance', 'conceiving', 'quadruplets', 'hundred', 'billionth', 'crayon', 'Crayola', 'High', 'Desert', 'organs', 'donated', 'Fodor', 'publish', 'pies', 'wound', 'manufacturing', 'throwing', 'toys', 'lottery', 'Buffett', 'concert', 'E', 'Camden', 'deep', 'fathom', 'Milt', 'Harper', 'measured', '1974', 'occupation', 'modem', 'access', 'Nintendo', '64', 'rent', 'Volkswagen', 'portal', 'Goodall', 'CE', 'particularly', 'electrical', 'purchased', 'bails', 'cricket', 'wicket', 'par', '455', 'yard', 'storm', 'wave', 'Doegs', 'visiting', 'Duvalier', 'principles', 'learning', 'Research', 'Learning', 'IRL', 'advertizing', 'Frito', 'Lay', 'Moorish', 'patents', 'credit', 'toast', 'alloy', 'copper', 'tin', 'Pennsylvania', 'Fawaz', 'Younis', 'commit', 'piracy', 'hostage', 'taking', 'Justin', 'Fort', 'Knox', 'valuable', 'here', 'hijacking', 'sword', 'Corporal', 'cunnilingus', 'gestation', 'coconut', 'pineapple', 'schematics', 'windshield', 'wiper', 'mechanism', 'credited', 'saying', 'met', 'shrubs', 'planted', 'safe', 'deer', 'creator', 'Bradbury', 'Chronicles', 'pollution', 'Bert', 'Loomis', 'Hazmat', 'stands', 'astronomical', 'phenomenon', 'Jan.', 'Quebec', 'meter', '..', 'alternator', 'Wayne', 'Gretzky', 'jerk', 'Fray', 'Bentos', 'Mackinaw', 'N', 'est', 'ce', 'pas', 'iron', 'turnkey', 'virtual', 'IP', 'starting', 'beginning', 'lawyers', 'cullions', 'chromatology', 'Guadalcanal', 'wheat', 'Worlds', 'silk', 'screening', 'electoral', 'votes', 'Colorado', 'radioactive', 'Nevil', 'Shute', 'doomed', 'survivors', 'generals', 'abbreviate', 'cc', 'Miller', 'commercial', 'stuck', 'tongue', 'friendly', 'greeting', 'Teflon', 'pan', 'inducted', 'Swimming', 'routinely', 'dem', 'bums', 'frustrated', 'wheel', 'e.g.', 'tire', 'spin', 'slows', 'Dennis', 'Menace', 'stripe', 'Coho', 'salmon', 'thing', 'Brave', 'powered', 'sank', 'Norwegian', 'April', '1989', 'M', 'People', 'calculate', 'change', 'enthalpy', 'reaction', 'snakes', 'rugs', 'still', 'calluses', 'mailman', 'Beasley', 'preface', 'foreword', 'unaccounted', 'temple', 'Laos', 'Farmer', 'Almanac', 'adding', 'Lactobacillus', 'bulgaricus', 'Twin', 'Cities', 'pressured', 'appointing', '1933', 'Browns', 'lyrics', 'Spangled', 'Banner', 'B', 'assassination', 'fax', 'UK', 'behind', 'pig', 'pulls', 'strings', 'speaks', 'Miss', 'Piggy', 'Holden', 'Caulfield', 'rich', 'quick', 'Pepper', 'bottles', 'scorpion', 'caul', 'auto', 'thefts', 'came', 'nautical', 'ships', 'sailing', 'Canal', 'Christina', 'Austerlitz', 'subway', 'stops', 'adjournment', '25th', 'anniversary', 'session', 'Nations', 'Assembly', 'goldenseal', 'tarantula', 'imaginary', 'Rocky', 'Mountains', 'voices', 'lustrum', '36893', 'ignores', 'friends', 'Med', 'molybdenum', 'porphyria', 'bat', 'fellatio', 'interesting', 'facts', 'dogsledding', 'better', 'identity', 'Merrick', 'noble', 'ogre', 'sagebrush', 'Prewitt', 'shows', 'territories', 'Irkutsk', 'Yakutsk', 'Kamchatka', 'Winnie', 'Pooh', 'Shea', 'Gould', 'L.A.', 'Chiang', 'Kai', 'shek', 'sexiest', 'Hajo', 'thalassemia', 'defined', 'a.m.', 'p.m.', 'emperors', 'Roman', 'problems', 'hooligans', 'manager', 'mascot', 'Notre', 'Dame', 'buys', '25', 'tea', 'exports', 'Christian', 'pasta', 'merchandise', 'sales', 'AIDS', 'headquarters', 'Procter', 'Gamble', 'vegetable', 'z', 'reputed', 'maker', 'mythology', 'Gaulle', 'alphabetical', 'fingernails', 'earn', 'tourism', 'Ishmael', 'Moby', 'jobs', 'earns', 'Hell', 'Kitchen', 'clothes', 'aids', 'riding', 'our', 'mammal', 'eats', 'sleeps', 'underground', 'cookie', 'abolished', 'importance', 'Magellan', 'joined', 'Gran', 'Bernardo', 'arab', 'strap', 'facility', 'ballcock', 'overflow', 'tube', 'girls', 'bras', 'Five', 'Italians', 'Bavaria', 'submarines', 'attractions', 'draw', 'visitors', 'proposition', 'Peugeot', 'manufacture', 'blondes', 'discontent', 'caps', 'G7', 'formed', 'took', 'prize', 'Omni', 'magazine', 'ultimate', 'unanswerable', 'Hermann', 'Hesse', 'procedure', 'drilling', 'skull', 'acheive', 'higher', 'consciousness', 'ion', 'Perfect', 'Fool', 'bullseye', 'darts', 'millimeters', 'Aldous', 'Huxley', 'Halloween', 'aortic', 'abdominal', 'aneurysm', 'surroundings', 'Ninety', 'Theses', '39', 'stratocaster', 'Element', 'screensaver', 'funnel', 'Rossetti', 'paint', 'Beata', 'Beatrix', 'Patricia', 'Hearst', 'kidnaped', 'Hass', 'cut', 'thin', 'daminozide', 'weapons', 'warfare', 'mourning', 'recorded', '1957', 'unsuccessful', 'overthrow', 'Bavarian', 'Munich', '1923', 'condensed', 'relax', 'clear', 'mathematical', 'factor', 'authors', 'goals', 'scored', 'single', 'NHL', 'Marco', 'Polo', 'introduce', 'Kubla', 'Khan', 'tenths', 'surface', 'mother', 'duke', 'NASDAQ', 'marvelous', 'spokesman', 'Canon', 'Major', 'Megan', 'law', 'research', 'expedition', 'climbing', 'Wilkes', 'plantation', 'Gone', 'Wind', 'established', 'hydrogen', 'encyclopedia', 'published', 'crooner', 'Andrews', 'Sisters', 'Pistol', 'Packin', 'Mama', 'cricketer', '1898', 'Knight', 'Ridder', 'roommates', 'Company', 'cigar', 'chewing', 'observed', 'feel', 'Iberian', 'peninsula', 'Penn', 'Landing', 'banks', 'Delaware', 'sons', 'Ozzie', 'Harriet', 'Nelson', 'Mercury', 'JESSICA', 'mystical', 'ravens', 'Odin', 'increase', 'biceps', 'gain', 'starring', 'saliva', 'consist', 'Bastille', 'kidnaping', 'termed', 'Crime', 'Century', 'efficient', 'barbeque', 'immigration', 'laws', 'Vince', 'Lombardi', 'coaching', 'studio', 'Bateau', 'Lavoir', 'Montmartre', 'ability', 'silkworm', 'moth', 'domestication', 'issue', 'else', 'swastika', 'stood', 'D.C.', 'paintball', 'clot', 'distilling', 'height', 'Eyes', '80', 'secondary', 'Mammoth', 'Cave', '1940s', 'collectible', 'Donald', 'Duck', 'Boat', 'acres', 'fox', 'suspension', 'northeast', 'vehicles', 'anthem', 'nonconsecutive', 'cartoondom', 'climb', 'Nepal', 'denied', 'voting', 'Victoria', 'regarding', 'India', 'Bonnie', 'Blue', 'Butler', 'snatches', 'jerks', 'broke', 'barrier', 'Czech', 'Pole', 'Wallbanger', 'Spartacus', 'gladiator', 'qualifications', 'individuals', 'donating', 'Nicois', 'Don', '1937', 'Garrett', 'Morgan', 'Sigmund', 'Freud', 'fascinated', 'experimenting', 'neurasthenia', 'really', 'Dialing', '900', '740', 'TREE', 'Janet', 'month', 'Gregorian', 'row', 'typewriter', 'Lewis', 'Carroll', 'Humpty', 'Dumpty', 'deltiologist', 'collect', 'firewall', 'Milky', 'Galaxy', 'revolution', 'skyline', 'Gateway', 'Arch', '1990s', 'droppings', 'Angelus', '1967', 'Best', 'Actor', 'slavery', 'properly', 'Stevie', 'twice', 'Ukrainians', 'builder', 'captured', 'Syrian', 'mounted', 'guerrilla', 'Jesse', 'Coleman', 'Younger', 'ridden', 'sequencing', 'Beverly', 'Hillbillies', 'Daisy', 'Moses', 'received', 'assent', 'emblem', '1975', 'ratified', 'nematode', 'Cartesian', 'Diver', 'pusher', 'poorly', 'economists', 'chancery', 'estate', 'Mel', 'Brooks', 'Movie', 'buxom', 'blonde', 'cover', 'magazines', 'nominated', 'Let', 'goddamn', 'airborne', 'Boston', 'Kreme', 'glacier', 'U.S', 'ATM', 'Malta', 'Mont', 'Blanc', 'Tunnel', 'practical', 'marketed', 'socioeconomic', 'R.E.M.', 'purchase', 'insurance', 'ranges', 'continental', 'Bic', 'flame', 'clockwise', 'counterclockwise', 'Graffiti', 'Zimbabwe', 'Abigail', 'Arcane', 'related', 'villainous', 'opponent', 'Swamp', '1998', 'Cromwell', 'pepper', 'rows', 'whiskers', 'journalism', 'Universal', 'Import', 'Export', 'agricultural', 'crop', 'belly', 'buttons', 'Thompson', 'flood', 'Luis', 'Rey', 'Marino', 'smallest', 'Republic', 'prenatal', 'engineer', 'pull', 'tab', 'Harold', 'Stassen', 'announced', 'Esa', 'Pekka', 'Salonen', 'winning', '1973', 'Belmont', 'Stakes', '31', 'Brenner', 'chief', 'Powhatan', 'Rolfe', 'uniforms', 'kiss', 'somene', 'honey', 'HIGHEST', 'numeral', 'KDGE', 'Burkina', 'Faso', 'seasons', 'cookies', 'similar', 'Lyrics', 'Server', 'Ohio', 'pneumonia', 'afflict', 'Prophet', 'Medina', 'Britain', 'mainland', 'cash', 'surpassing', 'corn', 'blew', 'Lakehurst', 'clay', 'mixture', 'bone', 'china', 'ports', 'chickens', 'chicks', 'Docklands', 'Light', 'constructed', 'safest', 'pedestrians', 'Francis', 'Assisi', 'Stage', 'driven', 'Smokey', 'Bandit', 'Collector', 'Tampa', 'Fulton', 'steamboat', 'stove', 'Flintstones', 'Scrooge', 'partner', 'Dickens', 'Carol', 'ground', 'bombing', 'Am', 'Flight', '103', 'Lockerbie', 'Scotland', 'portly', 'criminologist', 'Carl', 'Hyatt', 'Checkmate', 'Ed', 'allegedly', 'obscene', 'gesture', 'dogs', 'consider', 'blunder', 'previous', 'Commonwealth', 'songs', 'mustachioed', 'Frankie', 'example', 'sixties', 'Bourbon', 'restored', 'throne', 'Napoleon', 'abdication', 'estimated', 'whitetail', 'soy', 'sauce', 'electronics', 'coastal', 'Dana', 'Two', 'Years', 'Before', 'Mast', 'seafarers', 'abandoned', 'beach', 'succeeded', 'Nikita', 'recomended', 'switch', 'crib', 'bed', 'Nero', 'Wolfe', 'hours', 'lying', 'fastest', 'dollars', 'Frommer', 'employed', '72', 'baseemen', 'incorporate', 'Ball', 'Booth', 'Catherine', 'hits', 'foul', 'filling', 'tonsils', 'Bud', 'Melman', 'Toast', 'Stick', 'Mpilo', 'Tutu', 'beat', 'oyster', 'phobophobe', 'stings', 'fame', 'Camp', 'Gillette', 'thermal', 'equilibrium', 'belt', 'pressure', 'equator', 'Our', 'Lives', 'Cookbook', '198', 'Liz', 'Chandler', 'Guiteau', 'childhood', 'Muhammad', 'Ali', 'Foreman', 'victim', 'copier', 'Doodyville', 'Orphans', 'Fund', 'bachelor', 'bedroom', 'apartment', 'fingers', 'shall', 'Sistine', 'Chapel', 'Life', 'popularly', 'pallbearer', 'tsetse', 'Peruvian', 'mummified', 'Pizarro', 'cecum', 'manufactures', 'hardware', 'sells', 'weakness', 'enforce', 'youngsters', 'please', 'vessels', 'melt', 'quicker', '401', 'K', 'plan', 'Jonathan', 'Livingstone', 'Seagull', 'browser', 'Mosaic', 'includes', '1896', 'italian', 'traditional', 'Derby', 'pink', 'No.1', 'apparel', 'Horsemen', 'Apocalypse', 'feeling', 'having', 'experienced', 'something', 'laser', 'Nebraska', 'mid', '1900s', 'Catch-22', 'dozen', 'radiographer', 'rating', 'conditioner', 'efficiency', 'Gay', 'Science', 'Maximo', 'Golden', 'Gate', 'Picts', 'processing', 'FUBU', 'childbirth', 'zone', 'archery', 'target', 'hearts', 'octopus', 'orca', 'fungal', 'infection', 'picture', 'Meanie', 'equation', 'Andorra', 'nestled', 'ceremony', 'traditions', 'Elizabethian', 'pyramid', 'Giza', 'S&P', 'shampoo', 'prevents', 'eczema', 'seborrhea', 'psoriasis', 'Aristotle', 'Onassis', 'yacht', 'trace', 'roots', 'Ringo', 'Stagecoach', 'fun', 'things', 'Cozumel', 'teenagers', 'violent', 'Steve', 'McQueen', 'Edward', 'G.', 'Robinson', 'Cincinnati', 'normal', 'lasts', 'tells', 'betrayed', 'Norway', 'underwater', 'thrillers', 'Girl', 'Cortez', '69', 'Ford', 'Econoline', 'van', 'F25', 'V1', 'hypnotherapy', 'inspiration', 'schoolteacher', 'Dead', 'Poets', 'surfboard', 'well', 'personality', 'adopted', 'Hans', 'Henderson', 'spanish', 'camel', 'spine', 'image', 'alleged', 'Shroud', 'Turin', 'honorary', 'irate', 'oxide', 'protects', 'Comics', 'realm', 'dreams', 'Hooked', 'Feeling', 'episode', 'Ally', 'Mcbeal', 'Jaco', 'Pastorius', 'titled', 'Raging', 'health', 'nutrition', 'Erica', 'Jong', 'deals', 'Isadora', 'Wing', 'maids', 'milking', 'funny', 'creative', 'genius', 'Everything', 'him', 'hustles', 'waits', 'zoological', 'ruminant', 'cherry', 'flowers', 'digital', 'audio', 'delicacy', 'indelicately', 'pickled', 'roe', 'Sandra', 'Bullock', 'ejaculation', 'nuts', 'marzipan', 'travelling', 'u', 'back', 'Wilbur', 'Reed', 'owning', 'Sondheim', 'ballad', 'Well', 'maybe', 'lose', 'flab', 'chin', 'autobiography', 'Yesterdays', 'bow', 'goulash', 'feat', 'U-2', 'quickest', 'easiest', 'nail', 'polish', 'Mexican', 'youngest', 'Beatles', 'recognize', 'anorexia', 'translate', 'gandy', 'dancer', 'reactivity', 'Elvis', 'Presley', 'Sultan', 'Ted', 'grooves', 'dime', 'edge', 'Vladimir', 'Nabokov', 'Professor', 'Humbert', 'girl', 'chemiosmotic', 'theory', 'Bernadette', 'Peters', 'specifically', 'Noah', 'Ark', 'Antonia', 'Shimerda', 'farm', 'powder', 'lotion', 'smell', 'Langerhans', 'zipper', 'temperance', 'advocate', 'wielded', 'hatchet', 'saloons', 'assassinations', 'attempts', '1865', 'Hammer', 'Studios', 'class', 'thirds', 'midwest', 'slang', 'darn', 'tootin', 'Dylan', 'series', 'aquatic', 'scenes', 'Springs', 'crossed', 'slits', 'castles', 'accommodate', 'photograph', 'professor', 'Randolph', 'Quirk', 'Indiglo', 'Theresa', 'At', 'village', 'La', 'Mancha', 'Depression', 'probability', 'Funk', 'Lata', 'Brazilian', 'aurora', 'motorcycle', 'folk', 'Chapman', 'legendary', 'reputation', 'stealing', 'jokes', 'touching', 'Korea', 'ambassadorial', 'relations', 'apostle', 'Caldwell', 'Lion', 'VDRL', 'test', 'solar', 'experiment', 'northernmost', 'Darwin', 'Basilica', 'AFS', 'mosquitoes', 'August', '13', '1971', 'fair', 'projects', '8th', 'Wassermann', 'develop', 'specific', 'Captain', 'stationed', 'Swampy', 'strips', 'Joseph', 'Mankiewicz', 'entries', '1669', 'secret', 'blend', 'herbs', 'spices', 'basic', 'swimming', 'strokes', 'criticism', 'crystals', 'exist', 'O', 'Neal', 'NBA', 'glory', 'grandeur', 'Rome', 'Sonny', 'Liston', 'succeed', 'champion', 'policeman', 'Canadian', 'edition', 'Morris', 'bishop', 'becomes', 'Hawkeye', 'Stockyards', 'Gothic', 'architecture', 'flourish', 'Soldiers', 'battles', 'nevermind', 'tie', 'dye', 'compass', 'prefix', 'surnames', 'breeding', 'edentulous', 'smile', 'fined', 'hormone', 'Chiricahua', 'Bush', 'ingredients', 'consumption', 'cheese', 'Tyler', 'regular', 'Tadeus', 'Wladyslaw', 'Konopka', 'might', 'pilots', 'Vasco', 'Gama', 'discover', 'Hampshire', 'hamlet', 'rises', 'vote', 'elections', 'biochemists', 'medicine', '1992', 'Warlock', 'forehead', 'hamburger', 'ham', '1789', 'Far', 'Madding', 'Crowd', 'Cribbage', 'isolationist', '1853', 'Theo', 'Rousseau', 'Forest', 'Fontaine', 'Finger', 'eighth', 'Nones', 'C.', 'Calhoun', 'Clay', 'arometherapy', 'occurred', 'Bebrenia', 'Amazonis', 'Bagdad', 'bends', 'clip', 'Fordham', 'defeat', 'Waynesburg', '12601', 'September', 'Skrunch', 'Butter', 'Oompas', 'pass', 'Copyright', 'lemurs', 'chess', 'Matterhorn', 'Isis', 'nature', 'goddess', 'Grace', 'Metalious', 'seller', 'Edmund', 'bought', 'Russia', 'pilot', 'jersey', 'Suzy', 'Parker', 'prophet', 'Islam', 'trials', 'resulting', 'hiemal', 'activity', 'normally', 'quality', 'drinks', 'stone', 'rolling', 'east', 'coast', 'west', 'completed', 'quarterbacks', 'shortstop', 'various', 'belonged', 'tattoo', 'wrist', 'reading', 'Forever', 'Sir', 'climbed', 'Mt.', 'young', 'flights', 'PhotoShop', 'Gompers', 'Mayan', 'Allies', 'Operation', 'Avalanche', 'plural', 'Elongated', 'afoot', 'heavier', 'U.S.S.R.', 'appointed', 'chair', 'Federal', 'Reserve', 'sled', 'Iditarod', 'cannon', 'saltpeter', 'egg', 'Broadway', 'birthdate', 'BMW', 'waterfall', 'hyperlink', 'ribbon', 'Video', 'Aeul', 'Jell', 'mayonnaise', 'refers', 'differences', 'Methodist', 'religions', '139', 'papal', 'tail', 'kite', 'cocktail', 'Doxat', 'Stirred', 'Not', 'Shaken', 'corridors', 'Pentagon', 'rare', 'symptoms', 'involuntary', 'movements', 'tics', 'swearing', 'incoherent', 'vocalizations', 'grunts', 'shouts', 'replies', 'Leia', 'confession', 'Strikes', 'Back', 'endangered', 'heroine', 'Scruples', 'Madilyn', 'Kahn', 'Wilder', 'manifest', 'latent', 'theories', 'Viking', 'taller', 'ran', 'Lenny', 'Bruce', 'arrested', 'marriage', 'gangsters', 'Clyde', 'Koran', 'Sinclair', 'Main', 'Fe', 'Trail', 'Brandenburg', 'gate', 'erected', 'daughters', 'traded', 'Hermitage', 'materials', 'decompose', 'Visine', 'Lagos', 'permanently', 'frozen', 'actors', 'Cleaveland', 'Cavaliers', 'Pines', 'cholera', 'Gerald', 'markets', 'brunettes', 'scares', 'credits', 'Also', 'Known', 'jack', '-lantern', 'Elroy', 'Hirsch', 'tabulates', 'ballots', 'victor', 'doorstep', 'Gasoline', 'Katie', 'Dartmouth', 'College', 'Woodstock', 'NAFTA', 'propaganda', 'Water', 'Carrier', 'zodiacal', 'wash', 'winner', 'contagious', 'governmental', 'agency', 'responsible', 'dealing', 'racism', 'gametophytic', 'tissue', 'Boris', 'Pasternak', 'sold', 'copies', 'fiction', '1958', 'Kim', 'Philby', 'Chief', 'Aztecs', 'Gods', 'editor', 'cake', 'soap', 'liberty', 'steam', 'rainstorm', 'dates', 'running', 'bulls', 'Pamplona', 'suspect', 'Clue', 'Percival', 'Lovell', 'claws', 'pistol', 'ailment', 'Urgent', 'Fury', 'advised', 'listeners', 'U.S.A.', 'Chevrolet', 'excluded', 'ANZUS', 'alliance', 'L.L.', 'Cool', 'malls', 'holiday', 'decorations', 'Pride', 'Poker', 'chip', 'assigned', 'value', 'incompetent', 'shores', 'Gitchee', 'Gumee', 'house', 'philosophy', 'plans', 'forerunner', 'ranch', 'direct', 'folklore', 'talking', 'Admiral', 'Seas', 'Viceroy', 'Governor', 'granted', '10-', 'profits', 'voyage', 'background', 'noise', 'stereo', 'Dondi', 'adoptive', 'grandfather', 'Abbey', 'Hoffman', 'Rubin', 'Hayden', 'feeding', 'pigeons', 'Piazza', 'colony', 'Greenland', '985', 'equivalence', 'Bloom', 'resident', 'wreaks', 'destroyed', 'x', 'Kinks', 'hiking', 'Iraqis', 'Faber', 'Mongol', 'pencil', 'lucky', 'enough', 'sprayed', 'Corpus', 'Christi', 'skiing', 'Calgary', 'clitoris', 'cooking', 'costliest', 'disaster', 'industry', 'J.R.R.', 'Tolkien', 'Bilbo', 'Baggins', 'central', 'Sebastian', 'Taj', 'Mahal', 'firm', 'Buffalo', 'Sabres', 'chick', 'breathe', 'nearest', 'sundaes', 'hang', 'Mona', 'Lisa', 'Crimean', 'meeting', 'Churchill', 'artificial', 'intelligence', 'stained', 'window', 'Gustav', 'V', '195', 'transistor', 'Charge', 'magnetar', 'lion', 'brew', 'Christine', 'possessed', 'magenta', 'Ligurian', '1797', '185', 'RCD', 'equity', 'securities', 'Lutine', 'Bell', 'announce', 'Eli', 'Lilly', 'jealousy', 'referring', 'it-', 'racehorse', 'Associated', 'Press', 'poll', '20th', 'panties', 'Titans', 'adopt', 'AIM-54C', 'threat', 'Goldie', 'Hawn', 'boyfriend', 'filmmakers', 'collabrative', 'period', 'pregnancies', 'rathaus', 'Frankfurt', 'fell', 'Elizabeth', 'Malawi', 'sprocket', 'holes', '35', 'millimeter', 'spelled', 'T', 'Y', 'Finnish', 'psychology', 'Tiny', 'Tim', 'alone', 'Astroturf', 'creams', 'seaweed', 'double', 'spaces', 'Scrabble', 'Crossword', 'Game', 'Declaration', 'slinky', 'mosquito', 'bite', 'draws', 'CC', 'brush', 'actually', 'Fickle', 'Fate', 'congressional', 'delegation', 'June', 'Tutankhamun', 'exhibit', 'moving', 'transported', 'Deere', 'tractors', 'bounded', 'Coral', 'Tasman', 'shipment', 'proper', 'respones', 'Say', 'goodnight', 'Kwai', 'pet', 'invaded', 'Poland', 'leprosy', 'licensed', 'geese', 'U.S.-based', 'Jenna', 'eligible', 'Roller', 'Ian', 'Fleming', 'seed', 'couple', 'divorce', 'piles', 'canonize', 'Post', 'Tristar', 'phone', '1895', 'H.G.', 'Wells', 'Chronic', 'Argonauts', 'tips', 'fireplace', 'geological', 'neurons', 'divides', 'Eastern', 'Shores', 'Louise', 'Fletcher', 'deodorant', 'builds', 'resistance', 'odor', 'bill', 'cows', 'MTV', 'salesman', 'UOL', 'Marciano', 'pelvic', 'restore', 'distribute', 'humanitarian', 'relief', 'Somalia', 'Anthony', 'monarchs', 'crowned', 'LMDS', 'worst', 'finish', '1926', 'fourth', 'Hand', 'Luke', 'Julie', 'Poppins', 'Motown', 'Records', 'bar', 'Drinks', 'Abbie', 'dose', 'singles', 'Fred', 'Perry', 'dying', 'His', 'Voice', 'fade', 'hijack', 'aol.com', 'yahoo.com', 'substance', 'Priestley', 'erase', 'nylon', 'stockings', 'sale', 'pocket', 'billiards', 'hocks', 'personal', 'Minnesota', 'hemisphere', 'July', 'nonaggression', 'pact', 'tenpin', 'ratio', 'brilliant', 'economist', 'creation', 'Helps', 'hurt', 'hurting', 'Warren', 'Spahn', '20', 'Gates', 'curies', 'seventeen', 'defensive', 'Diplomacy', 'challenged', 'explore', 'Frontier', 'tools', 'crewel', '1915', 'gross', 'Redskin', 'fan', 'lemon', 'automobiles', 'Zatanna', 'appropriate', 'Yom', 'Kippur', 'sunk', 'mine', 'Havana', 'harbor', 'Vincent', 'Gogh', 'Lust', 'data', 'collection', 'forests', 'Feynman', 'Physics', 'marrow', 'transplant', 'repeating', 'voter', 'builders', 'Egypt', 'mainly', 'Al', 'Capone', 'prism', 'transmitted', 'Anopheles', 'gymnophobia', 'Chaplin', 'Modern', 'Dictator', 'areas', 'Titus', 'Metropolis', 'expect', 'monthly', 'publication', 'Bigfoot', 'News', 'judiciary', 'Music', 'Iceland', 'ready', 'breakfast', 'cereal', 'paracetamol', 'touched', 'wind', 'quiz', 'Vera', 'Lynn', 'We', 'Meet', 'Again', 'exclusively', 'Inoco', 'laptop', 'Blaise', 'Pascal', 'search', 'returned', 'fraudulent', 'operations', 'free', 'pacer', 'compete', 'alcohol', 'limit', 'Gimli', 'Bobby', 'Fischer', 'flow', 'Astaire', 'Angela', 'bicornate', 'Olsen', 'Led', 'Zeppelin', '187s', 'mining', 'cranes', 'buildings', 'tampon', 'AOL', 'users', 'raced', 'Tour', 'haven', 'plugged', 'Coffee', 'Monet', 'firemen', 'migrates', 'farthest', 'parts', 'psorisis', 'disappear', 'photographer', 'Yousuf', 'Karsh', 'shiest', 'airline', 'safety', 'Oklahoma', 'savings', 'bond', 'mature', 'Air', 'Force', 'raid', 'Citizen', 'Kane', 'neon', 'coal', 'adventures', 'Swiss', 'objects', 'orbit', 'Emperor', 'Palpatine', 'preferably', 'bells', 'radius', 'ellipse', 'Congo', '14', '1946', 'Westview', 'Funky', 'Winkerbean', 'rounded', 'matchbook', 'recruited', 'Saddam', 'Hussein', 'therapy', 'elicit', 'primal', 'scream', 'photosynthesis', 'Sleeping', 'Yahoo', 'Excite', 'crust', 'Manson', 'trial', 'VIII', 'Anne', 'Boleyn', 'provided', 'listen', 'Paraguay', 'vacations', 'Kemper', 'calleda', 'Model', '1928', 'affectionate', 'Erich', 'boob', 'bomb', 'prehistoric', 'erotic', 'Daniel', 'apart', 'n', 'Boulevard', 'doll', '1959', 'Bulge', 'offices', 'failure', 'Famine', 'Guernsey', 'Sark', 'Herm', 'Lagoon', '1982', 'Pirate', 'Heaven', 'speaking', 'airman', 'Goering', 'Zolotow', 'Shooting', 'Soft', 'Self', 'Portrait', 'Grilled', 'Bacon', 'spiritual', 'cultural', 'contact', 'brothel', 'chosen', 'military', 'Joint', 'Chiefs', 'Staff', 'boxcars', 'flush', 'Gibson', 'learned', 'saxophone', 'speak', 'Sons', 'Lovers', 'Mills', 'Cheerios', 'paths', 'enhance', 'athletic', 'sporting', 'collapsed', 'coney', 'Wembley', 'dish', 'pigs', 'intestines', 'colleges', 'engineering', 'civilization', 'nearsightedness', 'Renaud', 'headquartered', 'contestant', 'picking', 'toes', 'shower', 'opened', 'Stonehenge', 'Rockettes', 'heavily', 'caffeinated', 'Biggest', 'Autry', 'Please', 'regards', 'Dale', 'Economy', 'popularized', 'soup', 'cans', 'Brillo', 'pad', 'boxes', 'multicultural', 'multilingual', 'Belle', 'Beast', 'Film', 'heir', 'raising', 'wreckage', 'Andrea', 'Doria', 'Mideast', 'slotbacks', 'tailbacks', 'touchbacks', 'student', 'Amherst', 'Sicilian', 'Rhett', 'leaving', 'Scarlett', 'Hara', 'justify', 'emergency', 'decrees', 'imprisoning', 'opponents', 'vesting', 'themselves', 'snowboarding', 'currency', 'Argentina', 'leftovers', 'marl', 'mineral', 'pursued', 'Tweety', 'Pie', 'currently', 'inescapable', 'purveyor', 'heaviest', 'fiddlers', 'Cole', 'Chilean', 'coup', 'd', 'etat', 'hotel', 'burned', '84', 'Judith', 'Rossner', 'Diane', 'Keaton', 'Eduard', 'primitives', 'rural', 'goldfish', 'dimly', 'lit', 'Hawks', 'unique', 'Canadians', 'emmigrate', 'curl', 'tee', 'Masters', 'vichyssoise', 'shake', 'friendliness', 'Olive', 'Oyl', 'brother', 'Viagra', 'node', 'Woodrow', 'Wilson', 'dimension', 'viscosity', 'oldtime', 'kids', 'Guide', 'Jeff', 'Greenfield', 'subversive', 'B.Y.O.B.', 'berry', 'blackberry', 'raspberry', 'strawberry', 'Fatman', 'extensively', 'grown', 'eaten', 'appoint', 'cabinet', 'Aspartame', 'menu', 'item', 'spicey', 'Tex', 'Avery', 'arriving', 'MGM', 'span', 'monkey', 'treated', 'Occam', 'Razor', 'puzzle', '1913', 'brightest', 'visible', 'ART', 'JPEG', 'Bitmap', 'news', 'typically', '55', 'hairs', 'chihuahuas', 'neurosurgeon', 'Bjorn', 'Borg', 'forehand', 'diamond', 'producer', 'Even', 'akita', 'G', 'False', 'delicate', 'tasting', 'onion', 'innings', 'constitute', 'trigonometry', 'topic', 'outline', 'subtitled', 'Reflections', 'Holland', 'info', 'Mackenzie', 'Government', 'Standard', 'Industrial', 'Classification', 'SIC', 'Antilles', '33', 'Rock', 'approaches', 'systems', 'analysis', 'Salk', 'Head', 'Start', 'postal', 'ouzo', 'Milo', 'knighted', 'novels', 'section', 'finger', 'joint', 'hobby', 'A&W', 'camels', 'humps', 'surge', 'Pompeii', 'dial', 'tragedy', 'Dogtown', 'Jolson', 'Freddy', 'Freeman', 'Cinderslut', 'skein', 'wool', 'capitalizes', 'singular', 'pronoun', 'TO', 'VB', 'POS', 'reference', 'Biblical', 'quotation', 'together', 'colored', 'squares', 'Rubik', 'Cube', 'colt', 'Anything', 'Goes', 'IT', 'User', 'Satisfaction', 'Level', 'Joyce', 'Ulysses', 'Musician', 'HDLC', 'rugby', 'bull', 'sailors', 'Masons', 'Rosanne', 'Rosanna', '48', 'conterminous', 'Dorsets', 'Lincolns', 'Oxfords', 'Southdowns', 'Transparent', 'Salvador', 'Dali', 'signature', 'worlds', 'supplier', 'cannabis', 'amaretto', 'biscuits', 'Estonia', 'bottled', 'jeroboams', 'Louie', 'stethoscope', 'disks', 'handed', 'Crokinole', 'Smartnet', 'Georgetown', 'Hoya', 'cables', 'corgi', 'Nine', 'Inch', 'Nails', 'Mandy', 'approval', 'Pelt', 'psychiatric', 'sessions', 'husbands', 'Hilton', 'Wilding', 'binomial', 'coefficients', 'Shirtwaist', 'describes', 'usage', 'Elysium', 'impress', 'guy', 'Ezra', 'Taft', 'Benson', 'install', 'tile', 'showers', 'Sicily', 'drafted', 'logarithmic', 'scales', 'slide', 'softball', 'weekend', 'Monterey', 'Jazz', 'hairdryer', 'Bullwinkle', 'dextropropoxyphen', 'napsylate', 'Pluribus', 'Unum', 'ante', 'mortem', 'Tel', 'Aviv', 'Memphis', 'mailing', 'lists', 'hardest', 'pens', 'Roe', 'Wade', 'decision', 'Supreme', 'wolverine', 'habits', 'foreclosure', 'Yale', 'Lock', '1976', 'Deep', 'Throat', 'clone', 'provides', 'larynx', 'USSR', 'dissolved', 'Kythnos', 'Siphnos', 'Seriphos', 'Mykonos', 'Baffin', 'Frobisher', 'looking', 'rarest', 'coin', 'encounters', 'auberge', 'tabs', 'Third', 'Eye', 'Blind', 'watchman', 'Frank', 'Wills', 'racoon', 'trinitrotoluene', 'Nordic', 'Montana', '1952', '1954', 'exactly', 'anteater', 'Seine', 'Konigsberg', 'behavior', 'violates', 'accepted', 'standards', 'sexual', 'morality', 'pins', 'skittles', 'centurion', 'deal', 'arms', 'Harlow', '1932', 'artists', 'tend', 'portraits', 'Fairground', 'Valley', 'Keller', 'flightless', 'cuckoo', 'Bunyan', 'ox', 'Child', 'suburban', 'Feminine', 'Mystique', '5.9', 'virus', 'HIV', 'Frederick', 'Holy', 'ingredient', 'yogurt', 'Renaissance', 'Kyriakos', 'Theotokopoulos', '219', 'Archie', 'writers', 'Smothers', 'Comedy', 'Hour', 'Nicklaus', 'Professional', 'Golfers', 'Association', 'tour', 'Midsummer', 'Dream', 'Fang', 'Tooth', 'Pookie', 'Cisalpine', 'passing', 'contents', 'cuckquean', 'BIOS', 'residence', 'kings', 'Edinburgh', 'Yiddish', 'Theater', 'barnstorming', 'recently', 'revive', 'nebbish', 'Neil', 'Grenada', 'commodity', 'Wittenberg', 'twenty', 'twirl', 'ballet', 'patrons', 'Stonewall', 'Greenwich', 'Village', 'balance', 'goes', 'snap', 'crackle', 'within', 'webpage', 'Basketball', 'Associaton', 'Havlicek', 'stay', 'Only', 'Twice', 'gives', 'pound', 'Tiffany', 'cookers', 'kitchen', 'Ozzy', 'Osbourne', 'LOL', 'Voyager', 'wished', 'looked', 'respond', 'idea', 'millenium', 'superstar', 'Giant', 'Steps', 'wiener', 'schnitzel', 'Brigham', 'Young', 'traveled', 'Rodeo', 'Cowboys', 'gringo', '1919', 'occurrence', 'unarmed', 'protestors', 'Mile', 'senator', 'Knicks', 'obelisk', 'Getting', 'Married', 'Today', 'jets', 'vapor', 'trail', 'Jennifer', 'Merrie', 'Melodies', 'Walt', 'Disney', 'none', 'employees', 'service', 'vermicilli', 'rigati', 'zitoni', 'tubetti', 'Emma', 'Peel', 'Caesar', 'magnate', 'initials', 'sleeve', 'Padres', 'producers', 'promoters', 'landing', 'controls', 'diamonds', 'rabbit', 'resembled', 'jackass', 'Corvette', 'Tornado', 'void', 'pulse', 'wise', 'Esquire', 'Ash', 'Hole', '2001', 'Odyssey', 'leaky', 'valve', 'enigmatic', 'acquitted', 'treason', 'plot', 'Apartheid', 'Deadwood', 'territory', 'ukulele', 'Valentine', 'anti', 'D.H.', 'Lawrence', 'Tenderness', 'C.C.', 'Magee', 'Ferry', 'Napolean', 'Jena', 'Auerstadt', 'Cody', 'Abominable', 'Snowman', 'wander', 'kill', 'Community', 'Chest', 'Drink', 'thine', 'Apollo', 'minded', 'Armstrong', 'saute', 'Petrified', 'Eleven', 'opener', 'obtained', 'recruits', 'oilseeds', 'thru', 'genetics', 'mushroom', 'optical', 'weakest', 'Yes', 'Can', 'ear', 'hear', 'presided', 'hackers', 'Cuckoo', 'Egg', 'Tracking', 'Spy', 'Through', 'Maze', 'Espionage', 'myth', 'experience', '123', 'Calcutta', 'Dubai', 'concrete', 'Hundred', 'Secret', 'Mitty', 'Zionism', 'meta', 'dragonflies', 'Inuits', 'quantum', 'leaps', 'simpler', 'fascist', 'Snoopy', 'enemy', 'Knute', 'Rockne', 'text', 'massive', 'complex', 'extends', 'Alabama', 'Hooters', '1930s', 'angles', 'isosceles', 'rejection', 'Sun', 'beats', 'Barney', 'Rubble', 'drops', 'drought', 'attendance', 'Last', 'Supper', 'Poems', 'fools', 'teats', 'Lifesaver', 'petroleum', 'solve', 'Webster', 'myself', 'manicure', 'Hope', 'hostages', 'Entebbe', 'rid', 'woodpeckers', 'earthworms', 'pasture', 'Tempelhol', 'stretch', 'besides', 'lifting', 'indicate', '007', 'feud', '1891', 'sand', 'dunes', 'psychologically', 'storms', 'Cherokee', 'roles', 'Streetcar', 'Named', 'Desire', 'lent', 'dot', 'i', 'lens', 'iris', 'course', 'Myrtle', 'Rider', 'shark', 'reliable', 'download', 'Heretic', 'occurs', 'travelers', 'Goldfinger', 'Denmark', 'pit', 'Monument', 'Circle', 'monument', 'spectacle', 'telecast', 'coined', 'cyberspace', 'Neuromancer', 'Statistical', 'Abstract', 'shopping', 'mall', 'Guam', 'fool', 'oath', 'paradise', 'kickoff', 'climbs', 'Bunker', 'ripening', 'accompanied', 'Ghost', 'missions', 'taught', 'Matt', 'Murdock', 'extraordinary', 'abilities', '175', 'tons', 'happy', 'trade', 'Portuguese', 'hope', 'bocci', 'attorneys', 'Defense', 'Free', 'publisher', 'camcorders', 'Malaysia', 'Ileana', 'Cotrubas', 'utilities', 'Casey', 'boss', 'tent', 'figs', 'ripe', 'chronicled', 'Katy', 'Holstrum', 'Congressman', 'Glen', 'Morley', '45Mhz', 'processor', 'acetylsalicylic', 'Spider', 'evaporate', 'Pudding', 'mark', 'describing', 'Godiva', 'chocolates', 'horoscope', 'RCA', 'Coronado', 'bless', 'sneeze', 'exercises', '1927', 'revival', 'bestowed', 'commentary', 'athletes', 'Puerto', 'Rico', 'vowel', 'profit', '836', 'commerce', 'truly', 'tape', 'understand', 'fine', 'entertainment', 'Joan', 'Collins', 'grades', 'closed', 'courier', 'Hohenzollerns', 'Uruguay', 'Edo', 'mixable', 'anyone', 'Around', 'Loop', 'dumplings', 'says', 'don', 'BladeRunner', 'Pepsi', 'Clear', 'cleaner', 'limbo', 'Lucas', 'thumb', 'BUD', 'Maiden', 'acceptance', 'speech', 'prompted', 'yous', 'galaxy', 'formation', 'ninjitsu', 'kung', 'fu', 'Craps', 'swapped', 'families', 'Period', 'length', 'Ukraine', 'onetime', 'socialism', 'highways', 'broadcasting', 'Parma', 'goodness', 'Living', 'Room', 'connected', 'seriously', 'footed', 'Musca', 'domestica', 'enters', 'villain', 'afraid', 'carelessness', 'carefreeness', 'Scientists', 'Stock', 'Exchange', 'poodle', 'domesticated', 'ferret', 'biorhythm', 'robbers', '7th', 'inning', 'successfully', 'liver', 'shelf', 'beside', 'crouching', '1886', 'Tub', 'Immaculate', 'Conception', 'tiles', 'Shuttle', 'jar', 'dispatched', 'cruiser', 'carry', 'Leslie', 'Hornby', 'injectors', 'Silence', 'Lambs', 'compound', 'resource', 'departments', 'ribavirin', 'dam', 'flatfish', 'Bridges', 'Upstairs', 'Downstairs', 'Pesth', 'Buda', 'merged', 'eastern', 'sprouted', 'glowsticks', 'repealed', 'amendment', 'predominant', 'Tab', '1st', 'sense', 'tide', 'ebb', 'dipsomaniac', 'crave', 'tequila', 'galliano', 'Scott', '194', 'delivered', 'newscast', 'hijacked', 'Nuremberg', 'hebephrenia', 'bullfighting', 'article', 'dice', 'mendelevium', 'frog', 'dig', \"'em\", 'appropriates', 'butcher', 'handful', 'eating', 'utensils', 'handicapped', 'referees', 'sought', 'AAA', 'liability', 'skim', 'annotated', 'Coulee', 'Dam', 'Belize', 'slow', 'aging', 'computers', 'FDR', 'MVP', 'means', 'Godfather', 'tornado', 'Green', 'Packers', 'philosophized', 'There', 'nothing', 'stokes', 'Janis', 'Joplin', 'McLean', 'laments', 'Buddy', 'Holly', 'Slightly', 'ahead', '86', 'Socrates', 'Rayburn', 'walked', 'OZ', 'Explorer', 'February', 'D.B.', 'registers', 'trademarks', 'derived', 'biritch', 'Whist', 'Leos', 'mythical', 'hourglass', 'scythe', 'ticker', '1870', 'Tufts', 'sequel', 'Pink', 'Panther', 'Falklands', 'hairless', 'touring', 'Porgy', 'Bess', 'Chicken', 'Pox', 'corners', 'spritsail', 'harmful', 'spray', '43rd', 'VW', 'Beetle', 'changes', 'supercontinent', 'Pangaea', 'break', 'walks', 'multitalented', 'award', 'failed', 'moderated', 'debate', 'concoct', 'methods', 'regulate', 'monopolies', 'suffrage', 'replied', 'begun', 'terrorized', 'Stalker', 'Gourd', 'Ms.', 'expectant', 'motor', 'snoogans', 'possum', 'Trinidad', 'Everybody', 'Comes', 'Rick', 'comprises', 'Highlands', 'Lowlands', 'Uplands', 'including', 'cook', 'Rawhide', 'pitched', 'Power', 'Bars', 'honors', 'Stones', 'Natchitoches', 'Louisiana', 'Mia', 'Farrow', 'reports', 'Morning', '46', '227', 'reviews', 'Turbulent', 'Souls', 'evidence', 'register', 'Cohan', 'Dandy', 'stern', 'fiber', 'stricken', 'polio', '528', 'doctorate', 'hike', 'blinking', 'burns', 'vitamin', 'B12', 'Choo', 'rabies', 'Milton', 'Obote', 'Waverly', 'assign', 'agents', 'fraction', 'beaver', 'geckos', 'variations', 'Canfield', 'Klondike', 'hamburgers', 'steakburgers', 'poing', 'powers', 'weaknesses', 'Lantern', 'impenetrable', 'fortifications', 'frontier', 'satirized', 'countinghouse', 'counting', 'Nasty', 'infomatics', 'mountainous', 'Lhasa', 'Apso', 'native', 'pain', 'abuse', 'choose', 'witnesses', 'execution', 'forward', 'thinking', 'Simon', 'Bakery', 'insert', 'bagels', 'boost', 'predators', 'Antarctica', 'genie', 'conjured', 'shared', 'Nancy', 'Chuck', 'ethnic', 'mill', 'shadows', 'McCall', 'prisoners', 'competitor', 'Trans', 'plagues', 'InterLata', 'possible', 'bid', 'Contract', 'spears', 'Kenya', 'surgeon', 'performed', 'Hurricane', 'Annie', 'neurotic', 'Duane', 'Song', 'Solomon', 'Squares', 'bullets', 'poetic', 'blank', 'verse', 'Pro', 'wrestler', 'cwt', 'elephants', 'rabbits', 'Person', 'combat', 'act', 'Suzette', 'assume', 'Strasbourg', 'oxidation', 'worm', '1980s', 'Pilgrim', 'survivor', 'Dresden', 'firestorm', 'polka', 'dots', 'surfing', 'beaches', 'quantity', 'Martha', 'distinction', 'Thurgood', 'Marshall', 'proof', 'houseplants', 'metabolize', 'carcinogens', 'Dipper', 'Gestapo', 'attends', 'Pencey', 'Prep', 'wingspan', 'condor', 'spamming', 'hen', 'lay', 'sent', 'brief', 'message', 'conquered', 'oddsmaker', 'Snyder', 'compounds', 'categorized', 'bourgeoisie', 'aimed', 'audience', 'hermit', 'crabs', 'reproduce', 'legally', 'testament', 'kilamanjaro', 'varieties', 'apple', 'URL', 'extensions', 'Pictures', 'forged', 'Cliff', 'Robertson', 'check', 'Some', 'newspapers', 'dispose', 'garbage', 'printing', 'bald', 'Presidents', 'Zapper', 'dwarf', 'advertises', 'Frosted', 'Flakes', 'Gleason', 'Bendix', 'flat', 'explosion', 'sphere', 'frogs', 'Dennison', 'doesn', 'www.answers.com', 'Eckley', 'Bombay', 'duck', 'Kindergarden', 'Cop', 'T.S.', 'Eliot', 'false', 'funded', 'Indies', 'fertile', 'postage', 'sardonyx', 'dramatized', 'Scopes', 'tyvek', 'Baryshnikov', 'danced', 'millionth', 'verdict', '1925', 'T.', 'Devo', 'biologist', 'Mack', 'Sennett', 'ticklish', 'bytes', 'terabyte', 'guys', 'whoever', 'finds', 'wins', 'backup', '23', '924', 'rebounds', 'horseshoes', 'bring', 'Ruby', 'M3', 'growth', 'agencies', 'employment', 'verification', 'organic', 'dwellers', 'slane', 'Rita', 'Hayworth', 'Doyle', 'Honeymooners', 'Television', 'promising', 'Kong', 'Jim', 'Bohannon', 'Talk', 'steepest', 'streets', 'quarters', 'furlongs', 'quarter', 'recetrack', 'laid', 'baseman', 'Supergirl', 'penguins', 'Pump', 'entirely', 'idle', 'Romania', 'Fig', 'Newtons', 'filthiest', 'alive', 'monarchy', 'ladybugs', 'actual', 'Fourteenth', 'Rubens', 'Dyck', 'Bruegel', 'citizens', 'Gina', 'donor', 'slime', 'Sunnyside', 'special', 'prosecutor', 'later', 'Karenna', 'Gore', 'constitution', 'relatives', 'Tears', 'Calder', 'Curious', 'curious', 'luggage', 'flier', 'Puccini', 'Boheme', 'indoor', 'Inferno', '111', 'McDonald', 'Wordsworth', 'Budweis', 'capitalism', 'according', 'Max', 'Weber', 'topped', 'Bombshell', '1873', 'drunken', 'drivers', 'mayfly', 'Globe', 'Theatre', 'burn', 'sponsor', 'transistors', 'sacred', 'Breony', 'grocer', 'fingertips', 'S.O.S.', 'Rococo', 'answers.com', 'albums', 'Maldive', 'piece', 'chessboard', 'Confucius', 'Jett', 'monsters', 'Electoral', 'examples', 'stamps', 'OJ', 'doubles', 'pleasure', 'endurance', 'conscious', 'Colonel', 'Edwin', 'Drake', 'drill', 'Paine', 'stomach', 'Alpert', 'Moss', 'Ancient', 'osmosis', 'snafu', 'throw', 'housewarming', 'risk', 'venture', 'bet', 'nihilist', 'Belgium', '1815', 'dirty', '1699', '172', 'McCain', 'Rifleman', 'channel', 'ESPN', 'mandibulofacial', 'dysostosis', 'amen', 'Philebus', 'Certified', 'Nurse', 'Midwife', 'Missionary', 'Researches', '1857', 'Tailors', 'dummy', 'degree', 'Northwestern', 'attic', 'rice', 'importers', 'caber', 'tossing', 'Beany', 'Cecil', 'sailed', 'sued', 'Dannon', 'yougurt', 'Ron', 'Raider', 'promotion', 'Ismail', 'Palace', 'Farouk', 'Body', 'Asiento', 'posh', 'businessman', 'Humor', 'Cream', 'Muslim', 'doctors', 'diagnose', 'blasted', 'valley', 'Mojave', 'fatalism', 'determinism', 'true', 'hepcats', 'isthmus', 'gambler', 'wants', 'gets', 'eels', 'category', 'caucasian', 'Enola', 'log', 'antidisestablishmentarianism', 'identify', 'botanical', 'marvel', 'Nebuchadnezzar', 'Israeli', 'lobsters', 'shiver', 'SAP', 'geographic', 'submerged', 'fringe', 'bikini', 'bathing', 'unusual', '1951', 'NBC', 'Ernie', 'Amelia', 'Earhart', 'disappeared', 'Mauritania', 'tribe', 'Troop', 'perpetually', 'Leon', 'Uris', 'capture', 'microprocessors', 'microcontrollers', 'deadrise', 'commonplace', 'rhomboideus', 'minor', 'panoramic', 'sugar', 'ones', 'scene', 'Ku', 'Klux', 'Klan', 'taxed', 'Leno', 'Rosemary', 'LaBianca', 'Scientology', 'bucks', 'Ty', 'Cobb', 'pines', 'Leif', 'Ericson', 'Parthenon', 'zebras', 'geoscientist', 'darning', 'needles', 'stingers', 'Imam', 'Hussain', 'Shia', 'virtues', 'Sen.', 'Everett', 'Dirkson', '70', 'Li', 'l', 'Abner', 'Nina', 'warmup', 'pitches', 'reliever', 'Englishmen', 'dunk', 'circulation', 'anesthetic', 'allow', 'cop', 'Kindergarten', 'Macarthur', '1767', '1834', 'Sellers', 'Bowls', 'Cowardly', 'Wizard', 'Oz', 'yo', 'yos', 'seccession', 'snarly', 'Shelleen', 'Loco', 'Motion', 'Spock', 'lump', 'Galloping', 'Gourmet', 'veronica', 'developmental', 'stages', 'Bligh', '1960s', '1970s', '50s', 'polis', 'Minneapolis', 'Dita', 'Beard', 'Of', 'yearly', 'Jinnah', 'pecan', 'peanut', 'individual', 'tested', 'entire', 'circumnavigator', 'globe', 'translations', 'phrase', 'Thank', 'Eagle', 'syndrome', 'styloid', 'worms', 'derogatory', 'applied', 'painters', 'Sisley', 'Pissarro', 'Renoir', 'multicolored', 'cube', '42.3', 'quintillion', 'potential', 'combinations', 'managing', 'Apricot', 'RAM', 'Stein', 'Eriksen', 'manatees', 'Shape', 'Up', 'Horlick', 'Hunter', 'Tylo', 'Jacques', 'Cousteau', 'Bank', 'Senator', 'touted', 'Representative', 'Abolitionists', 'Kashmir', 'invasion', 'SCSI', 'values', 'denote', 'Boomer', 'Edition', 'compiled', 'eclairs', 'veins', 'circulatory', 'vamp', 'm', 'jealous', 'bible', 'Sky', 'experts', 'Chernobyl', 'accident', 'loved', 'regulation', 'Maris', '61', 'Cash', 'offered', 'Preservation', 'Favoured', 'Struggle', 'tornadoes', 'Ocho', 'Rios', 'castellated', 'Kremlin', 'snowballs', 'rodder', 'dissented', '1941', 'declaration', 'homeostasis', 'existence', 'Zebulon', 'Pike', 'statues', 'Mauis', 'NY', 'Shakespearean', 'Shylock', 'cognac', 'Calypso', 'thrilled', 'buds', 'Snickers', 'Musketeers', 'impulse', 'hardening', 'equipment', 'Uncle', 'Duke', 'Honey', 'yohimbine', 'tenants', 'adjoining', 'cabinets', 'sprawling', 'airports', 'comparisons', 'prices', 'fabric', 'Kilvington', '18', '327', 'shelves', 'Grimace', 'Mayor', 'McCheese', 'revolutionaries', 'Franz', 'Country', 'Doctor', 'trying', 'powdered', 'mix', 'Sawyer', 'aunt', 'pita', 'Majal', 'Indianapolis', 'corporate', 'Chance', 'Animals', 'Hispaniola', 'alveoli', 'Simple', 'fishing', 'pail', 'bovine', 'accused', 'Trial', 'Janurary', 'culture']\n",
      "Vocabulary Size: 8091\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Iterate through the train dataset tokens and get the unique words to build vocabulary\n",
    "vocab_counter = Counter([word for tokens in train_tokens for word in tokens])\n",
    "print(vocab_counter)\n",
    "train_vocab = list(vocab_counter.keys())\n",
    "print(train_vocab)\n",
    "# Get the size of the vocabulary\n",
    "vocab_size = len(train_vocab)\n",
    "\n",
    "# Answer to 1(a)\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bd170",
   "metadata": {},
   "source": [
    "Using glove model, we attain a vocabulary of size 8091."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79928481",
   "metadata": {},
   "source": [
    "**b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but\n",
    "not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?\n",
    "What is the number of OOV words for each topic category?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075fbcc",
   "metadata": {},
   "source": [
    "OOV words: words in training data that are not found in the glove model's dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33ab0e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of all OOV word occurrences: 202\n",
      "Total number of OOV words in training data: 190\n",
      "Number of OOV words per topic category:\n",
      "Topic DESC: 68 unique OOV words\n",
      "Total occurrence of OOV words:  69\n",
      "--------------------------------------------------\n",
      "[('indiglo', 2), ('osteichthyes', 1), ('86ed', 1), ('p-32', 1), ('characterstics', 1), ('orgin', 1), ('xoxoxox', 1), ('cullion', 1), ('short-', 1), ('knowpost.com', 1)]\n",
      "Topic ENTY: 47 unique OOV words\n",
      "Total occurrence of OOV words:  52\n",
      "--------------------------------------------------\n",
      "[('..', 5), ('daminozide', 2), ('burnford', 1), ('37803', 1), ('pothooks', 1), ('fossilizes', 1), ('chairbound', 1), ('basophobic', 1), ('amendements', 1), ('.tbk', 1)]\n",
      "Topic HUM: 39 unique OOV words\n",
      "Total occurrence of OOV words:  40\n",
      "--------------------------------------------------\n",
      "[('2th', 2), ('microverse', 1), ('9971', 1), ('bergeres', 1), ('jdr3', 1), ('shoplifts', 1), ('deveopment', 1), ('foot-9', 1), ('cassidey', 1), ('vlaja', 1)]\n",
      "Topic ABBR: 4 unique OOV words\n",
      "Total occurrence of OOV words:  4\n",
      "--------------------------------------------------\n",
      "[('.com', 1), ('g.m.t.', 1), ('svhs', 1), ('b.y.o.b.', 1)]\n",
      "Topic NUM: 21 unique OOV words\n",
      "Total occurrence of OOV words:  21\n",
      "--------------------------------------------------\n",
      "[('n.m', 1), ('www.questions.com', 1), ('pregnacy', 1), ('lucelly', 1), ('7847', 1), ('5943', 1), ('superbowls', 1), ('hendecasyllabic', 1), ('manhatten', 1), ('cullions', 1)]\n",
      "Topic LOC: 15 unique OOV words\n",
      "Total occurrence of OOV words:  16\n",
      "--------------------------------------------------\n",
      "[('cotrubas', 2), ('freidreich', 1), ('filenes', 1), ('8/28/1941', 1), ('rednitz', 1), ('adventours', 1), ('nicois', 1), ('kdge', 1), ('bebrenia', 1), ('amazonis', 1)]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from collections import defaultdict, Counter\n",
    "# Get only the GloVe vocab (words with pretrained vectors)\n",
    "glove_vocab = set(glove.stoi.keys())\n",
    "\n",
    "oov_words_by_topic = defaultdict(Counter)\n",
    "\n",
    "for example in train_data.examples:\n",
    "    label = example.label\n",
    "    tokens = example.text\n",
    "    for token in tokens:\n",
    "        # Lowercase token to match GloVe casing\n",
    "        token_lower = token.lower().strip()\n",
    "        if token_lower not in glove_vocab:\n",
    "            oov_words_by_topic[label][token_lower] += 1\n",
    "\n",
    "total_oov_words = set()\n",
    "for counter in oov_words_by_topic.values():\n",
    "    total_oov_words.update(counter.keys())\n",
    "\n",
    "total_count = sum(\n",
    "    count\n",
    "    for counter in oov_words_by_topic.values()\n",
    "    for count in counter.values()\n",
    ")\n",
    "print(f\"Total count of all OOV word occurrences: {total_count}\")\n",
    "print(f\"Total number of OOV words in training data: {len(total_oov_words)}\")\n",
    "print(\"Number of OOV words per topic category:\")\n",
    "\n",
    "for topic, counter in oov_words_by_topic.items():\n",
    "    print(f\"Topic {topic}: {len(counter)} unique OOV words\")\n",
    "    print(\"Total occurrence of OOV words: \", sum(counter.values()))\n",
    "    print(\"-\"*50)\n",
    "    print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad4a56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi['indiglo'])\n",
    "TEXT.vocab.vectors[0]  # Should be 0, as OOV words have no vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9eba1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "['svhs', '.tbk', 'lustrum', 'carefreeness', 'chickenpoxs', 'indiglo', 'mixable', 'lavoir', 'dirkson', '45mhz', 'contibution', 'rigati', 'fossilizes', 'microverse', 'napsylate', 'hebephrenia', 'gametophytic', 'mendelevium', 'dorsets', 'freidreich', 'smartnet', 'deltiologist', 'infomatics', 'inkhorn', 'burnford', 'dipsomaniac', 'interlata', 'haboob', 'antidisestablishmentarianism', 'mcpugg', '2th', 'auerstadt', 'inoco', 'shimerda', '37803', 'biritch', 'it-', 'cassidey', 'cullions', '5943', 'theotokopoulos', 'shoplifts', 'a_tisket', 'tadeus', 'emmigrate', 'chemiosmotic', 'bellworts', 'jeroboams', 'adventours', 'breony', 'recetrack', 'spicey', 'doxat', 'spritsail', 'kdge', 'amazonis', 'countinghouse', 'b.y.o.b.', 'bicornate', 'shelleen', 'crokinole', 'rednitz', 'yougurt', 'cinderslut', 'mandibulofacial', 'ballcock', 'hendecasyllabic', 'cotrubas', 'aim-54c', 'cullion', 'calleda', 'vermicilli', 'skrunch', 'somene', 'tasket', '-lantern', 'osteichthyes', 'madilyn', 'wallbanger', 'recomended', 'gitchee', 'bebrenia', 'zitoni', 'gymnophobia', 'jdr3', 'seriphos', 'pregnacy', 'short-', 'pothooks', 'protanopia', 'phobophobe', 'homerian', 'deveopment', 'p-32', 'g.m.t.', 'daminozide', 'ouarterly', 'seccession', 'aeul', 'www.questions.com', 'kindergarden', '187s', 'mauis', 'knowpost.com', 'pointsettia', 'pencey', 'lucelly', 'zoonose', 'vbp', 'n.m', 'southdowns', 'sinemet', '36893', 'charcter', 'gumee', 'winkerbean', 'rhomboideus', 'verdandi', '12601', 'tubetti', 'no.1', 'pesth', 'tzimisce', '86ed', 'amendements', '.dbf', 'troilism', 'philebus', 'chairbound', 'nicois', 'psorisis', 'snoogans', 'hepcats', 'filenes', 'topophobic', 'tempelhol', 'palmiped', 'cabarnet', 'www.answers.com', '10-', 'garmat', 'chromatology', 'vlaja', 'characterstics', 'dextropropoxyphen', 'coppertop', 'superbowls', 'collabrative', 'janurary', 'steakburgers', 'lutine', 'deadrise', '7847', 'orgin', '.com', 'sysrq', 'dysostosis', 'seborrhea', 'circumnavigator', 'oompas', 'nipsy', 'tastebud', 'polyorchid', 'siphnos', 'hiemal', 'shiest', 'slotbacks', 'vdrl', 'cartoondom', 'doodyville', 'xoxoxox', 'foot-9', 'bergeres', 'respones', 'mccheese', 'arometherapy', 'f25', 'elizabethian', 'kilamanjaro', '9971', 'cuckquean', '..', 'baseemen', 'doegs', 'holstrum', 'manhatten', 'basophobic', 'ninjitsu', 'resurrectionist', '8/28/1941']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Assuming your defaultdict of Counters is named oov_counter_dict\n",
    "def convert_counter_to_list(oov_counter_dict):\n",
    "    oov_set = set()\n",
    "    for counter in oov_counter_dict.values():\n",
    "        oov_set.update(counter.keys())\n",
    "    return list(oov_set)\n",
    "oov_words = convert_counter_to_list(oov_words_by_topic)\n",
    "print(len(oov_words))\n",
    "print(oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee5733",
   "metadata": {},
   "source": [
    "**(c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc56dd7",
   "metadata": {},
   "source": [
    "## 1. Use lemmatisation\n",
    "Lemmatisation reduces words to their base form, which may be present in the Glove dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4650a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "wn_lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_token(token):\n",
    "    \"\"\"\n",
    "    Lemmatizes the given token using WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    return wn_lem.lemmatize(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff89dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_embedding_from_glove(token):\n",
    "    \"\"\"\n",
    "    Get embedding vector for token from glove object.\n",
    "    Returns numpy array or None if not found.\n",
    "    \"\"\"\n",
    "    lemma_token = lemmatize_token(token)\n",
    "    if lemma_token in glove.stoi:\n",
    "        idx = glove.stoi[lemma_token]\n",
    "        return glove.vectors[idx].cpu().numpy()\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49cd2b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized match found: Original 'chickenpoxs' -> Lemma 'chickenpox'\n",
      "Lemmatized match found: Original 'jeroboams' -> Lemma 'jeroboam'\n",
      "Lemmatized match found: Original 'mauis' -> Lemma 'maui'\n",
      "{'chickenpoxs': 'chickenpox', 'jeroboams': 'jeroboam', 'mauis': 'maui'}\n"
     ]
    }
   ],
   "source": [
    "def find_lemmatized_oov_matches(oov_words):\n",
    "    found_words = {}\n",
    "    for word in oov_words:\n",
    "        lemma = lemmatize_token(word)\n",
    "        if lemma in glove.stoi:\n",
    "            found_words[word] = lemma\n",
    "            print(f\"Lemmatized match found: Original '{word}' -> Lemma '{lemma}'\")\n",
    "    return found_words\n",
    "lemmad_oov_words = find_lemmatized_oov_matches(oov_words)\n",
    "print(lemmad_oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91210f3",
   "metadata": {},
   "source": [
    "We found 3 OOV word embeddings from lemmatization. We shall save these OOV words and update their embeddings with the embeddings of the lemmatized word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f94c07",
   "metadata": {},
   "source": [
    "Lemmatization seems to be ineffective in reducing OOV words. Let's try a different approach, fuzzy matching. Misspelled words contribute to a large proportion of the OOV word pool so we set a threshold of 1 for finding the correctly spelled word. We set the minimum Levenshtein distance at 90 to filter out low quality matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e6954f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy matched OOV words:  {'indiglo': 'indigo', 'lavoir': 'avoir', 'dirkson': 'dikson', 'contibution': 'contribution', 'rigati': 'brigati', 'fossilizes': 'fossilize', 'dorsets': 'dorset', 'freidreich': 'freidrich', 'infomatics': 'informatics', 'burnford': 'burford', 'inoco': 'tinoco', 'cassidey': 'cassidy', 'cullions': 'cullins', 'shoplifts': 'shoplift', 'tadeus': 'tadeusz', 'emmigrate': 'emigrate', 'breony': 'breon', 'spicey': 'spice', 'amazonis': 'amazons', 'yougurt': 'yogurt', 'ballcock': 'allcock', 'aim-54c': 'aim-54', 'cullion': 'scullion', 'calleda': 'called', 'skrunch': 'krunch', 'somene': 'someone', '-lantern': 'lantern', 'madilyn': 'madlyn', 'recomended': 'recommended', 'zitoni': 'zitouni', 'pregnacy': 'pregnancy', 'short-': 'short', 'deveopment': 'development', 'seccession': 'secession', 'kindergarden': 'kindergarten', 'pointsettia': 'poinsettia', 'pencey': 'pence', 'zoonose': 'zoonoses', 'southdowns': 'southdown', 'charcter': 'character', 'gumee': 'gumede', 'amendements': 'amendments', 'nicois': 'nicoise', 'psorisis': 'psoriasis', 'hepcats': 'hepcat', 'filenes': 'filene', 'cabarnet': 'cabaret', 'garmat': 'garma', 'characterstics': 'characteristics', 'dextropropoxyphen': 'dextropropoxyphene', 'superbowls': 'superbowl', 'collabrative': 'collaborative', 'janurary': 'january', 'lutine': 'lutin', 'orgin': 'origin', 'circumnavigator': 'circumnavigation', 'oompas': 'loompas', 'nipsy': 'nipsey', 'tastebud': 'tastebuds', 'hiemal': 'himal', 'shiest': 'shies', 'slotbacks': 'slotback', 'bergeres': 'bergere', 'respones': 'responses', 'arometherapy': 'aromatherapy', 'elizabethian': 'elizabethan', 'kilamanjaro': 'kilimanjaro', 'baseemen': 'basemen', 'resurrectionist': 'resurrectionists'}\n",
      "Number of fuzzy matched OOV words:  69\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import process\n",
    "\n",
    "def fuzzy_match(word, choices, length_tolerance=1):\n",
    "    # Filter candidates by length heuristic: length within length_tolerance of original word\n",
    "    filtered_choices = [c for c in choices if abs(len(c) - len(word)) <= length_tolerance]\n",
    "\n",
    "    if not filtered_choices:\n",
    "        return None  # no match found within length heuristic\n",
    "\n",
    "    match = process.extractOne(word, filtered_choices)\n",
    "    # print(match, \" vs \",  word)\n",
    "    return match\n",
    "\n",
    "fuzzy_matched_oov = dict()\n",
    "oov_words_no_lemma = [word for word in oov_words if word not in lemmad_oov_words.keys()]\n",
    "for word in oov_words_no_lemma:\n",
    "    match = fuzzy_match(word, glove.stoi.keys())\n",
    "    if match[1] > 90 and word not in fuzzy_matched_oov:\n",
    "        fuzzy_matched_oov[word] = match[0]\n",
    "print(\"Fuzzy matched OOV words: \", fuzzy_matched_oov)\n",
    "print(\"Number of fuzzy matched OOV words: \", len(fuzzy_matched_oov))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6308da",
   "metadata": {},
   "source": [
    "## 2. Fasttext with Subword Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5ffb742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "#reduce to 100 dimensions\n",
    "# fasttext.util.reduce_model(ft, 100)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "204a0d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings found by FastText model for the following OOV words:  {'lustrum', 'carefreeness', 'spritsail', 'mixable', 'steakburgers', 'deadrise', '7847', '.com', 'dysostosis', 'gametophytic', 'seborrhea', 'mendelevium', 'hendecasyllabic', '12601', 'deltiologist', 'inkhorn', 'dipsomaniac', 'haboob', 'antidisestablishmentarianism', '2th', '86ed', '.dbf', 'xoxoxox', 'it-', '5943', 'protanopia', '9971', 'cuckquean', '..', 'chemiosmotic', 'www.answers.com', '10-', 'ninjitsu'}\n",
      "Number of OOV word embeddings created from FastText model: 33\n"
     ]
    }
   ],
   "source": [
    "# Function to get FastText embeddings, with subword handling\n",
    "def get_fasttext_embedding(model, word):\n",
    "    return model[word]\n",
    "\n",
    "oov_words_no_lemma_no_fuzzy = [word for word in oov_words if word not in lemmad_oov_words and word not in fuzzy_matched_oov.keys()]\n",
    "# Initialize OOV counters\n",
    "oov_to_ft = set()\n",
    "# Loop through each word in the vocabulary\n",
    "for word in train_vocab:\n",
    "    # Check if the word is in the vocabulary of the FastText model\n",
    "    if word in ft and word in oov_words_no_lemma_no_fuzzy:\n",
    "        oov_to_ft.add(word)\n",
    "\n",
    "print(\"embeddings found by FastText model for the following OOV words: \", oov_to_ft)\n",
    "print(\"Number of OOV word embeddings created from FastText model:\", len(oov_to_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9ae9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "Number of OOV words according to both GloVE, Lemmatization and FastText:  105\n",
      "{'svhs', '.tbk', '45mhz', 'microverse', 'napsylate', 'hebephrenia', 'smartnet', 'mcpugg', 'interlata', 'auerstadt', 'shimerda', '37803', 'biritch', 'theotokopoulos', 'a_tisket', 'bellworts', 'adventours', 'recetrack', 'doxat', 'kdge', 'countinghouse', 'b.y.o.b.', 'bicornate', 'shelleen', 'crokinole', 'rednitz', 'cinderslut', 'mandibulofacial', 'cotrubas', 'vermicilli', 'tasket', 'osteichthyes', 'wallbanger', 'gitchee', 'bebrenia', 'gymnophobia', 'jdr3', 'seriphos', 'pothooks', 'phobophobe', 'g.m.t.', 'homerian', 'p-32', 'daminozide', 'ouarterly', 'aeul', 'www.questions.com', '187s', 'knowpost.com', 'lucelly', 'vbp', 'n.m', 'sinemet', '36893', 'winkerbean', 'rhomboideus', 'verdandi', 'tubetti', 'no.1', 'pesth', 'tzimisce', 'troilism', 'philebus', 'chairbound', 'snoogans', 'topophobic', 'tempelhol', 'palmiped', 'chromatology', 'vlaja', 'coppertop', 'sysrq', 'polyorchid', 'siphnos', 'vdrl', 'cartoondom', 'doodyville', 'foot-9', 'mccheese', 'f25', 'doegs', 'holstrum', 'manhatten', 'basophobic', '8/28/1941'}\n"
     ]
    }
   ],
   "source": [
    "print(len(oov_words_no_lemma_no_fuzzy))\n",
    "remaining_oov_words = set(oov_words_no_lemma_no_fuzzy) - oov_to_ft\n",
    "print(\"Number of OOV words according to both GloVE, Lemmatization and FastText: \", 190 - len(remaining_oov_words))\n",
    "print(remaining_oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef136a26",
   "metadata": {},
   "source": [
    "### Total OOV words replaced by words in vocab: 105\n",
    "~55% reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37450cbd",
   "metadata": {},
   "source": [
    "The remaining OOV words are rare words and their embeddings will be estimated. We avoid unknown tokens as they are represented by zero vectors which are unhelpful and potentially introduces bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b089c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of OOV words to embeddings using lemmatization and context fallback\n",
    "def create_oov_embedding_dict(training_sentences, window_size=2):\n",
    "    oov_embeddings = {}\n",
    "\n",
    "    for sentence in training_sentences:\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in glove.stoi and token not in oov_embeddings:\n",
    "                #case for lemmatized words\n",
    "                if token in lemmad_oov_words.keys():\n",
    "                    glove_vector = glove.vectors[glove.stoi[lemmad_oov_words[token]]]\n",
    "                    oov_embeddings[token] = glove_vector\n",
    "                elif token in fuzzy_matched_oov.keys():\n",
    "                    glove_vector = glove.vectors[glove.stoi[fuzzy_matched_oov[token]]]\n",
    "                    oov_embeddings[token] = glove_vector\n",
    "                elif token in oov_to_ft:\n",
    "                    ft_vector = get_fasttext_embedding(ft, token)\n",
    "                    oov_embeddings[token] = ft_vector\n",
    "                elif token in remaining_oov_words:\n",
    "                    embedding = np.mean([glove[word] for word in sentence], axis=0)\n",
    "                    if embedding is not None:\n",
    "                        oov_embeddings[token] = embedding\n",
    "    return oov_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "433c930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [example.text for example in train_data.examples]\n",
    "\n",
    "oov_embedding_dict = create_oov_embedding_dict(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4ba05",
   "metadata": {},
   "source": [
    "Now we update the embeddings in training vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41fda4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced embedding for 'osteichthyes' at index 6987\n",
      "Replaced embedding for '86ed' at index 3193\n",
      "Replaced embedding for 'burnford' at index 0\n",
      "Replaced embedding for 'p-32' at index 0\n",
      "Replaced embedding for '37803' at index 3161\n",
      "Replaced embedding for 'characterstics' at index 5718\n",
      "Replaced embedding for 'orgin' at index 6985\n",
      "Replaced embedding for 'xoxoxox' at index 8069\n",
      "Replaced embedding for 'microverse' at index 0\n",
      "Replaced embedding for '.com' at index 3050\n",
      "Replaced embedding for 'n.m' at index 0\n",
      "Replaced embedding for 'pothooks' at index 7153\n",
      "Replaced embedding for 'freidreich' at index 0\n",
      "Replaced embedding for 'cullion' at index 5933\n",
      "Replaced embedding for 'g.m.t.' at index 0\n",
      "Replaced embedding for 'short-' at index 7508\n",
      "Replaced embedding for '9971' at index 3199\n",
      "Replaced embedding for 'knowpost.com' at index 0\n",
      "Replaced embedding for 'www.questions.com' at index 8067\n",
      "Replaced embedding for 'bergeres' at index 0\n",
      "Replaced embedding for 'pregnacy' at index 7170\n",
      "Replaced embedding for 'contibution' at index 5862\n",
      "Replaced embedding for 'fossilizes' at index 6347\n",
      "Replaced embedding for 'troilism' at index 7876\n",
      "Replaced embedding for 'chairbound' at index 5708\n",
      "Replaced embedding for 'basophobic' at index 5496\n",
      "Replaced embedding for 'lucelly' at index 0\n",
      "Replaced embedding for '2th' at index 1916\n",
      "Replaced embedding for 'filenes' at index 0\n",
      "Replaced embedding for 'jdr3' at index 0\n",
      "Replaced embedding for 'inkhorn' at index 6584\n",
      "Replaced embedding for 'shoplifts' at index 7504\n",
      "Replaced embedding for 'deveopment' at index 6008\n",
      "Replaced embedding for '7847' at index 3186\n",
      "Replaced embedding for '5943' at index 3178\n",
      "Replaced embedding for 'foot-9' at index 6332\n",
      "Replaced embedding for 'cassidey' at index 0\n",
      "Replaced embedding for 'amendements' at index 5383\n",
      "Replaced embedding for 'palmiped' at index 7015\n",
      "Replaced embedding for '8/28/1941' at index 3188\n",
      "Replaced embedding for 'svhs' at index 0\n",
      "Replaced embedding for 'cabarnet' at index 0\n",
      "Replaced embedding for '.tbk' at index 3052\n",
      "Replaced embedding for 'homerian' at index 0\n",
      "Replaced embedding for 'coppertop' at index 5876\n",
      "Replaced embedding for 'sysrq' at index 0\n",
      "Replaced embedding for 'rednitz' at index 0\n",
      "Replaced embedding for 'verdandi' at index 7938\n",
      "Replaced embedding for 'superbowls' at index 0\n",
      "Replaced embedding for 'topophobic' at index 7824\n",
      "Replaced embedding for 'hendecasyllabic' at index 6492\n",
      "Replaced embedding for 'vlaja' at index 0\n",
      "Replaced embedding for 'adventours' at index 0\n",
      "Replaced embedding for 'sinemet' at index 0\n",
      "Replaced embedding for 'manhatten' at index 0\n",
      "Replaced embedding for 'polyorchid' at index 7130\n",
      "Replaced embedding for 'ouarterly' at index 0\n",
      "Replaced embedding for 'garmat' at index 0\n",
      "Replaced embedding for '.dbf' at index 3051\n",
      "Replaced embedding for 'resurrectionist' at index 7357\n",
      "Replaced embedding for 'a_tisket' at index 0\n",
      "Replaced embedding for 'tasket' at index 0\n",
      "Replaced embedding for 'haboob' at index 6453\n",
      "Replaced embedding for 'tastebud' at index 7746\n",
      "Replaced embedding for 'chickenpoxs' at index 5730\n",
      "Replaced embedding for 'bellworts' at index 0\n",
      "Replaced embedding for 'pointsettia' at index 7119\n",
      "Replaced embedding for 'tzimisce' at index 0\n",
      "Replaced embedding for 'nipsy' at index 0\n",
      "Replaced embedding for 'vbp' at index 0\n",
      "Replaced embedding for 'mcpugg' at index 0\n",
      "Replaced embedding for 'protanopia' at index 7213\n",
      "Replaced embedding for 'zoonose' at index 8092\n",
      "Replaced embedding for 'charcter' at index 5719\n",
      "Replaced embedding for 'doegs' at index 0\n",
      "Replaced embedding for '..' at index 743\n",
      "Replaced embedding for 'cullions' at index 5934\n",
      "Replaced embedding for 'chromatology' at index 5748\n",
      "Replaced embedding for 'lustrum' at index 6740\n",
      "Replaced embedding for '36893' at index 3160\n",
      "Replaced embedding for 'ballcock' at index 5482\n",
      "Replaced embedding for 'daminozide' at index 2508\n",
      "Replaced embedding for 'lavoir' at index 0\n",
      "Replaced embedding for 'cartoondom' at index 5684\n",
      "Replaced embedding for 'wallbanger' at index 0\n",
      "Replaced embedding for 'nicois' at index 0\n",
      "Replaced embedding for 'deltiologist' at index 5983\n",
      "Replaced embedding for 'somene' at index 7576\n",
      "Replaced embedding for 'kdge' at index 0\n",
      "Replaced embedding for 'recomended' at index 7298\n",
      "Replaced embedding for 'baseemen' at index 5492\n",
      "Replaced embedding for 'phobophobe' at index 7073\n",
      "Replaced embedding for 'doodyville' at index 0\n",
      "Replaced embedding for 'no.1' at index 0\n",
      "Replaced embedding for 'elizabethian' at index 0\n",
      "Replaced embedding for 'seborrhea' at index 7457\n",
      "Replaced embedding for 'f25' at index 0\n",
      "Replaced embedding for 'chemiosmotic' at index 5724\n",
      "Replaced embedding for 'shimerda' at index 0\n",
      "Replaced embedding for 'indiglo' at index 0\n",
      "Replaced embedding for 'vdrl' at index 0\n",
      "Replaced embedding for 'tadeus' at index 0\n",
      "Replaced embedding for 'arometherapy' at index 5430\n",
      "Replaced embedding for 'bebrenia' at index 0\n",
      "Replaced embedding for 'amazonis' at index 0\n",
      "Replaced embedding for '12601' at index 3062\n",
      "Replaced embedding for 'skrunch' at index 0\n",
      "Replaced embedding for 'oompas' at index 0\n",
      "Replaced embedding for 'hiemal' at index 6497\n",
      "Replaced embedding for 'aeul' at index 0\n",
      "Replaced embedding for 'doxat' at index 0\n",
      "Replaced embedding for 'madilyn' at index 0\n",
      "Replaced embedding for '-lantern' at index 3049\n",
      "Replaced embedding for 'gametophytic' at index 6382\n",
      "Replaced embedding for 'gitchee' at index 0\n",
      "Replaced embedding for 'gumee' at index 0\n",
      "Replaced embedding for '10-' at index 3055\n",
      "Replaced embedding for 'lutine' at index 0\n",
      "Replaced embedding for 'it-' at index 6614\n",
      "Replaced embedding for 'aim-54c' at index 0\n",
      "Replaced embedding for 'collabrative' at index 5785\n",
      "Replaced embedding for 'respones' at index 7354\n",
      "Replaced embedding for 'gymnophobia' at index 6450\n",
      "Replaced embedding for 'inoco' at index 0\n",
      "Replaced embedding for 'bicornate' at index 5531\n",
      "Replaced embedding for '187s' at index 3097\n",
      "Replaced embedding for 'psorisis' at index 7220\n",
      "Replaced embedding for 'shiest' at index 7496\n",
      "Replaced embedding for 'winkerbean' at index 0\n",
      "Replaced embedding for 'calleda' at index 5655\n",
      "Replaced embedding for 'slotbacks' at index 7551\n",
      "Replaced embedding for 'emmigrate' at index 6153\n",
      "Replaced embedding for 'b.y.o.b.' at index 0\n",
      "Replaced embedding for 'spicey' at index 7605\n",
      "Replaced embedding for 'cinderslut' at index 0\n",
      "Replaced embedding for 'dorsets' at index 0\n",
      "Replaced embedding for 'southdowns' at index 0\n",
      "Replaced embedding for 'jeroboams' at index 6628\n",
      "Replaced embedding for 'crokinole' at index 0\n",
      "Replaced embedding for 'smartnet' at index 0\n",
      "Replaced embedding for 'dextropropoxyphen' at index 2524\n",
      "Replaced embedding for 'napsylate' at index 2762\n",
      "Replaced embedding for 'siphnos' at index 0\n",
      "Replaced embedding for 'seriphos' at index 0\n",
      "Replaced embedding for 'theotokopoulos' at index 0\n",
      "Replaced embedding for 'cuckquean' at index 5932\n",
      "Replaced embedding for 'vermicilli' at index 7941\n",
      "Replaced embedding for 'rigati' at index 7378\n",
      "Replaced embedding for 'zitoni' at index 8088\n",
      "Replaced embedding for 'tubetti' at index 7883\n",
      "Replaced embedding for 'auerstadt' at index 0\n",
      "Replaced embedding for 'tempelhol' at index 0\n",
      "Replaced embedding for 'cotrubas' at index 0\n",
      "Replaced embedding for 'holstrum' at index 0\n",
      "Replaced embedding for '45mhz' at index 0\n",
      "Replaced embedding for 'mixable' at index 6847\n",
      "Replaced embedding for 'ninjitsu' at index 6931\n",
      "Replaced embedding for 'carefreeness' at index 5680\n",
      "Replaced embedding for 'pesth' at index 0\n",
      "Replaced embedding for 'dipsomaniac' at index 6029\n",
      "Replaced embedding for 'hebephrenia' at index 6485\n",
      "Replaced embedding for 'mendelevium' at index 6806\n",
      "Replaced embedding for 'biritch' at index 5546\n",
      "Replaced embedding for 'spritsail' at index 7620\n",
      "Replaced embedding for 'snoogans' at index 7566\n",
      "Replaced embedding for 'steakburgers' at index 7639\n",
      "Replaced embedding for 'countinghouse' at index 5892\n",
      "Replaced embedding for 'infomatics' at index 6576\n",
      "Replaced embedding for 'interlata' at index 0\n",
      "Replaced embedding for 'pencey' at index 0\n",
      "Replaced embedding for 'kilamanjaro' at index 6649\n",
      "Replaced embedding for 'www.answers.com' at index 8066\n",
      "Replaced embedding for 'kindergarden' at index 0\n",
      "Replaced embedding for 'recetrack' at index 7295\n",
      "Replaced embedding for 'breony' at index 0\n",
      "Replaced embedding for 'mandibulofacial' at index 6765\n",
      "Replaced embedding for 'dysostosis' at index 6106\n",
      "Replaced embedding for 'philebus' at index 0\n",
      "Replaced embedding for 'yougurt' at index 8079\n",
      "Replaced embedding for 'hepcats' at index 6493\n",
      "Replaced embedding for 'antidisestablishmentarianism' at index 5403\n",
      "Replaced embedding for 'deadrise' at index 5961\n",
      "Replaced embedding for 'rhomboideus' at index 7369\n",
      "Replaced embedding for 'dirkson' at index 0\n",
      "Replaced embedding for 'seccession' at index 7458\n",
      "Replaced embedding for 'shelleen' at index 0\n",
      "Replaced embedding for 'circumnavigator' at index 5757\n",
      "Replaced embedding for 'mauis' at index 0\n",
      "Replaced embedding for 'mccheese' at index 0\n",
      "Replaced embedding for 'janurary' at index 0\n",
      "Total OOV embeddings replaced: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/2xkb6hns26q621bjtf6rkkmc0000gn/T/ipykernel_12623/3097141130.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  TEXT.vocab.vectors[idx] = torch.tensor(embedding, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Replace OOV vectors in TEXT.vocab.vectors inplace and print progress\n",
    "def replace_oov_vectors(TEXT, oov_embeddings):\n",
    "    count = 0\n",
    "    lowercase_stoi = {key.lower(): value for key, value in TEXT.vocab.stoi.items() if isinstance(key, str)}\n",
    "    for oov_word, embedding in oov_embeddings.items():\n",
    "        if oov_word in lowercase_stoi:\n",
    "            idx = TEXT.vocab.stoi[oov_word]\n",
    "            TEXT.vocab.vectors[idx] = torch.tensor(embedding, dtype=torch.float32)\n",
    "            print(f\"Replaced embedding for '{oov_word}' at index {idx}\")\n",
    "            count += 1\n",
    "        else:\n",
    "            print(oov_word)\n",
    "    print(f\"Total OOV embeddings replaced: {count}\")\n",
    "replace_oov_vectors(TEXT, oov_embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83c3e8",
   "metadata": {},
   "source": [
    "**d) Select the 20 most frequent words from each topic category in the training set (removing\n",
    "stopwords if necessary). Retrieve their pretrained embeddings (from Word2Vec or GloVe).\n",
    "Project these embeddings into 2D space (using e.g., t-SNE or Principal Component Analysis).\n",
    "Plot the points in a scatter plot, color-coded by their topic category. Attach your plot here.\n",
    "Analyze your findings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2a9f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/asherlim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import torchtext\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Collect tokens by topic, remove stopwords, non-alpha tokens\n",
    "tokens_by_topic = defaultdict(list)\n",
    "for example in train_data.examples:\n",
    "    label = example.label\n",
    "    tokens = example.text\n",
    "    filtered_tokens = [t.lower() for t in tokens if t.isalpha() and t.lower() not in stop_words]\n",
    "    tokens_by_topic[label].extend(filtered_tokens)\n",
    "\n",
    "# Select top 20 frequent words by topic\n",
    "top_words_by_topic = {}\n",
    "for topic, tokens in tokens_by_topic.items():\n",
    "    counter = Counter(tokens)\n",
    "    top_words_by_topic[topic] = [w for w, _ in counter.most_common(20)]\n",
    "\n",
    "# Retrieve pretrained embeddings for these words\n",
    "vectors = TEXT.vocab.vectors\n",
    "vocab_stoi = TEXT.vocab.stoi\n",
    "\n",
    "word_vectors = []\n",
    "labels_for_words = []\n",
    "words_flat = []\n",
    "\n",
    "for topic, words in top_words_by_topic.items():\n",
    "    for word in words:\n",
    "        idx = vocab_stoi.get(word, -1)\n",
    "        if idx >= 0:\n",
    "            vec = vectors[idx].numpy()\n",
    "            word_vectors.append(vec)\n",
    "            labels_for_words.append(topic)\n",
    "            words_flat.append(word)\n",
    "\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# t-SNE projection to 2D\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot scatter plot color-coded by topics\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.tab10.colors\n",
    "unique_topics = sorted(top_words_by_topic.keys())\n",
    "color_dict = {topic: colors[i % 10] for i, topic in enumerate(unique_topics)}\n",
    "\n",
    "for topic in unique_topics:\n",
    "    indices = [i for i, t in enumerate(labels_for_words) if t == topic]\n",
    "    plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1],\n",
    "                c=[color_dict[topic]], label=topic)\n",
    "\n",
    "for i, word in enumerate(words_flat):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8)\n",
    "\n",
    "plt.legend(title='Topic Category')\n",
    "plt.title('t-SNE projection of top 20 frequent words by topic in TREC training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4679759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pack(dataset):\n",
    "    return [\n",
    "        {\"tokens\": list(example.text), \"label\": example.label}\n",
    "        for example in dataset.examples\n",
    "    ]\n",
    "\n",
    "artifacts = {\n",
    "    \"text_field_kwargs\": {\n",
    "        \"tokenize\": \"spacy\",\n",
    "        \"tokenizer_language\": \"en_core_web_sm\",\n",
    "        \"include_lengths\": True,\n",
    "        \"pad_token\": TEXT.pad_token,\n",
    "        \"unk_token\": TEXT.unk_token,\n",
    "    },\n",
    "    \"label_field_kwargs\": {},          # keep for symmetry\n",
    "    \"text_vocab_itos\": list(TEXT.vocab.itos),\n",
    "    \"text_vocab_vectors\": TEXT.vocab.vectors.cpu(),\n",
    "    \"label_vocab_itos\": list(LABEL.vocab.itos),\n",
    "    \"train_examples\": _pack(train_data),\n",
    "    \"valid_examples\": _pack(valid_data),\n",
    "    \"test_examples\": _pack(test_data),\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "torch.save(artifacts, \"trec_artifacts.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4ac86",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Distinct Topic Clusters: Some topic categories (like LOC for locations and HUM for human/person entities) show partial clustering, indicating that frequent words in these topics are semantically related in the embedding space.\n",
    "\n",
    "Overlap Between Topics: There is overlap between certain categories, especially between NUM (numerical entities), DESC (descriptive), and ENTY (entities), revealing that these categories share common or semantically similar vocabularies (e.g., number, year, name).\n",
    "\n",
    "Topic-specific Outliers: Certain words such as baseball (NUM), capital (LOC), actor (HUM) are positioned toward the periphery, suggesting that they have more unique, topic-centric semantic content compared to other frequent words.\n",
    "\n",
    "Embedding Quality: The visualization demonstrates that GloVe embeddings capture topical and semantic relationships to a large extent; words with similar meanings or roles often emerge closer together regardless of label.\n",
    "\n",
    "Ambiguity & Polysemy: Some words appear in the vicinity of different clusters or overlap areas (e.g., mean in DESC and NUM, movie in HUM and ENTY), reflecting ambiguity or multi-topic relevance.\n",
    "\n",
    "Vocabulary Sharing Across Topics: Frequent generic words (name, meaning, state, form) are distributed among various topics, supporting the linguistically expected overlap among question categories.\n",
    "\n",
    "Dimensionality Reduction Artifacts: t-SNE is nonlinear and can distort true distances, so while relative local clusters are meaningful, global distances should not be overinterpreted.\n",
    "\n",
    "Label Noise: Some outliers may be artifacts of labeling or low sample counts; actual use in downstream classification should consider this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30792392",
   "metadata": {},
   "source": [
    "## End of Qn 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e2a5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Qn 2: Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cea7f8",
   "metadata": {},
   "source": [
    "**(a) Report the final configuration of your best model, namely: the number of training epochs, learning rate, optimizer, batch size and hidden dimension.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1c7c0",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "Here our approach is to do hyperparameter tuning by trying out different combinations of learning rate, optimizers, batch size and number of hidden dimensions.\n",
    "1. Learning Rate: [0.0001, 0.0005, 0.001, 0.005]\n",
    "2. Optimizers: [SGD, AdaGrad, Adam, RMSprop]\n",
    "3. Batch Size: [32. 64, 128]\n",
    "4. Number of Hidden Dimensions: [64, 128, 256]\n",
    "\n",
    "For the number of training epochs, we implemented early stopping here, and so we will report that as the number obtained for this part.\n",
    "\n",
    "Note: Although we could have done cross validation through methods like GridSearchCV and RandomSearchCV, we had a huge parameter list to search from and doing even 2/3 fold CV would be computationally demanding. We also made this decision after running the initial hyperparam tuning and we decided that the results are decent enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25d4b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class for Qn 2 - Make them learnable parameters\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, text, label, vocab):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.text[idx].split()\n",
    "        label = self.label[idx]\n",
    "        indices = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] for token in tokens]\n",
    "\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=TEXT.vocab.stoi['<pad>'])\n",
    "    labels = torch.stack(labels)\n",
    "    return texts_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "023d3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4362\n",
      "4362\n"
     ]
    }
   ],
   "source": [
    "# Create train, valid, test texts and labels\n",
    "train_texts = [\" \".join(example.text) for example in train_data.examples]\n",
    "train_labels = [LABEL.vocab.stoi[example.label] for example in train_data.examples]\n",
    "\n",
    "valid_texts = [\" \".join(example.text) for example in valid_data.examples]\n",
    "valid_labels = [LABEL.vocab.stoi[example.label] for example in valid_data.examples]\n",
    "\n",
    "test_texts = [\" \".join(example.text) for example in test_data.examples]\n",
    "test_labels = [LABEL.vocab.stoi[example.label] for example in test_data.examples]\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SentenceDataset(train_texts, train_labels, TEXT.vocab.stoi)\n",
    "valid_dataset = SentenceDataset(valid_texts, valid_labels, TEXT.vocab.stoi)\n",
    "test_dataset = SentenceDataset(test_texts, test_labels, TEXT.vocab.stoi)\n",
    "\n",
    "# Check length of train dataset and train data\n",
    "print(len(train_dataset))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2208c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN Classifier, here we do only a single hidden layer to keep it small and simple\n",
    "# We take input word embeddings, pass through RNN, apply dropout, and then a fully connected layer to get logits for 6 classes (as per qn requirement)\n",
    "class ClassifierRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, dropout=0.0):\n",
    "        super(ClassifierRNN, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=False)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 6) # since 6 possible labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = self.dropout(hidden[-1]) # Default is to take last layer's hidden state\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fcd84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training, evaluation, and testing loops\n",
    "def train_loop(model, loader, optimizer, criterion, grad_clip=False, max_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for texts, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_correct = total_correct / len(loader.dataset)\n",
    "    return avg_loss, avg_correct\n",
    "\n",
    "def eval_loop(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_correct = total_correct / len(loader.dataset)\n",
    "    return avg_loss, avg_correct\n",
    "\n",
    "def test_loop(model, loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            outputs = model(texts)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(loader.dataset)\n",
    "    return acc\n",
    "\n",
    "# Early stopper to prevent overfitting as default to determine a suitable num_epochs,\n",
    "# here we use val_acc as the metrics as recommended metrics for training (as per qn requirements)\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_acc = float('-inf')\n",
    "\n",
    "    def early_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc + self.min_delta:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ea7a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture metrics and print training results per epoch\n",
    "def training_step(model, train_loader, valid_loader, optimizer, criterion, num_epochs, grad_clip=False, max_norm=1.0):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    valid_losses, valid_accuracies = [], []\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_loop(model, train_loader, optimizer, criterion, grad_clip=grad_clip, max_norm=max_norm)\n",
    "        valid_loss, valid_acc = eval_loop(model, valid_loader, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
    "        print(f\"Valid loss: {valid_loss:.4f}, Valid acc: {valid_acc:.4f}\")\n",
    "\n",
    "        if early_stopper.early_stop(valid_acc):\n",
    "            print(\"Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\")\n",
    "            no_epochs = epoch+1\n",
    "            break\n",
    "\n",
    "        no_epochs = epoch+1\n",
    "\n",
    "    return train_losses, train_accuracies, valid_losses, valid_accuracies, no_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd8b69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get optimal hyperparameters\n",
    "def find_optimal_hyperparams(param_grid, embedding_matrix):\n",
    "    best_valid_acc = 0\n",
    "    best_hyperparams = {}\n",
    "    results = []\n",
    "\n",
    "    combinations_list = list(product(*param_grid.values()))\n",
    "\n",
    "    for params in combinations_list:\n",
    "        lr, optimizer, batch_size, hidden_dim = params\n",
    "        hyperparams = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with hyperparameters: {hyperparams}\")\n",
    "\n",
    "        model = ClassifierRNN(embedding_matrix, hidden_dim, dropout=0)\n",
    "        criterion = nn.CrossEntropyLoss() # Use CE Loss, since multi-class classification\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        optimizer = optimizer(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "        _, _, _, valid_accuracies, no_epochs = training_step(model, train_loader, valid_loader, optimizer, criterion, num_epochs=200) # Only take val acc since that is what we want to report\n",
    "\n",
    "        max_acc = max(valid_accuracies)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'optimizer': optimizer,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'best_valid_acc': max(valid_accuracies),\n",
    "            'epochs ran': no_epochs\n",
    "        })\n",
    "\n",
    "        if max_acc > best_valid_acc:\n",
    "            best_valid_acc = max_acc\n",
    "            best_hyperparams = {\n",
    "                'lr': lr,\n",
    "                'optimizer': optimizer,\n",
    "                'batch_size': batch_size,\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'epochs ran': no_epochs\n",
    "            }\n",
    "\n",
    "    print(\"Best Hyperparamters: \", best_hyperparams)\n",
    "    print(\"Best validation accuracy: \", best_valid_acc)\n",
    "\n",
    "    return results, best_hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c5141d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.8278, Train acc: 0.0195\n",
      "Valid loss: 1.8197, Valid acc: 0.0248\n",
      "Epoch 2:\n",
      "Train loss: 1.8252, Train acc: 0.0193\n",
      "Valid loss: 1.8172, Valid acc: 0.0257\n",
      "Epoch 3:\n",
      "Train loss: 1.8225, Train acc: 0.0199\n",
      "Valid loss: 1.8148, Valid acc: 0.0266\n",
      "Epoch 4:\n",
      "Train loss: 1.8200, Train acc: 0.0215\n",
      "Valid loss: 1.8125, Valid acc: 0.0284\n",
      "Epoch 5:\n",
      "Train loss: 1.8174, Train acc: 0.0238\n",
      "Valid loss: 1.8102, Valid acc: 0.0284\n",
      "Epoch 6:\n",
      "Train loss: 1.8162, Train acc: 0.0195\n",
      "Valid loss: 1.8080, Valid acc: 0.0294\n",
      "Epoch 7:\n",
      "Train loss: 1.8133, Train acc: 0.0232\n",
      "Valid loss: 1.8058, Valid acc: 0.0284\n",
      "Epoch 8:\n",
      "Train loss: 1.8111, Train acc: 0.0257\n",
      "Valid loss: 1.8037, Valid acc: 0.0394\n",
      "Epoch 9:\n",
      "Train loss: 1.8088, Train acc: 0.0543\n",
      "Valid loss: 1.8016, Valid acc: 0.1917\n",
      "Epoch 10:\n",
      "Train loss: 1.8069, Train acc: 0.2043\n",
      "Valid loss: 1.7996, Valid acc: 0.2220\n",
      "Epoch 11:\n",
      "Train loss: 1.8038, Train acc: 0.2095\n",
      "Valid loss: 1.7976, Valid acc: 0.2284\n",
      "Epoch 12:\n",
      "Train loss: 1.8013, Train acc: 0.2160\n",
      "Valid loss: 1.7957, Valid acc: 0.2284\n",
      "Epoch 13:\n",
      "Train loss: 1.8002, Train acc: 0.2153\n",
      "Valid loss: 1.7938, Valid acc: 0.2294\n",
      "Epoch 14:\n",
      "Train loss: 1.7984, Train acc: 0.2134\n",
      "Valid loss: 1.7920, Valid acc: 0.2284\n",
      "Epoch 15:\n",
      "Train loss: 1.7967, Train acc: 0.2157\n",
      "Valid loss: 1.7902, Valid acc: 0.2303\n",
      "Epoch 16:\n",
      "Train loss: 1.7948, Train acc: 0.2114\n",
      "Valid loss: 1.7884, Valid acc: 0.2294\n",
      "Epoch 17:\n",
      "Train loss: 1.7926, Train acc: 0.2162\n",
      "Valid loss: 1.7866, Valid acc: 0.2294\n",
      "Epoch 18:\n",
      "Train loss: 1.7911, Train acc: 0.2166\n",
      "Valid loss: 1.7850, Valid acc: 0.2294\n",
      "Epoch 19:\n",
      "Train loss: 1.7890, Train acc: 0.2146\n",
      "Valid loss: 1.7833, Valid acc: 0.2294\n",
      "Epoch 20:\n",
      "Train loss: 1.7870, Train acc: 0.2166\n",
      "Valid loss: 1.7816, Valid acc: 0.2294\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7957, Train acc: 0.2270\n",
      "Valid loss: 1.7930, Valid acc: 0.2321\n",
      "Epoch 2:\n",
      "Train loss: 1.7932, Train acc: 0.2276\n",
      "Valid loss: 1.7912, Valid acc: 0.2312\n",
      "Epoch 3:\n",
      "Train loss: 1.7919, Train acc: 0.2313\n",
      "Valid loss: 1.7894, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.7900, Train acc: 0.2263\n",
      "Valid loss: 1.7877, Valid acc: 0.2064\n",
      "Epoch 5:\n",
      "Train loss: 1.7880, Train acc: 0.2242\n",
      "Valid loss: 1.7860, Valid acc: 0.2000\n",
      "Epoch 6:\n",
      "Train loss: 1.7866, Train acc: 0.2281\n",
      "Valid loss: 1.7843, Valid acc: 0.1963\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.8022, Train acc: 0.1534\n",
      "Valid loss: 1.8017, Valid acc: 0.1541\n",
      "Epoch 2:\n",
      "Train loss: 1.8002, Train acc: 0.1529\n",
      "Valid loss: 1.7996, Valid acc: 0.1550\n",
      "Epoch 3:\n",
      "Train loss: 1.7983, Train acc: 0.1554\n",
      "Valid loss: 1.7976, Valid acc: 0.1550\n",
      "Epoch 4:\n",
      "Train loss: 1.7961, Train acc: 0.1557\n",
      "Valid loss: 1.7956, Valid acc: 0.1550\n",
      "Epoch 5:\n",
      "Train loss: 1.7948, Train acc: 0.1547\n",
      "Valid loss: 1.7937, Valid acc: 0.1560\n",
      "Epoch 6:\n",
      "Train loss: 1.7926, Train acc: 0.1547\n",
      "Valid loss: 1.7919, Valid acc: 0.1560\n",
      "Epoch 7:\n",
      "Train loss: 1.7910, Train acc: 0.1538\n",
      "Valid loss: 1.7900, Valid acc: 0.1560\n",
      "Epoch 8:\n",
      "Train loss: 1.7893, Train acc: 0.1550\n",
      "Valid loss: 1.7882, Valid acc: 0.1550\n",
      "Epoch 9:\n",
      "Train loss: 1.7877, Train acc: 0.1550\n",
      "Valid loss: 1.7865, Valid acc: 0.1560\n",
      "Epoch 10:\n",
      "Train loss: 1.7858, Train acc: 0.1559\n",
      "Valid loss: 1.7848, Valid acc: 0.1560\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.8018, Train acc: 0.2194\n",
      "Valid loss: 1.7875, Valid acc: 0.2312\n",
      "Epoch 2:\n",
      "Train loss: 1.8010, Train acc: 0.2208\n",
      "Valid loss: 1.7866, Valid acc: 0.2321\n",
      "Epoch 3:\n",
      "Train loss: 1.7995, Train acc: 0.2242\n",
      "Valid loss: 1.7857, Valid acc: 0.2330\n",
      "Epoch 4:\n",
      "Train loss: 1.7988, Train acc: 0.2221\n",
      "Valid loss: 1.7848, Valid acc: 0.2339\n",
      "Epoch 5:\n",
      "Train loss: 1.7967, Train acc: 0.2212\n",
      "Valid loss: 1.7839, Valid acc: 0.2339\n",
      "Epoch 6:\n",
      "Train loss: 1.7977, Train acc: 0.2226\n",
      "Valid loss: 1.7830, Valid acc: 0.2339\n",
      "Epoch 7:\n",
      "Train loss: 1.7975, Train acc: 0.2226\n",
      "Valid loss: 1.7821, Valid acc: 0.2339\n",
      "Epoch 8:\n",
      "Train loss: 1.7949, Train acc: 0.2235\n",
      "Valid loss: 1.7812, Valid acc: 0.2339\n",
      "Epoch 9:\n",
      "Train loss: 1.7935, Train acc: 0.2212\n",
      "Valid loss: 1.7804, Valid acc: 0.2339\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7859, Train acc: 0.1756\n",
      "Valid loss: 1.7928, Valid acc: 0.1523\n",
      "Epoch 2:\n",
      "Train loss: 1.7850, Train acc: 0.1770\n",
      "Valid loss: 1.7919, Valid acc: 0.1523\n",
      "Epoch 3:\n",
      "Train loss: 1.7849, Train acc: 0.1763\n",
      "Valid loss: 1.7911, Valid acc: 0.1523\n",
      "Epoch 4:\n",
      "Train loss: 1.7825, Train acc: 0.1804\n",
      "Valid loss: 1.7902, Valid acc: 0.1523\n",
      "Epoch 5:\n",
      "Train loss: 1.7828, Train acc: 0.1768\n",
      "Valid loss: 1.7894, Valid acc: 0.1523\n",
      "Epoch 6:\n",
      "Train loss: 1.7814, Train acc: 0.1784\n",
      "Valid loss: 1.7885, Valid acc: 0.1541\n",
      "Epoch 7:\n",
      "Train loss: 1.7808, Train acc: 0.1793\n",
      "Valid loss: 1.7877, Valid acc: 0.1541\n",
      "Epoch 8:\n",
      "Train loss: 1.7798, Train acc: 0.1820\n",
      "Valid loss: 1.7869, Valid acc: 0.1532\n",
      "Epoch 9:\n",
      "Train loss: 1.7798, Train acc: 0.1790\n",
      "Valid loss: 1.7861, Valid acc: 0.1541\n",
      "Epoch 10:\n",
      "Train loss: 1.7788, Train acc: 0.1788\n",
      "Valid loss: 1.7852, Valid acc: 0.1532\n",
      "Epoch 11:\n",
      "Train loss: 1.7776, Train acc: 0.1825\n",
      "Valid loss: 1.7844, Valid acc: 0.1550\n",
      "Epoch 12:\n",
      "Train loss: 1.7773, Train acc: 0.1790\n",
      "Valid loss: 1.7836, Valid acc: 0.1550\n",
      "Epoch 13:\n",
      "Train loss: 1.7757, Train acc: 0.1834\n",
      "Valid loss: 1.7829, Valid acc: 0.1578\n",
      "Epoch 14:\n",
      "Train loss: 1.7756, Train acc: 0.1811\n",
      "Valid loss: 1.7821, Valid acc: 0.1578\n",
      "Epoch 15:\n",
      "Train loss: 1.7743, Train acc: 0.1800\n",
      "Valid loss: 1.7813, Valid acc: 0.1615\n",
      "Epoch 16:\n",
      "Train loss: 1.7746, Train acc: 0.1834\n",
      "Valid loss: 1.7805, Valid acc: 0.1615\n",
      "Epoch 17:\n",
      "Train loss: 1.7724, Train acc: 0.1880\n",
      "Valid loss: 1.7798, Valid acc: 0.1633\n",
      "Epoch 18:\n",
      "Train loss: 1.7721, Train acc: 0.1933\n",
      "Valid loss: 1.7790, Valid acc: 0.1615\n",
      "Epoch 19:\n",
      "Train loss: 1.7729, Train acc: 0.1809\n",
      "Valid loss: 1.7782, Valid acc: 0.1624\n",
      "Epoch 20:\n",
      "Train loss: 1.7710, Train acc: 0.1999\n",
      "Valid loss: 1.7775, Valid acc: 0.1771\n",
      "Epoch 21:\n",
      "Train loss: 1.7707, Train acc: 0.2196\n",
      "Valid loss: 1.7767, Valid acc: 0.2046\n",
      "Epoch 22:\n",
      "Train loss: 1.7688, Train acc: 0.2219\n",
      "Valid loss: 1.7760, Valid acc: 0.2064\n",
      "Epoch 23:\n",
      "Train loss: 1.7690, Train acc: 0.2270\n",
      "Valid loss: 1.7752, Valid acc: 0.2064\n",
      "Epoch 24:\n",
      "Train loss: 1.7683, Train acc: 0.2205\n",
      "Valid loss: 1.7745, Valid acc: 0.2064\n",
      "Epoch 25:\n",
      "Train loss: 1.7677, Train acc: 0.2249\n",
      "Valid loss: 1.7738, Valid acc: 0.2064\n",
      "Epoch 26:\n",
      "Train loss: 1.7669, Train acc: 0.2201\n",
      "Valid loss: 1.7730, Valid acc: 0.2064\n",
      "Epoch 27:\n",
      "Train loss: 1.7660, Train acc: 0.2240\n",
      "Valid loss: 1.7723, Valid acc: 0.2064\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7857, Train acc: 0.2217\n",
      "Valid loss: 1.7848, Valid acc: 0.2404\n",
      "Epoch 2:\n",
      "Train loss: 1.7854, Train acc: 0.2199\n",
      "Valid loss: 1.7840, Valid acc: 0.2413\n",
      "Epoch 3:\n",
      "Train loss: 1.7841, Train acc: 0.2231\n",
      "Valid loss: 1.7832, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.7829, Train acc: 0.2240\n",
      "Valid loss: 1.7824, Valid acc: 0.2422\n",
      "Epoch 5:\n",
      "Train loss: 1.7828, Train acc: 0.2210\n",
      "Valid loss: 1.7816, Valid acc: 0.2413\n",
      "Epoch 6:\n",
      "Train loss: 1.7815, Train acc: 0.2221\n",
      "Valid loss: 1.7808, Valid acc: 0.2413\n",
      "Epoch 7:\n",
      "Train loss: 1.7810, Train acc: 0.2256\n",
      "Valid loss: 1.7800, Valid acc: 0.2413\n",
      "Epoch 8:\n",
      "Train loss: 1.7803, Train acc: 0.2219\n",
      "Valid loss: 1.7793, Valid acc: 0.2422\n",
      "Epoch 9:\n",
      "Train loss: 1.7790, Train acc: 0.2244\n",
      "Valid loss: 1.7785, Valid acc: 0.2422\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.8117, Train acc: 0.0174\n",
      "Valid loss: 1.8107, Valid acc: 0.0202\n",
      "Epoch 2:\n",
      "Train loss: 1.8115, Train acc: 0.0181\n",
      "Valid loss: 1.8101, Valid acc: 0.0202\n",
      "Epoch 3:\n",
      "Train loss: 1.8098, Train acc: 0.0190\n",
      "Valid loss: 1.8094, Valid acc: 0.0202\n",
      "Epoch 4:\n",
      "Train loss: 1.8105, Train acc: 0.0179\n",
      "Valid loss: 1.8087, Valid acc: 0.0202\n",
      "Epoch 5:\n",
      "Train loss: 1.8092, Train acc: 0.0172\n",
      "Valid loss: 1.8081, Valid acc: 0.0193\n",
      "Epoch 6:\n",
      "Train loss: 1.8091, Train acc: 0.0177\n",
      "Valid loss: 1.8075, Valid acc: 0.0193\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7927, Train acc: 0.1696\n",
      "Valid loss: 1.7942, Valid acc: 0.1440\n",
      "Epoch 2:\n",
      "Train loss: 1.7926, Train acc: 0.1699\n",
      "Valid loss: 1.7938, Valid acc: 0.1440\n",
      "Epoch 3:\n",
      "Train loss: 1.7924, Train acc: 0.1708\n",
      "Valid loss: 1.7934, Valid acc: 0.1440\n",
      "Epoch 4:\n",
      "Train loss: 1.7929, Train acc: 0.1696\n",
      "Valid loss: 1.7931, Valid acc: 0.1440\n",
      "Epoch 5:\n",
      "Train loss: 1.7914, Train acc: 0.1703\n",
      "Valid loss: 1.7927, Valid acc: 0.1440\n",
      "Epoch 6:\n",
      "Train loss: 1.7907, Train acc: 0.1699\n",
      "Valid loss: 1.7923, Valid acc: 0.1440\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7807, Train acc: 0.2256\n",
      "Valid loss: 1.7808, Valid acc: 0.2440\n",
      "Epoch 2:\n",
      "Train loss: 1.7799, Train acc: 0.2256\n",
      "Valid loss: 1.7805, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.7802, Train acc: 0.2258\n",
      "Valid loss: 1.7801, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.7821, Train acc: 0.2260\n",
      "Valid loss: 1.7798, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.7804, Train acc: 0.2258\n",
      "Valid loss: 1.7794, Valid acc: 0.2440\n",
      "Epoch 6:\n",
      "Train loss: 1.7801, Train acc: 0.2256\n",
      "Valid loss: 1.7791, Valid acc: 0.2440\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7856, Train acc: 0.2157\n",
      "Valid loss: 1.7728, Valid acc: 0.2413\n",
      "Epoch 2:\n",
      "Train loss: 1.7741, Train acc: 0.2169\n",
      "Valid loss: 1.7642, Valid acc: 0.2376\n",
      "Epoch 3:\n",
      "Train loss: 1.7663, Train acc: 0.2215\n",
      "Valid loss: 1.7573, Valid acc: 0.2394\n",
      "Epoch 4:\n",
      "Train loss: 1.7601, Train acc: 0.2208\n",
      "Valid loss: 1.7514, Valid acc: 0.2450\n",
      "Epoch 5:\n",
      "Train loss: 1.7544, Train acc: 0.2228\n",
      "Valid loss: 1.7460, Valid acc: 0.2486\n",
      "Epoch 6:\n",
      "Train loss: 1.7490, Train acc: 0.2309\n",
      "Valid loss: 1.7409, Valid acc: 0.2532\n",
      "Epoch 7:\n",
      "Train loss: 1.7448, Train acc: 0.2377\n",
      "Valid loss: 1.7360, Valid acc: 0.2394\n",
      "Epoch 8:\n",
      "Train loss: 1.7395, Train acc: 0.2293\n",
      "Valid loss: 1.7313, Valid acc: 0.2413\n",
      "Epoch 9:\n",
      "Train loss: 1.7344, Train acc: 0.2313\n",
      "Valid loss: 1.7268, Valid acc: 0.2431\n",
      "Epoch 10:\n",
      "Train loss: 1.7312, Train acc: 0.2325\n",
      "Valid loss: 1.7225, Valid acc: 0.2422\n",
      "Epoch 11:\n",
      "Train loss: 1.7264, Train acc: 0.2320\n",
      "Valid loss: 1.7183, Valid acc: 0.2404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7992, Train acc: 0.1564\n",
      "Valid loss: 1.7880, Valid acc: 0.1615\n",
      "Epoch 2:\n",
      "Train loss: 1.7826, Train acc: 0.1646\n",
      "Valid loss: 1.7756, Valid acc: 0.1679\n",
      "Epoch 3:\n",
      "Train loss: 1.7712, Train acc: 0.1719\n",
      "Valid loss: 1.7655, Valid acc: 0.1743\n",
      "Epoch 4:\n",
      "Train loss: 1.7619, Train acc: 0.2210\n",
      "Valid loss: 1.7555, Valid acc: 0.2064\n",
      "Epoch 5:\n",
      "Train loss: 1.7521, Train acc: 0.2279\n",
      "Valid loss: 1.7454, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.7404, Train acc: 0.2295\n",
      "Valid loss: 1.7344, Valid acc: 0.2055\n",
      "Epoch 7:\n",
      "Train loss: 1.7286, Train acc: 0.2254\n",
      "Valid loss: 1.7219, Valid acc: 0.2055\n",
      "Epoch 8:\n",
      "Train loss: 1.7168, Train acc: 0.2299\n",
      "Valid loss: 1.7088, Valid acc: 0.2055\n",
      "Epoch 9:\n",
      "Train loss: 1.7028, Train acc: 0.2212\n",
      "Valid loss: 1.6958, Valid acc: 0.2055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7633, Train acc: 0.2137\n",
      "Valid loss: 1.7360, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.7023, Train acc: 0.2297\n",
      "Valid loss: 1.6734, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6665, Train acc: 0.2254\n",
      "Valid loss: 1.6591, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6570, Train acc: 0.2240\n",
      "Valid loss: 1.6544, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6532, Train acc: 0.2274\n",
      "Valid loss: 1.6520, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.6510, Train acc: 0.2302\n",
      "Valid loss: 1.6510, Valid acc: 0.2422\n",
      "Epoch 7:\n",
      "Train loss: 1.6499, Train acc: 0.2258\n",
      "Valid loss: 1.6508, Valid acc: 0.2110\n",
      "Epoch 8:\n",
      "Train loss: 1.6482, Train acc: 0.2240\n",
      "Valid loss: 1.6504, Valid acc: 0.2101\n",
      "Epoch 9:\n",
      "Train loss: 1.6469, Train acc: 0.2304\n",
      "Valid loss: 1.6501, Valid acc: 0.2092\n",
      "Epoch 10:\n",
      "Train loss: 1.6466, Train acc: 0.2304\n",
      "Valid loss: 1.6497, Valid acc: 0.2477\n",
      "Epoch 11:\n",
      "Train loss: 1.6487, Train acc: 0.2242\n",
      "Valid loss: 1.6496, Valid acc: 0.2486\n",
      "Epoch 12:\n",
      "Train loss: 1.6466, Train acc: 0.2334\n",
      "Valid loss: 1.6496, Valid acc: 0.2385\n",
      "Epoch 13:\n",
      "Train loss: 1.6476, Train acc: 0.2166\n",
      "Valid loss: 1.6496, Valid acc: 0.2477\n",
      "Epoch 14:\n",
      "Train loss: 1.6475, Train acc: 0.2327\n",
      "Valid loss: 1.6496, Valid acc: 0.2101\n",
      "Epoch 15:\n",
      "Train loss: 1.6463, Train acc: 0.2276\n",
      "Valid loss: 1.6495, Valid acc: 0.2119\n",
      "Epoch 16:\n",
      "Train loss: 1.6470, Train acc: 0.2270\n",
      "Valid loss: 1.6494, Valid acc: 0.2459\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.8070, Train acc: 0.0532\n",
      "Valid loss: 1.8162, Valid acc: 0.1908\n",
      "Epoch 2:\n",
      "Train loss: 1.7986, Train acc: 0.2196\n",
      "Valid loss: 1.8092, Valid acc: 0.1991\n",
      "Epoch 3:\n",
      "Train loss: 1.7921, Train acc: 0.2283\n",
      "Valid loss: 1.8041, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.7880, Train acc: 0.2283\n",
      "Valid loss: 1.7999, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.7853, Train acc: 0.2272\n",
      "Valid loss: 1.7962, Valid acc: 0.2037\n",
      "Epoch 6:\n",
      "Train loss: 1.7826, Train acc: 0.2281\n",
      "Valid loss: 1.7926, Valid acc: 0.2055\n",
      "Epoch 7:\n",
      "Train loss: 1.7785, Train acc: 0.2281\n",
      "Valid loss: 1.7896, Valid acc: 0.2055\n",
      "Epoch 8:\n",
      "Train loss: 1.7771, Train acc: 0.2276\n",
      "Valid loss: 1.7867, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.7739, Train acc: 0.2293\n",
      "Valid loss: 1.7840, Valid acc: 0.2028\n",
      "Epoch 10:\n",
      "Train loss: 1.7712, Train acc: 0.2302\n",
      "Valid loss: 1.7813, Valid acc: 0.2055\n",
      "Epoch 11:\n",
      "Train loss: 1.7703, Train acc: 0.2270\n",
      "Valid loss: 1.7789, Valid acc: 0.2055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7963, Train acc: 0.1850\n",
      "Valid loss: 1.7814, Valid acc: 0.2385\n",
      "Epoch 2:\n",
      "Train loss: 1.7796, Train acc: 0.2107\n",
      "Valid loss: 1.7688, Valid acc: 0.2385\n",
      "Epoch 3:\n",
      "Train loss: 1.7680, Train acc: 0.2116\n",
      "Valid loss: 1.7568, Valid acc: 0.2394\n",
      "Epoch 4:\n",
      "Train loss: 1.7560, Train acc: 0.2130\n",
      "Valid loss: 1.7448, Valid acc: 0.2413\n",
      "Epoch 5:\n",
      "Train loss: 1.7435, Train acc: 0.2144\n",
      "Valid loss: 1.7319, Valid acc: 0.2422\n",
      "Epoch 6:\n",
      "Train loss: 1.7302, Train acc: 0.2164\n",
      "Valid loss: 1.7182, Valid acc: 0.2413\n",
      "Epoch 7:\n",
      "Train loss: 1.7169, Train acc: 0.2148\n",
      "Valid loss: 1.7047, Valid acc: 0.2422\n",
      "Epoch 8:\n",
      "Train loss: 1.7041, Train acc: 0.2134\n",
      "Valid loss: 1.6926, Valid acc: 0.2422\n",
      "Epoch 9:\n",
      "Train loss: 1.6925, Train acc: 0.2164\n",
      "Valid loss: 1.6827, Valid acc: 0.2413\n",
      "Epoch 10:\n",
      "Train loss: 1.6841, Train acc: 0.2139\n",
      "Valid loss: 1.6750, Valid acc: 0.2413\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7663, Train acc: 0.2258\n",
      "Valid loss: 1.7440, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.7407, Train acc: 0.2290\n",
      "Valid loss: 1.7175, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.7147, Train acc: 0.2288\n",
      "Valid loss: 1.6855, Valid acc: 0.2459\n",
      "Epoch 4:\n",
      "Train loss: 1.6866, Train acc: 0.2299\n",
      "Valid loss: 1.6631, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6704, Train acc: 0.2286\n",
      "Valid loss: 1.6539, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6632, Train acc: 0.2290\n",
      "Valid loss: 1.6494, Valid acc: 0.2477\n",
      "Epoch 7:\n",
      "Train loss: 1.6589, Train acc: 0.2302\n",
      "Valid loss: 1.6470, Valid acc: 0.2486\n",
      "Epoch 8:\n",
      "Train loss: 1.6545, Train acc: 0.2304\n",
      "Valid loss: 1.6465, Valid acc: 0.2477\n",
      "Epoch 9:\n",
      "Train loss: 1.6514, Train acc: 0.2325\n",
      "Valid loss: 1.6460, Valid acc: 0.2477\n",
      "Epoch 10:\n",
      "Train loss: 1.6519, Train acc: 0.2315\n",
      "Valid loss: 1.6456, Valid acc: 0.2477\n",
      "Epoch 11:\n",
      "Train loss: 1.6494, Train acc: 0.2325\n",
      "Valid loss: 1.6451, Valid acc: 0.2477\n",
      "Epoch 12:\n",
      "Train loss: 1.6497, Train acc: 0.2290\n",
      "Valid loss: 1.6450, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7846, Train acc: 0.2144\n",
      "Valid loss: 1.7831, Valid acc: 0.2009\n",
      "Epoch 2:\n",
      "Train loss: 1.7780, Train acc: 0.2205\n",
      "Valid loss: 1.7784, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.7741, Train acc: 0.2247\n",
      "Valid loss: 1.7749, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.7687, Train acc: 0.2260\n",
      "Valid loss: 1.7719, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.7678, Train acc: 0.2276\n",
      "Valid loss: 1.7693, Valid acc: 0.2037\n",
      "Epoch 6:\n",
      "Train loss: 1.7661, Train acc: 0.2272\n",
      "Valid loss: 1.7669, Valid acc: 0.2028\n",
      "Epoch 7:\n",
      "Train loss: 1.7622, Train acc: 0.2281\n",
      "Valid loss: 1.7647, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7770, Train acc: 0.2242\n",
      "Valid loss: 1.7700, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.7653, Train acc: 0.2254\n",
      "Valid loss: 1.7620, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.7584, Train acc: 0.2260\n",
      "Valid loss: 1.7553, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.7521, Train acc: 0.2263\n",
      "Valid loss: 1.7490, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.7455, Train acc: 0.2258\n",
      "Valid loss: 1.7426, Valid acc: 0.2440\n",
      "Epoch 6:\n",
      "Train loss: 1.7404, Train acc: 0.2270\n",
      "Valid loss: 1.7364, Valid acc: 0.2440\n",
      "Epoch 7:\n",
      "Train loss: 1.7322, Train acc: 0.2267\n",
      "Valid loss: 1.7298, Valid acc: 0.2450\n",
      "Epoch 8:\n",
      "Train loss: 1.7255, Train acc: 0.2274\n",
      "Valid loss: 1.7229, Valid acc: 0.2440\n",
      "Epoch 9:\n",
      "Train loss: 1.7187, Train acc: 0.2290\n",
      "Valid loss: 1.7156, Valid acc: 0.2450\n",
      "Epoch 10:\n",
      "Train loss: 1.7102, Train acc: 0.2276\n",
      "Valid loss: 1.7085, Valid acc: 0.2450\n",
      "Epoch 11:\n",
      "Train loss: 1.7017, Train acc: 0.2272\n",
      "Valid loss: 1.7015, Valid acc: 0.2450\n",
      "Epoch 12:\n",
      "Train loss: 1.6970, Train acc: 0.2265\n",
      "Valid loss: 1.6953, Valid acc: 0.2459\n",
      "Epoch 13:\n",
      "Train loss: 1.6893, Train acc: 0.2267\n",
      "Valid loss: 1.6895, Valid acc: 0.2459\n",
      "Epoch 14:\n",
      "Train loss: 1.6848, Train acc: 0.2295\n",
      "Valid loss: 1.6848, Valid acc: 0.2468\n",
      "Epoch 15:\n",
      "Train loss: 1.6789, Train acc: 0.2295\n",
      "Valid loss: 1.6805, Valid acc: 0.2468\n",
      "Epoch 16:\n",
      "Train loss: 1.6733, Train acc: 0.2279\n",
      "Valid loss: 1.6769, Valid acc: 0.2468\n",
      "Epoch 17:\n",
      "Train loss: 1.6704, Train acc: 0.2283\n",
      "Valid loss: 1.6740, Valid acc: 0.2450\n",
      "Epoch 18:\n",
      "Train loss: 1.6689, Train acc: 0.2295\n",
      "Valid loss: 1.6713, Valid acc: 0.2468\n",
      "Epoch 19:\n",
      "Train loss: 1.6691, Train acc: 0.2249\n",
      "Valid loss: 1.6693, Valid acc: 0.2018\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7764, Train acc: 0.2056\n",
      "Valid loss: 1.7603, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.7482, Train acc: 0.2276\n",
      "Valid loss: 1.7334, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.7184, Train acc: 0.2286\n",
      "Valid loss: 1.7001, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6856, Train acc: 0.2293\n",
      "Valid loss: 1.6754, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6737, Train acc: 0.2293\n",
      "Valid loss: 1.6649, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6620, Train acc: 0.2295\n",
      "Valid loss: 1.6590, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7450, Train acc: 0.1891\n",
      "Valid loss: 1.6638, Valid acc: 0.2495\n",
      "Epoch 2:\n",
      "Train loss: 1.6545, Train acc: 0.2318\n",
      "Valid loss: 1.6485, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6500, Train acc: 0.2281\n",
      "Valid loss: 1.6478, Valid acc: 0.2450\n",
      "Epoch 4:\n",
      "Train loss: 1.6480, Train acc: 0.2217\n",
      "Valid loss: 1.6455, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6459, Train acc: 0.2331\n",
      "Valid loss: 1.6433, Valid acc: 0.2450\n",
      "Epoch 6:\n",
      "Train loss: 1.6463, Train acc: 0.2370\n",
      "Valid loss: 1.6493, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7128, Train acc: 0.2187\n",
      "Valid loss: 1.6489, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6485, Train acc: 0.2279\n",
      "Valid loss: 1.6449, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6473, Train acc: 0.2329\n",
      "Valid loss: 1.6409, Valid acc: 0.2495\n",
      "Epoch 4:\n",
      "Train loss: 1.6439, Train acc: 0.2327\n",
      "Valid loss: 1.6460, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6459, Train acc: 0.2295\n",
      "Valid loss: 1.6407, Valid acc: 0.2651\n",
      "Epoch 6:\n",
      "Train loss: 1.6429, Train acc: 0.2315\n",
      "Valid loss: 1.6403, Valid acc: 0.2404\n",
      "Epoch 7:\n",
      "Train loss: 1.6430, Train acc: 0.2391\n",
      "Valid loss: 1.6414, Valid acc: 0.2606\n",
      "Epoch 8:\n",
      "Train loss: 1.6357, Train acc: 0.2481\n",
      "Valid loss: 1.5971, Valid acc: 0.3183\n",
      "Epoch 9:\n",
      "Train loss: 1.3932, Train acc: 0.3982\n",
      "Valid loss: 1.2917, Valid acc: 0.4578\n",
      "Epoch 10:\n",
      "Train loss: 1.1737, Train acc: 0.4810\n",
      "Valid loss: 1.1393, Valid acc: 0.5128\n",
      "Epoch 11:\n",
      "Train loss: 1.0115, Train acc: 0.5773\n",
      "Valid loss: 1.0536, Valid acc: 0.5780\n",
      "Epoch 12:\n",
      "Train loss: 0.8915, Train acc: 0.6488\n",
      "Valid loss: 0.9795, Valid acc: 0.5982\n",
      "Epoch 13:\n",
      "Train loss: 0.7830, Train acc: 0.7036\n",
      "Valid loss: 0.9054, Valid acc: 0.6404\n",
      "Epoch 14:\n",
      "Train loss: 0.7094, Train acc: 0.7265\n",
      "Valid loss: 0.8847, Valid acc: 0.6495\n",
      "Epoch 15:\n",
      "Train loss: 0.6576, Train acc: 0.7409\n",
      "Valid loss: 0.8864, Valid acc: 0.6550\n",
      "Epoch 16:\n",
      "Train loss: 0.6357, Train acc: 0.7437\n",
      "Valid loss: 0.8655, Valid acc: 0.6606\n",
      "Epoch 17:\n",
      "Train loss: 0.5705, Train acc: 0.7728\n",
      "Valid loss: 0.9040, Valid acc: 0.6550\n",
      "Epoch 18:\n",
      "Train loss: 0.5359, Train acc: 0.7779\n",
      "Valid loss: 0.8930, Valid acc: 0.6688\n",
      "Epoch 19:\n",
      "Train loss: 0.5353, Train acc: 0.7790\n",
      "Valid loss: 0.8716, Valid acc: 0.6477\n",
      "Epoch 20:\n",
      "Train loss: 0.4817, Train acc: 0.7996\n",
      "Valid loss: 0.9053, Valid acc: 0.6688\n",
      "Epoch 21:\n",
      "Train loss: 0.4746, Train acc: 0.7875\n",
      "Valid loss: 0.9005, Valid acc: 0.6661\n",
      "Epoch 22:\n",
      "Train loss: 0.4470, Train acc: 0.8083\n",
      "Valid loss: 0.8992, Valid acc: 0.6688\n",
      "Epoch 23:\n",
      "Train loss: 0.4520, Train acc: 0.8024\n",
      "Valid loss: 0.9297, Valid acc: 0.6688\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6769, Train acc: 0.2290\n",
      "Valid loss: 1.6460, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6490, Train acc: 0.2274\n",
      "Valid loss: 1.6404, Valid acc: 0.2541\n",
      "Epoch 3:\n",
      "Train loss: 1.6431, Train acc: 0.2359\n",
      "Valid loss: 1.6534, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6408, Train acc: 0.2439\n",
      "Valid loss: 1.6593, Valid acc: 0.2450\n",
      "Epoch 5:\n",
      "Train loss: 1.5382, Train acc: 0.3668\n",
      "Valid loss: 1.5132, Valid acc: 0.3789\n",
      "Epoch 6:\n",
      "Train loss: 1.4532, Train acc: 0.3971\n",
      "Valid loss: 1.4386, Valid acc: 0.4009\n",
      "Epoch 7:\n",
      "Train loss: 1.3922, Train acc: 0.4106\n",
      "Valid loss: 1.3662, Valid acc: 0.4275\n",
      "Epoch 8:\n",
      "Train loss: 1.2523, Train acc: 0.4587\n",
      "Valid loss: 1.3707, Valid acc: 0.4239\n",
      "Epoch 9:\n",
      "Train loss: 1.1474, Train acc: 0.5016\n",
      "Valid loss: 1.1342, Valid acc: 0.5284\n",
      "Epoch 10:\n",
      "Train loss: 1.0086, Train acc: 0.5768\n",
      "Valid loss: 1.0148, Valid acc: 0.5881\n",
      "Epoch 11:\n",
      "Train loss: 0.9096, Train acc: 0.6419\n",
      "Valid loss: 1.0857, Valid acc: 0.5734\n",
      "Epoch 12:\n",
      "Train loss: 0.8062, Train acc: 0.7011\n",
      "Valid loss: 1.0018, Valid acc: 0.5908\n",
      "Epoch 13:\n",
      "Train loss: 0.7822, Train acc: 0.6921\n",
      "Valid loss: 1.1221, Valid acc: 0.6018\n",
      "Epoch 14:\n",
      "Train loss: 0.7889, Train acc: 0.6958\n",
      "Valid loss: 1.0282, Valid acc: 0.6119\n",
      "Epoch 15:\n",
      "Train loss: 0.7465, Train acc: 0.7121\n",
      "Valid loss: 0.9837, Valid acc: 0.6413\n",
      "Epoch 16:\n",
      "Train loss: 0.6766, Train acc: 0.7382\n",
      "Valid loss: 0.8832, Valid acc: 0.6670\n",
      "Epoch 17:\n",
      "Train loss: 0.6490, Train acc: 0.7391\n",
      "Valid loss: 0.9385, Valid acc: 0.6505\n",
      "Epoch 18:\n",
      "Train loss: 0.6951, Train acc: 0.7270\n",
      "Valid loss: 0.9546, Valid acc: 0.6330\n",
      "Epoch 19:\n",
      "Train loss: 0.6390, Train acc: 0.7442\n",
      "Valid loss: 0.8749, Valid acc: 0.6560\n",
      "Epoch 20:\n",
      "Train loss: 0.5723, Train acc: 0.7659\n",
      "Valid loss: 1.0123, Valid acc: 0.6165\n",
      "Epoch 21:\n",
      "Train loss: 0.5650, Train acc: 0.7664\n",
      "Valid loss: 0.9464, Valid acc: 0.6679\n",
      "Epoch 22:\n",
      "Train loss: 0.5768, Train acc: 0.7643\n",
      "Valid loss: 0.8992, Valid acc: 0.6633\n",
      "Epoch 23:\n",
      "Train loss: 0.5785, Train acc: 0.7648\n",
      "Valid loss: 1.1622, Valid acc: 0.6046\n",
      "Epoch 24:\n",
      "Train loss: 0.5600, Train acc: 0.7691\n",
      "Valid loss: 0.9404, Valid acc: 0.6486\n",
      "Epoch 25:\n",
      "Train loss: 0.5146, Train acc: 0.7886\n",
      "Valid loss: 0.8734, Valid acc: 0.6862\n",
      "Epoch 26:\n",
      "Train loss: 0.5474, Train acc: 0.7762\n",
      "Valid loss: 0.9909, Valid acc: 0.6505\n",
      "Epoch 27:\n",
      "Train loss: 0.6043, Train acc: 0.7481\n",
      "Valid loss: 0.8782, Valid acc: 0.6743\n",
      "Epoch 28:\n",
      "Train loss: 0.5102, Train acc: 0.7785\n",
      "Valid loss: 0.8836, Valid acc: 0.6725\n",
      "Epoch 29:\n",
      "Train loss: 0.5640, Train acc: 0.7744\n",
      "Valid loss: 1.5435, Valid acc: 0.5651\n",
      "Epoch 30:\n",
      "Train loss: 0.5879, Train acc: 0.7703\n",
      "Valid loss: 0.8914, Valid acc: 0.6734\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7543, Train acc: 0.2194\n",
      "Valid loss: 1.7208, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6922, Train acc: 0.2293\n",
      "Valid loss: 1.6628, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6581, Train acc: 0.2313\n",
      "Valid loss: 1.6492, Valid acc: 0.2431\n",
      "Epoch 4:\n",
      "Train loss: 1.6513, Train acc: 0.2318\n",
      "Valid loss: 1.6465, Valid acc: 0.2064\n",
      "Epoch 5:\n",
      "Train loss: 1.6484, Train acc: 0.2270\n",
      "Valid loss: 1.6480, Valid acc: 0.2064\n",
      "Epoch 6:\n",
      "Train loss: 1.6522, Train acc: 0.2205\n",
      "Valid loss: 1.6484, Valid acc: 0.2394\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7532, Train acc: 0.2091\n",
      "Valid loss: 1.6810, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6552, Train acc: 0.2325\n",
      "Valid loss: 1.6423, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6485, Train acc: 0.2302\n",
      "Valid loss: 1.6383, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.6479, Train acc: 0.2265\n",
      "Valid loss: 1.6460, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6475, Train acc: 0.2352\n",
      "Valid loss: 1.6564, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6472, Train acc: 0.2331\n",
      "Valid loss: 1.6408, Valid acc: 0.2440\n",
      "Epoch 7:\n",
      "Train loss: 1.6524, Train acc: 0.2263\n",
      "Valid loss: 1.6379, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7058, Train acc: 0.2093\n",
      "Valid loss: 1.6521, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6505, Train acc: 0.2235\n",
      "Valid loss: 1.6315, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6483, Train acc: 0.2272\n",
      "Valid loss: 1.6432, Valid acc: 0.2459\n",
      "Epoch 4:\n",
      "Train loss: 1.6461, Train acc: 0.2274\n",
      "Valid loss: 1.6470, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6440, Train acc: 0.2311\n",
      "Valid loss: 1.6509, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.6389, Train acc: 0.2421\n",
      "Valid loss: 1.6492, Valid acc: 0.2119\n",
      "Epoch 7:\n",
      "Train loss: 1.6238, Train acc: 0.2630\n",
      "Valid loss: 1.5214, Valid acc: 0.3688\n",
      "Epoch 8:\n",
      "Train loss: 1.4072, Train acc: 0.4147\n",
      "Valid loss: 1.3628, Valid acc: 0.4532\n",
      "Epoch 9:\n",
      "Train loss: 1.1619, Train acc: 0.5218\n",
      "Valid loss: 1.1167, Valid acc: 0.5349\n",
      "Epoch 10:\n",
      "Train loss: 0.9728, Train acc: 0.6353\n",
      "Valid loss: 1.0097, Valid acc: 0.6257\n",
      "Epoch 11:\n",
      "Train loss: 0.8753, Train acc: 0.6664\n",
      "Valid loss: 1.0014, Valid acc: 0.5908\n",
      "Epoch 12:\n",
      "Train loss: 0.7963, Train acc: 0.7033\n",
      "Valid loss: 0.9687, Valid acc: 0.6468\n",
      "Epoch 13:\n",
      "Train loss: 0.7212, Train acc: 0.7419\n",
      "Valid loss: 0.9192, Valid acc: 0.7009\n",
      "Epoch 14:\n",
      "Train loss: 0.6369, Train acc: 0.7856\n",
      "Valid loss: 0.8767, Valid acc: 0.6752\n",
      "Epoch 15:\n",
      "Train loss: 0.6153, Train acc: 0.7923\n",
      "Valid loss: 0.9115, Valid acc: 0.6404\n",
      "Epoch 16:\n",
      "Train loss: 0.5416, Train acc: 0.8485\n",
      "Valid loss: 0.8163, Valid acc: 0.7376\n",
      "Epoch 17:\n",
      "Train loss: 0.5443, Train acc: 0.8418\n",
      "Valid loss: 0.8228, Valid acc: 0.7560\n",
      "Epoch 18:\n",
      "Train loss: 0.4386, Train acc: 0.8778\n",
      "Valid loss: 0.8170, Valid acc: 0.7624\n",
      "Epoch 19:\n",
      "Train loss: 0.4229, Train acc: 0.8890\n",
      "Valid loss: 0.8287, Valid acc: 0.7505\n",
      "Epoch 20:\n",
      "Train loss: 0.3832, Train acc: 0.8984\n",
      "Valid loss: 0.8361, Valid acc: 0.7413\n",
      "Epoch 21:\n",
      "Train loss: 0.3515, Train acc: 0.9133\n",
      "Valid loss: 0.7881, Valid acc: 0.7780\n",
      "Epoch 22:\n",
      "Train loss: 0.3311, Train acc: 0.9225\n",
      "Valid loss: 0.7753, Valid acc: 0.7936\n",
      "Epoch 23:\n",
      "Train loss: 0.3261, Train acc: 0.9221\n",
      "Valid loss: 1.0153, Valid acc: 0.7385\n",
      "Epoch 24:\n",
      "Train loss: 0.3753, Train acc: 0.9124\n",
      "Valid loss: 0.8651, Valid acc: 0.7523\n",
      "Epoch 25:\n",
      "Train loss: 0.3385, Train acc: 0.9156\n",
      "Valid loss: 0.8264, Valid acc: 0.7817\n",
      "Epoch 26:\n",
      "Train loss: 0.2935, Train acc: 0.9331\n",
      "Valid loss: 0.8485, Valid acc: 0.7853\n",
      "Epoch 27:\n",
      "Train loss: 0.2884, Train acc: 0.9328\n",
      "Valid loss: 0.8284, Valid acc: 0.7853\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7881, Train acc: 0.2086\n",
      "Valid loss: 1.7682, Valid acc: 0.2349\n",
      "Epoch 2:\n",
      "Train loss: 1.7564, Train acc: 0.2105\n",
      "Valid loss: 1.7312, Valid acc: 0.2349\n",
      "Epoch 3:\n",
      "Train loss: 1.7147, Train acc: 0.2118\n",
      "Valid loss: 1.6883, Valid acc: 0.2422\n",
      "Epoch 4:\n",
      "Train loss: 1.6793, Train acc: 0.2265\n",
      "Valid loss: 1.6661, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6625, Train acc: 0.2304\n",
      "Valid loss: 1.6567, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6556, Train acc: 0.2309\n",
      "Valid loss: 1.6545, Valid acc: 0.2495\n",
      "Epoch 7:\n",
      "Train loss: 1.6543, Train acc: 0.2254\n",
      "Valid loss: 1.6524, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6485, Train acc: 0.2293\n",
      "Valid loss: 1.6518, Valid acc: 0.2486\n",
      "Epoch 9:\n",
      "Train loss: 1.6497, Train acc: 0.2260\n",
      "Valid loss: 1.6529, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6481, Train acc: 0.2240\n",
      "Valid loss: 1.6532, Valid acc: 0.2046\n",
      "Epoch 11:\n",
      "Train loss: 1.6554, Train acc: 0.2293\n",
      "Valid loss: 1.6522, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7456, Train acc: 0.2267\n",
      "Valid loss: 1.7182, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6871, Train acc: 0.2276\n",
      "Valid loss: 1.6558, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6562, Train acc: 0.2254\n",
      "Valid loss: 1.6514, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.6580, Train acc: 0.2219\n",
      "Valid loss: 1.6525, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6513, Train acc: 0.2235\n",
      "Valid loss: 1.6519, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6478, Train acc: 0.2290\n",
      "Valid loss: 1.6515, Valid acc: 0.2028\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7290, Train acc: 0.2182\n",
      "Valid loss: 1.6571, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6636, Train acc: 0.2263\n",
      "Valid loss: 1.6525, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6461, Train acc: 0.2361\n",
      "Valid loss: 1.6572, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6546, Train acc: 0.2258\n",
      "Valid loss: 1.6511, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.6541, Train acc: 0.2221\n",
      "Valid loss: 1.6559, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6477, Train acc: 0.2286\n",
      "Valid loss: 1.6506, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6462, Train acc: 0.2254\n",
      "Valid loss: 1.6538, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6737, Train acc: 0.2199\n",
      "Valid loss: 1.6499, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6483, Train acc: 0.2272\n",
      "Valid loss: 1.6487, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6471, Train acc: 0.2325\n",
      "Valid loss: 1.6443, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6466, Train acc: 0.2309\n",
      "Valid loss: 1.6461, Valid acc: 0.2394\n",
      "Epoch 5:\n",
      "Train loss: 1.6452, Train acc: 0.2354\n",
      "Valid loss: 1.6466, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6458, Train acc: 0.2221\n",
      "Valid loss: 1.6473, Valid acc: 0.2367\n",
      "Epoch 7:\n",
      "Train loss: 1.6477, Train acc: 0.2251\n",
      "Valid loss: 1.6464, Valid acc: 0.2385\n",
      "Epoch 8:\n",
      "Train loss: 1.6446, Train acc: 0.2281\n",
      "Valid loss: 1.6430, Valid acc: 0.2422\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6636, Train acc: 0.2283\n",
      "Valid loss: 1.6682, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6469, Train acc: 0.2295\n",
      "Valid loss: 1.6523, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6466, Train acc: 0.2311\n",
      "Valid loss: 1.6413, Valid acc: 0.2477\n",
      "Epoch 4:\n",
      "Train loss: 1.6427, Train acc: 0.2336\n",
      "Valid loss: 1.6552, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6433, Train acc: 0.2329\n",
      "Valid loss: 1.6413, Valid acc: 0.2523\n",
      "Epoch 6:\n",
      "Train loss: 1.6398, Train acc: 0.2444\n",
      "Valid loss: 1.6298, Valid acc: 0.2697\n",
      "Epoch 7:\n",
      "Train loss: 1.4321, Train acc: 0.3829\n",
      "Valid loss: 1.3228, Valid acc: 0.4477\n",
      "Epoch 8:\n",
      "Train loss: 1.2252, Train acc: 0.4959\n",
      "Valid loss: 1.1916, Valid acc: 0.5413\n",
      "Epoch 9:\n",
      "Train loss: 1.0887, Train acc: 0.5674\n",
      "Valid loss: 1.0774, Valid acc: 0.5688\n",
      "Epoch 10:\n",
      "Train loss: 0.9822, Train acc: 0.6197\n",
      "Valid loss: 1.0272, Valid acc: 0.5972\n",
      "Epoch 11:\n",
      "Train loss: 0.9157, Train acc: 0.6497\n",
      "Valid loss: 0.9703, Valid acc: 0.6339\n",
      "Epoch 12:\n",
      "Train loss: 0.8612, Train acc: 0.6699\n",
      "Valid loss: 0.9283, Valid acc: 0.6468\n",
      "Epoch 13:\n",
      "Train loss: 0.7958, Train acc: 0.6990\n",
      "Valid loss: 0.9824, Valid acc: 0.6000\n",
      "Epoch 14:\n",
      "Train loss: 0.7794, Train acc: 0.7017\n",
      "Valid loss: 0.8949, Valid acc: 0.6385\n",
      "Epoch 15:\n",
      "Train loss: 0.7061, Train acc: 0.7327\n",
      "Valid loss: 0.9887, Valid acc: 0.6284\n",
      "Epoch 16:\n",
      "Train loss: 0.6876, Train acc: 0.7412\n",
      "Valid loss: 0.9139, Valid acc: 0.6147\n",
      "Epoch 17:\n",
      "Train loss: 0.6584, Train acc: 0.7499\n",
      "Valid loss: 0.8670, Valid acc: 0.6734\n",
      "Epoch 18:\n",
      "Train loss: 0.6157, Train acc: 0.7666\n",
      "Valid loss: 0.9776, Valid acc: 0.6294\n",
      "Epoch 19:\n",
      "Train loss: 0.5978, Train acc: 0.7779\n",
      "Valid loss: 0.8825, Valid acc: 0.6569\n",
      "Epoch 20:\n",
      "Train loss: 0.5869, Train acc: 0.7847\n",
      "Valid loss: 0.8681, Valid acc: 0.6587\n",
      "Epoch 21:\n",
      "Train loss: 0.5657, Train acc: 0.7879\n",
      "Valid loss: 0.8612, Valid acc: 0.6991\n",
      "Epoch 22:\n",
      "Train loss: 0.5359, Train acc: 0.8148\n",
      "Valid loss: 0.8670, Valid acc: 0.6771\n",
      "Epoch 23:\n",
      "Train loss: 0.5105, Train acc: 0.8189\n",
      "Valid loss: 0.9152, Valid acc: 0.6706\n",
      "Epoch 24:\n",
      "Train loss: 0.5087, Train acc: 0.8271\n",
      "Valid loss: 0.8681, Valid acc: 0.6872\n",
      "Epoch 25:\n",
      "Train loss: 0.4753, Train acc: 0.8475\n",
      "Valid loss: 0.8558, Valid acc: 0.7202\n",
      "Epoch 26:\n",
      "Train loss: 0.4427, Train acc: 0.8647\n",
      "Valid loss: 0.8267, Valid acc: 0.7413\n",
      "Epoch 27:\n",
      "Train loss: 0.3925, Train acc: 0.8975\n",
      "Valid loss: 0.8687, Valid acc: 0.7523\n",
      "Epoch 28:\n",
      "Train loss: 0.3612, Train acc: 0.9076\n",
      "Valid loss: 0.8418, Valid acc: 0.7495\n",
      "Epoch 29:\n",
      "Train loss: 0.3336, Train acc: 0.9207\n",
      "Valid loss: 0.8086, Valid acc: 0.7734\n",
      "Epoch 30:\n",
      "Train loss: 0.2910, Train acc: 0.9340\n",
      "Valid loss: 0.8029, Valid acc: 0.7835\n",
      "Epoch 31:\n",
      "Train loss: 0.2914, Train acc: 0.9340\n",
      "Valid loss: 0.8405, Valid acc: 0.7862\n",
      "Epoch 32:\n",
      "Train loss: 0.2663, Train acc: 0.9381\n",
      "Valid loss: 0.8268, Valid acc: 0.7927\n",
      "Epoch 33:\n",
      "Train loss: 0.2663, Train acc: 0.9411\n",
      "Valid loss: 0.7937, Valid acc: 0.7954\n",
      "Epoch 34:\n",
      "Train loss: 0.2307, Train acc: 0.9519\n",
      "Valid loss: 0.8152, Valid acc: 0.7936\n",
      "Epoch 35:\n",
      "Train loss: 0.2475, Train acc: 0.9450\n",
      "Valid loss: 0.8396, Valid acc: 0.7789\n",
      "Epoch 36:\n",
      "Train loss: 0.2141, Train acc: 0.9551\n",
      "Valid loss: 0.8576, Valid acc: 0.7752\n",
      "Epoch 37:\n",
      "Train loss: 0.2314, Train acc: 0.9466\n",
      "Valid loss: 0.9903, Valid acc: 0.7532\n",
      "Epoch 38:\n",
      "Train loss: 0.1925, Train acc: 0.9590\n",
      "Valid loss: 0.8461, Valid acc: 0.7881\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6601, Train acc: 0.2171\n",
      "Valid loss: 1.6601, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6488, Train acc: 0.2309\n",
      "Valid loss: 1.6445, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.5740, Train acc: 0.2815\n",
      "Valid loss: 1.4799, Valid acc: 0.3761\n",
      "Epoch 4:\n",
      "Train loss: 1.3288, Train acc: 0.4225\n",
      "Valid loss: 1.2497, Valid acc: 0.4596\n",
      "Epoch 5:\n",
      "Train loss: 1.1524, Train acc: 0.5188\n",
      "Valid loss: 1.1463, Valid acc: 0.5220\n",
      "Epoch 6:\n",
      "Train loss: 1.0407, Train acc: 0.5768\n",
      "Valid loss: 1.0632, Valid acc: 0.5761\n",
      "Epoch 7:\n",
      "Train loss: 0.9899, Train acc: 0.6114\n",
      "Valid loss: 0.9954, Valid acc: 0.6275\n",
      "Epoch 8:\n",
      "Train loss: 0.9258, Train acc: 0.6376\n",
      "Valid loss: 0.9997, Valid acc: 0.6092\n",
      "Epoch 9:\n",
      "Train loss: 0.8808, Train acc: 0.6708\n",
      "Valid loss: 0.9602, Valid acc: 0.6202\n",
      "Epoch 10:\n",
      "Train loss: 0.8341, Train acc: 0.7054\n",
      "Valid loss: 1.0744, Valid acc: 0.5688\n",
      "Epoch 11:\n",
      "Train loss: 0.7551, Train acc: 0.7421\n",
      "Valid loss: 0.8507, Valid acc: 0.7220\n",
      "Epoch 12:\n",
      "Train loss: 0.7259, Train acc: 0.7620\n",
      "Valid loss: 0.8331, Valid acc: 0.7367\n",
      "Epoch 13:\n",
      "Train loss: 0.6283, Train acc: 0.8065\n",
      "Valid loss: 0.8459, Valid acc: 0.7367\n",
      "Epoch 14:\n",
      "Train loss: 0.5738, Train acc: 0.8287\n",
      "Valid loss: 0.7638, Valid acc: 0.7578\n",
      "Epoch 15:\n",
      "Train loss: 0.5917, Train acc: 0.8237\n",
      "Valid loss: 0.8553, Valid acc: 0.7220\n",
      "Epoch 16:\n",
      "Train loss: 0.4803, Train acc: 0.8604\n",
      "Valid loss: 0.8729, Valid acc: 0.7055\n",
      "Epoch 17:\n",
      "Train loss: 0.4853, Train acc: 0.8631\n",
      "Valid loss: 0.7892, Valid acc: 0.7587\n",
      "Epoch 18:\n",
      "Train loss: 0.4744, Train acc: 0.8682\n",
      "Valid loss: 0.7465, Valid acc: 0.7679\n",
      "Epoch 19:\n",
      "Train loss: 0.3970, Train acc: 0.8978\n",
      "Valid loss: 0.7857, Valid acc: 0.7642\n",
      "Epoch 20:\n",
      "Train loss: 0.4232, Train acc: 0.8829\n",
      "Valid loss: 0.7620, Valid acc: 0.7789\n",
      "Epoch 21:\n",
      "Train loss: 0.4085, Train acc: 0.8941\n",
      "Valid loss: 0.8034, Valid acc: 0.7716\n",
      "Epoch 22:\n",
      "Train loss: 0.3601, Train acc: 0.9060\n",
      "Valid loss: 1.5158, Valid acc: 0.5560\n",
      "Epoch 23:\n",
      "Train loss: 0.3710, Train acc: 0.9060\n",
      "Valid loss: 0.7765, Valid acc: 0.7817\n",
      "Epoch 24:\n",
      "Train loss: 0.3512, Train acc: 0.9110\n",
      "Valid loss: 1.1756, Valid acc: 0.6532\n",
      "Epoch 25:\n",
      "Train loss: 0.3105, Train acc: 0.9241\n",
      "Valid loss: 0.8939, Valid acc: 0.7505\n",
      "Epoch 26:\n",
      "Train loss: 0.2937, Train acc: 0.9262\n",
      "Valid loss: 1.0567, Valid acc: 0.7064\n",
      "Epoch 27:\n",
      "Train loss: 0.3264, Train acc: 0.9216\n",
      "Valid loss: 0.7441, Valid acc: 0.8046\n",
      "Epoch 28:\n",
      "Train loss: 0.2824, Train acc: 0.9365\n",
      "Valid loss: 0.8695, Valid acc: 0.7688\n",
      "Epoch 29:\n",
      "Train loss: 0.2840, Train acc: 0.9303\n",
      "Valid loss: 0.8027, Valid acc: 0.7853\n",
      "Epoch 30:\n",
      "Train loss: 0.2622, Train acc: 0.9372\n",
      "Valid loss: 0.7665, Valid acc: 0.8000\n",
      "Epoch 31:\n",
      "Train loss: 0.3220, Train acc: 0.9246\n",
      "Valid loss: 0.8806, Valid acc: 0.7661\n",
      "Epoch 32:\n",
      "Train loss: 0.2373, Train acc: 0.9443\n",
      "Valid loss: 0.8209, Valid acc: 0.7936\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7101, Train acc: 0.2283\n",
      "Valid loss: 1.6686, Valid acc: 0.2147\n",
      "Epoch 2:\n",
      "Train loss: 1.6534, Train acc: 0.2208\n",
      "Valid loss: 1.6566, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6499, Train acc: 0.2274\n",
      "Valid loss: 1.6508, Valid acc: 0.2073\n",
      "Epoch 4:\n",
      "Train loss: 1.6516, Train acc: 0.2153\n",
      "Valid loss: 1.6520, Valid acc: 0.2055\n",
      "Epoch 5:\n",
      "Train loss: 1.6525, Train acc: 0.2178\n",
      "Valid loss: 1.6454, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.6480, Train acc: 0.2286\n",
      "Valid loss: 1.6438, Valid acc: 0.2459\n",
      "Epoch 7:\n",
      "Train loss: 1.6496, Train acc: 0.2290\n",
      "Valid loss: 1.6466, Valid acc: 0.2468\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6696, Train acc: 0.2189\n",
      "Valid loss: 1.6533, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6474, Train acc: 0.2164\n",
      "Valid loss: 1.6372, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6514, Train acc: 0.2329\n",
      "Valid loss: 1.6504, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6492, Train acc: 0.2315\n",
      "Valid loss: 1.6447, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.6487, Train acc: 0.2302\n",
      "Valid loss: 1.6447, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.6527, Train acc: 0.2322\n",
      "Valid loss: 1.6454, Valid acc: 0.2413\n",
      "Epoch 7:\n",
      "Train loss: 1.6449, Train acc: 0.2267\n",
      "Valid loss: 1.6347, Valid acc: 0.2459\n",
      "Epoch 8:\n",
      "Train loss: 1.6499, Train acc: 0.2281\n",
      "Valid loss: 1.6324, Valid acc: 0.2514\n",
      "Epoch 9:\n",
      "Train loss: 1.6457, Train acc: 0.2254\n",
      "Valid loss: 1.6400, Valid acc: 0.2459\n",
      "Epoch 10:\n",
      "Train loss: 1.6447, Train acc: 0.2306\n",
      "Valid loss: 1.6418, Valid acc: 0.2459\n",
      "Epoch 11:\n",
      "Train loss: 1.6407, Train acc: 0.2405\n",
      "Valid loss: 1.6610, Valid acc: 0.2046\n",
      "Epoch 12:\n",
      "Train loss: 1.5447, Train acc: 0.3249\n",
      "Valid loss: 1.4284, Valid acc: 0.4018\n",
      "Epoch 13:\n",
      "Train loss: 1.3271, Train acc: 0.4188\n",
      "Valid loss: 1.3143, Valid acc: 0.4560\n",
      "Epoch 14:\n",
      "Train loss: 1.2036, Train acc: 0.4746\n",
      "Valid loss: 1.4799, Valid acc: 0.3468\n",
      "Epoch 15:\n",
      "Train loss: 1.1170, Train acc: 0.5360\n",
      "Valid loss: 1.2018, Valid acc: 0.5055\n",
      "Epoch 16:\n",
      "Train loss: 1.0399, Train acc: 0.5761\n",
      "Valid loss: 1.1477, Valid acc: 0.5303\n",
      "Epoch 17:\n",
      "Train loss: 0.9772, Train acc: 0.6087\n",
      "Valid loss: 1.2527, Valid acc: 0.4862\n",
      "Epoch 18:\n",
      "Train loss: 0.9387, Train acc: 0.6263\n",
      "Valid loss: 1.2106, Valid acc: 0.5330\n",
      "Epoch 19:\n",
      "Train loss: 0.8858, Train acc: 0.6513\n",
      "Valid loss: 1.0493, Valid acc: 0.5817\n",
      "Epoch 20:\n",
      "Train loss: 0.8171, Train acc: 0.6889\n",
      "Valid loss: 1.0723, Valid acc: 0.5853\n",
      "Epoch 21:\n",
      "Train loss: 0.7958, Train acc: 0.6898\n",
      "Valid loss: 1.1291, Valid acc: 0.5780\n",
      "Epoch 22:\n",
      "Train loss: 0.7828, Train acc: 0.6985\n",
      "Valid loss: 1.3973, Valid acc: 0.5083\n",
      "Epoch 23:\n",
      "Train loss: 0.7142, Train acc: 0.7251\n",
      "Valid loss: 1.4596, Valid acc: 0.5642\n",
      "Epoch 24:\n",
      "Train loss: 0.6941, Train acc: 0.7368\n",
      "Valid loss: 1.3705, Valid acc: 0.5706\n",
      "Epoch 25:\n",
      "Train loss: 0.6738, Train acc: 0.7400\n",
      "Valid loss: 1.0501, Valid acc: 0.6165\n",
      "Epoch 26:\n",
      "Train loss: 0.6384, Train acc: 0.7558\n",
      "Valid loss: 1.0486, Valid acc: 0.6303\n",
      "Epoch 27:\n",
      "Train loss: 0.6160, Train acc: 0.7687\n",
      "Valid loss: 1.0012, Valid acc: 0.6239\n",
      "Epoch 28:\n",
      "Train loss: 0.5853, Train acc: 0.7746\n",
      "Valid loss: 1.0679, Valid acc: 0.6615\n",
      "Epoch 29:\n",
      "Train loss: 0.5789, Train acc: 0.7831\n",
      "Valid loss: 1.0498, Valid acc: 0.6303\n",
      "Epoch 30:\n",
      "Train loss: 0.5625, Train acc: 0.7907\n",
      "Valid loss: 1.3390, Valid acc: 0.5881\n",
      "Epoch 31:\n",
      "Train loss: 0.5697, Train acc: 0.7960\n",
      "Valid loss: 1.1746, Valid acc: 0.6394\n",
      "Epoch 32:\n",
      "Train loss: 0.5181, Train acc: 0.8313\n",
      "Valid loss: 0.9562, Valid acc: 0.6569\n",
      "Epoch 33:\n",
      "Train loss: 0.5237, Train acc: 0.8287\n",
      "Valid loss: 1.3295, Valid acc: 0.6642\n",
      "Epoch 34:\n",
      "Train loss: 0.4603, Train acc: 0.8652\n",
      "Valid loss: 1.0907, Valid acc: 0.7000\n",
      "Epoch 35:\n",
      "Train loss: 0.4592, Train acc: 0.8767\n",
      "Valid loss: 1.0771, Valid acc: 0.7165\n",
      "Epoch 36:\n",
      "Train loss: 0.3982, Train acc: 0.9014\n",
      "Valid loss: 1.5203, Valid acc: 0.6073\n",
      "Epoch 37:\n",
      "Train loss: 0.3786, Train acc: 0.9104\n",
      "Valid loss: 1.0104, Valid acc: 0.7550\n",
      "Epoch 38:\n",
      "Train loss: 0.3541, Train acc: 0.9152\n",
      "Valid loss: 1.1595, Valid acc: 0.7303\n",
      "Epoch 39:\n",
      "Train loss: 0.3651, Train acc: 0.9106\n",
      "Valid loss: 1.1003, Valid acc: 0.7000\n",
      "Epoch 40:\n",
      "Train loss: 0.3288, Train acc: 0.9198\n",
      "Valid loss: 1.3589, Valid acc: 0.6578\n",
      "Epoch 41:\n",
      "Train loss: 0.3058, Train acc: 0.9280\n",
      "Valid loss: 0.9707, Valid acc: 0.7651\n",
      "Epoch 42:\n",
      "Train loss: 0.2874, Train acc: 0.9388\n",
      "Valid loss: 1.0076, Valid acc: 0.7716\n",
      "Epoch 43:\n",
      "Train loss: 0.3377, Train acc: 0.9216\n",
      "Valid loss: 1.0895, Valid acc: 0.7193\n",
      "Epoch 44:\n",
      "Train loss: 0.2964, Train acc: 0.9337\n",
      "Valid loss: 1.2458, Valid acc: 0.7266\n",
      "Epoch 45:\n",
      "Train loss: 0.2656, Train acc: 0.9425\n",
      "Valid loss: 1.0715, Valid acc: 0.7560\n",
      "Epoch 46:\n",
      "Train loss: 0.2311, Train acc: 0.9519\n",
      "Valid loss: 0.9518, Valid acc: 0.7752\n",
      "Epoch 47:\n",
      "Train loss: 0.2305, Train acc: 0.9544\n",
      "Valid loss: 0.9407, Valid acc: 0.7798\n",
      "Epoch 48:\n",
      "Train loss: 0.2534, Train acc: 0.9459\n",
      "Valid loss: 0.9192, Valid acc: 0.7908\n",
      "Epoch 49:\n",
      "Train loss: 0.2364, Train acc: 0.9505\n",
      "Valid loss: 0.9068, Valid acc: 0.7872\n",
      "Epoch 50:\n",
      "Train loss: 0.2190, Train acc: 0.9564\n",
      "Valid loss: 1.2751, Valid acc: 0.7394\n",
      "Epoch 51:\n",
      "Train loss: 0.2601, Train acc: 0.9452\n",
      "Valid loss: 1.0181, Valid acc: 0.7734\n",
      "Epoch 52:\n",
      "Train loss: 0.2136, Train acc: 0.9571\n",
      "Valid loss: 1.0289, Valid acc: 0.7450\n",
      "Epoch 53:\n",
      "Train loss: 0.2236, Train acc: 0.9541\n",
      "Valid loss: 0.9792, Valid acc: 0.7945\n",
      "Epoch 54:\n",
      "Train loss: 0.2445, Train acc: 0.9491\n",
      "Valid loss: 0.8847, Valid acc: 0.7927\n",
      "Epoch 55:\n",
      "Train loss: 0.1901, Train acc: 0.9629\n",
      "Valid loss: 1.0232, Valid acc: 0.7844\n",
      "Epoch 56:\n",
      "Train loss: 0.2427, Train acc: 0.9505\n",
      "Valid loss: 0.9832, Valid acc: 0.7798\n",
      "Epoch 57:\n",
      "Train loss: 0.1776, Train acc: 0.9670\n",
      "Valid loss: 0.9851, Valid acc: 0.7927\n",
      "Epoch 58:\n",
      "Train loss: 0.2103, Train acc: 0.9560\n",
      "Valid loss: 0.9313, Valid acc: 0.7917\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6638, Train acc: 0.2228\n",
      "Valid loss: 1.6446, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6493, Train acc: 0.2276\n",
      "Valid loss: 1.6427, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6461, Train acc: 0.2242\n",
      "Valid loss: 1.6613, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6469, Train acc: 0.2201\n",
      "Valid loss: 1.6482, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6478, Train acc: 0.2338\n",
      "Valid loss: 1.6381, Valid acc: 0.2394\n",
      "Epoch 6:\n",
      "Train loss: 1.6102, Train acc: 0.2556\n",
      "Valid loss: 1.5803, Valid acc: 0.2908\n",
      "Epoch 7:\n",
      "Train loss: 1.4462, Train acc: 0.3838\n",
      "Valid loss: 1.4100, Valid acc: 0.3954\n",
      "Epoch 8:\n",
      "Train loss: 1.2765, Train acc: 0.4727\n",
      "Valid loss: 1.1836, Valid acc: 0.5275\n",
      "Epoch 9:\n",
      "Train loss: 1.1388, Train acc: 0.5273\n",
      "Valid loss: 1.1519, Valid acc: 0.5321\n",
      "Epoch 10:\n",
      "Train loss: 1.0862, Train acc: 0.5463\n",
      "Valid loss: 1.0941, Valid acc: 0.5734\n",
      "Epoch 11:\n",
      "Train loss: 0.9982, Train acc: 0.5846\n",
      "Valid loss: 1.0618, Valid acc: 0.5532\n",
      "Epoch 12:\n",
      "Train loss: 0.9577, Train acc: 0.5974\n",
      "Valid loss: 1.2058, Valid acc: 0.5248\n",
      "Epoch 13:\n",
      "Train loss: 0.9361, Train acc: 0.6176\n",
      "Valid loss: 1.0890, Valid acc: 0.5321\n",
      "Epoch 14:\n",
      "Train loss: 0.8690, Train acc: 0.6440\n",
      "Valid loss: 0.9727, Valid acc: 0.6064\n",
      "Epoch 15:\n",
      "Train loss: 0.8536, Train acc: 0.6671\n",
      "Valid loss: 0.9556, Valid acc: 0.6239\n",
      "Epoch 16:\n",
      "Train loss: 0.8027, Train acc: 0.6923\n",
      "Valid loss: 1.0425, Valid acc: 0.5725\n",
      "Epoch 17:\n",
      "Train loss: 0.7545, Train acc: 0.7293\n",
      "Valid loss: 1.0117, Valid acc: 0.5826\n",
      "Epoch 18:\n",
      "Train loss: 0.7449, Train acc: 0.7377\n",
      "Valid loss: 0.8988, Valid acc: 0.6459\n",
      "Epoch 19:\n",
      "Train loss: 0.6712, Train acc: 0.7623\n",
      "Valid loss: 0.9088, Valid acc: 0.6872\n",
      "Epoch 20:\n",
      "Train loss: 0.6913, Train acc: 0.7685\n",
      "Valid loss: 1.1036, Valid acc: 0.6165\n",
      "Epoch 21:\n",
      "Train loss: 0.6275, Train acc: 0.7941\n",
      "Valid loss: 0.8775, Valid acc: 0.7138\n",
      "Epoch 22:\n",
      "Train loss: 0.5989, Train acc: 0.8180\n",
      "Valid loss: 0.8692, Valid acc: 0.7046\n",
      "Epoch 23:\n",
      "Train loss: 0.5554, Train acc: 0.8436\n",
      "Valid loss: 1.2366, Valid acc: 0.5532\n",
      "Epoch 24:\n",
      "Train loss: 0.5662, Train acc: 0.8315\n",
      "Valid loss: 0.8355, Valid acc: 0.7440\n",
      "Epoch 25:\n",
      "Train loss: 0.4832, Train acc: 0.8684\n",
      "Valid loss: 0.8346, Valid acc: 0.7367\n",
      "Epoch 26:\n",
      "Train loss: 0.4679, Train acc: 0.8659\n",
      "Valid loss: 1.0633, Valid acc: 0.7083\n",
      "Epoch 27:\n",
      "Train loss: 0.4774, Train acc: 0.8602\n",
      "Valid loss: 0.8115, Valid acc: 0.7569\n",
      "Epoch 28:\n",
      "Train loss: 0.3937, Train acc: 0.8950\n",
      "Valid loss: 0.8524, Valid acc: 0.7413\n",
      "Epoch 29:\n",
      "Train loss: 0.4098, Train acc: 0.8893\n",
      "Valid loss: 0.8191, Valid acc: 0.7688\n",
      "Epoch 30:\n",
      "Train loss: 0.3742, Train acc: 0.9019\n",
      "Valid loss: 0.9010, Valid acc: 0.7569\n",
      "Epoch 31:\n",
      "Train loss: 0.3529, Train acc: 0.9074\n",
      "Valid loss: 0.8595, Valid acc: 0.7495\n",
      "Epoch 32:\n",
      "Train loss: 0.3504, Train acc: 0.9012\n",
      "Valid loss: 0.8036, Valid acc: 0.7908\n",
      "Epoch 33:\n",
      "Train loss: 0.3504, Train acc: 0.9110\n",
      "Valid loss: 0.8375, Valid acc: 0.7688\n",
      "Epoch 34:\n",
      "Train loss: 0.3817, Train acc: 0.8991\n",
      "Valid loss: 0.7776, Valid acc: 0.7743\n",
      "Epoch 35:\n",
      "Train loss: 0.3095, Train acc: 0.9188\n",
      "Valid loss: 0.7874, Valid acc: 0.7780\n",
      "Epoch 36:\n",
      "Train loss: 0.2663, Train acc: 0.9349\n",
      "Valid loss: 0.8074, Valid acc: 0.7807\n",
      "Epoch 37:\n",
      "Train loss: 0.2898, Train acc: 0.9310\n",
      "Valid loss: 0.8802, Valid acc: 0.7661\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7054, Train acc: 0.2217\n",
      "Valid loss: 1.6748, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6626, Train acc: 0.2274\n",
      "Valid loss: 1.6592, Valid acc: 0.2028\n",
      "Epoch 3:\n",
      "Train loss: 1.6533, Train acc: 0.2238\n",
      "Valid loss: 1.6544, Valid acc: 0.2055\n",
      "Epoch 4:\n",
      "Train loss: 1.6583, Train acc: 0.2144\n",
      "Valid loss: 1.6532, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6484, Train acc: 0.2270\n",
      "Valid loss: 1.6534, Valid acc: 0.2495\n",
      "Epoch 6:\n",
      "Train loss: 1.6518, Train acc: 0.2272\n",
      "Valid loss: 1.6527, Valid acc: 0.2028\n",
      "Epoch 7:\n",
      "Train loss: 1.6514, Train acc: 0.2258\n",
      "Valid loss: 1.6529, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.6522, Train acc: 0.2226\n",
      "Valid loss: 1.6510, Valid acc: 0.2028\n",
      "Epoch 9:\n",
      "Train loss: 1.6560, Train acc: 0.2295\n",
      "Valid loss: 1.6521, Valid acc: 0.2028\n",
      "Epoch 10:\n",
      "Train loss: 1.6443, Train acc: 0.2313\n",
      "Valid loss: 1.6516, Valid acc: 0.2468\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6861, Train acc: 0.2249\n",
      "Valid loss: 1.6636, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6495, Train acc: 0.2263\n",
      "Valid loss: 1.6556, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6473, Train acc: 0.2281\n",
      "Valid loss: 1.6636, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6494, Train acc: 0.2201\n",
      "Valid loss: 1.6513, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.6571, Train acc: 0.2265\n",
      "Valid loss: 1.6500, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6512, Train acc: 0.2205\n",
      "Valid loss: 1.6519, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6468, Train acc: 0.2226\n",
      "Valid loss: 1.6593, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6479, Train acc: 0.2302\n",
      "Valid loss: 1.6515, Valid acc: 0.2394\n",
      "Epoch 9:\n",
      "Train loss: 1.6542, Train acc: 0.2242\n",
      "Valid loss: 1.6503, Valid acc: 0.2413\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6706, Train acc: 0.2265\n",
      "Valid loss: 1.6544, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6516, Train acc: 0.2254\n",
      "Valid loss: 1.6544, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6472, Train acc: 0.2276\n",
      "Valid loss: 1.6505, Valid acc: 0.2376\n",
      "Epoch 4:\n",
      "Train loss: 1.6513, Train acc: 0.2258\n",
      "Valid loss: 1.6526, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6425, Train acc: 0.2306\n",
      "Valid loss: 1.7163, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6541, Train acc: 0.2249\n",
      "Valid loss: 1.6573, Valid acc: 0.2486\n",
      "Epoch 7:\n",
      "Train loss: 1.6479, Train acc: 0.2336\n",
      "Valid loss: 1.6497, Valid acc: 0.2450\n",
      "Epoch 8:\n",
      "Train loss: 1.6481, Train acc: 0.2359\n",
      "Valid loss: 1.6923, Valid acc: 0.2394\n",
      "Epoch 9:\n",
      "Train loss: 1.6433, Train acc: 0.2405\n",
      "Valid loss: 1.6754, Valid acc: 0.2532\n",
      "Epoch 10:\n",
      "Train loss: 1.5945, Train acc: 0.2980\n",
      "Valid loss: 1.5762, Valid acc: 0.3477\n",
      "Epoch 11:\n",
      "Train loss: 1.4914, Train acc: 0.3785\n",
      "Valid loss: 1.5349, Valid acc: 0.3734\n",
      "Epoch 12:\n",
      "Train loss: 1.4369, Train acc: 0.3957\n",
      "Valid loss: 1.4780, Valid acc: 0.3771\n",
      "Epoch 13:\n",
      "Train loss: 1.3756, Train acc: 0.4159\n",
      "Valid loss: 1.4554, Valid acc: 0.3752\n",
      "Epoch 14:\n",
      "Train loss: 1.2996, Train acc: 0.4356\n",
      "Valid loss: 1.4045, Valid acc: 0.3972\n",
      "Epoch 15:\n",
      "Train loss: 1.2504, Train acc: 0.4613\n",
      "Valid loss: 1.4939, Valid acc: 0.3844\n",
      "Epoch 16:\n",
      "Train loss: 1.1675, Train acc: 0.4991\n",
      "Valid loss: 1.3445, Valid acc: 0.4679\n",
      "Epoch 17:\n",
      "Train loss: 1.1531, Train acc: 0.5241\n",
      "Valid loss: 1.3286, Valid acc: 0.4697\n",
      "Epoch 18:\n",
      "Train loss: 1.0676, Train acc: 0.5573\n",
      "Valid loss: 1.2579, Valid acc: 0.5064\n",
      "Epoch 19:\n",
      "Train loss: 1.0675, Train acc: 0.5692\n",
      "Valid loss: 1.2040, Valid acc: 0.5431\n",
      "Epoch 20:\n",
      "Train loss: 0.9873, Train acc: 0.6084\n",
      "Valid loss: 1.3155, Valid acc: 0.4963\n",
      "Epoch 21:\n",
      "Train loss: 0.9936, Train acc: 0.5986\n",
      "Valid loss: 1.1572, Valid acc: 0.5550\n",
      "Epoch 22:\n",
      "Train loss: 0.9041, Train acc: 0.6428\n",
      "Valid loss: 1.1380, Valid acc: 0.5266\n",
      "Epoch 23:\n",
      "Train loss: 0.8711, Train acc: 0.6632\n",
      "Valid loss: 1.0920, Valid acc: 0.5936\n",
      "Epoch 24:\n",
      "Train loss: 0.8812, Train acc: 0.6502\n",
      "Valid loss: 1.2181, Valid acc: 0.5615\n",
      "Epoch 25:\n",
      "Train loss: 0.7963, Train acc: 0.6901\n",
      "Valid loss: 1.4568, Valid acc: 0.4312\n",
      "Epoch 26:\n",
      "Train loss: 0.8097, Train acc: 0.6715\n",
      "Valid loss: 1.0689, Valid acc: 0.5963\n",
      "Epoch 27:\n",
      "Train loss: 0.7444, Train acc: 0.6967\n",
      "Valid loss: 1.0073, Valid acc: 0.6339\n",
      "Epoch 28:\n",
      "Train loss: 0.7067, Train acc: 0.7125\n",
      "Valid loss: 1.0222, Valid acc: 0.6266\n",
      "Epoch 29:\n",
      "Train loss: 0.7074, Train acc: 0.7082\n",
      "Valid loss: 1.0883, Valid acc: 0.6138\n",
      "Epoch 30:\n",
      "Train loss: 0.6997, Train acc: 0.7141\n",
      "Valid loss: 0.9605, Valid acc: 0.6651\n",
      "Epoch 31:\n",
      "Train loss: 0.6339, Train acc: 0.7373\n",
      "Valid loss: 1.0257, Valid acc: 0.6294\n",
      "Epoch 32:\n",
      "Train loss: 0.6556, Train acc: 0.7338\n",
      "Valid loss: 1.0371, Valid acc: 0.6367\n",
      "Epoch 33:\n",
      "Train loss: 0.6136, Train acc: 0.7458\n",
      "Valid loss: 1.1089, Valid acc: 0.6101\n",
      "Epoch 34:\n",
      "Train loss: 0.6362, Train acc: 0.7299\n",
      "Valid loss: 1.1602, Valid acc: 0.6064\n",
      "Epoch 35:\n",
      "Train loss: 0.5757, Train acc: 0.7558\n",
      "Valid loss: 1.1112, Valid acc: 0.6193\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7863, Train acc: 0.2180\n",
      "Valid loss: 1.7897, Valid acc: 0.1936\n",
      "Epoch 2:\n",
      "Train loss: 1.7801, Train acc: 0.2144\n",
      "Valid loss: 1.7832, Valid acc: 0.1936\n",
      "Epoch 3:\n",
      "Train loss: 1.7739, Train acc: 0.2166\n",
      "Valid loss: 1.7771, Valid acc: 0.1954\n",
      "Epoch 4:\n",
      "Train loss: 1.7677, Train acc: 0.2178\n",
      "Valid loss: 1.7714, Valid acc: 0.1982\n",
      "Epoch 5:\n",
      "Train loss: 1.7626, Train acc: 0.2205\n",
      "Valid loss: 1.7660, Valid acc: 0.1991\n",
      "Epoch 6:\n",
      "Train loss: 1.7576, Train acc: 0.2196\n",
      "Valid loss: 1.7609, Valid acc: 0.1982\n",
      "Epoch 7:\n",
      "Train loss: 1.7527, Train acc: 0.2226\n",
      "Valid loss: 1.7560, Valid acc: 0.1982\n",
      "Epoch 8:\n",
      "Train loss: 1.7478, Train acc: 0.2244\n",
      "Valid loss: 1.7513, Valid acc: 0.2009\n",
      "Epoch 9:\n",
      "Train loss: 1.7434, Train acc: 0.2233\n",
      "Valid loss: 1.7469, Valid acc: 0.2018\n",
      "Epoch 10:\n",
      "Train loss: 1.7393, Train acc: 0.2256\n",
      "Valid loss: 1.7427, Valid acc: 0.2009\n",
      "Epoch 11:\n",
      "Train loss: 1.7348, Train acc: 0.2272\n",
      "Valid loss: 1.7386, Valid acc: 0.2028\n",
      "Epoch 12:\n",
      "Train loss: 1.7320, Train acc: 0.2267\n",
      "Valid loss: 1.7346, Valid acc: 0.2018\n",
      "Epoch 13:\n",
      "Train loss: 1.7278, Train acc: 0.2283\n",
      "Valid loss: 1.7308, Valid acc: 0.2028\n",
      "Epoch 14:\n",
      "Train loss: 1.7241, Train acc: 0.2274\n",
      "Valid loss: 1.7271, Valid acc: 0.2037\n",
      "Epoch 15:\n",
      "Train loss: 1.7209, Train acc: 0.2276\n",
      "Valid loss: 1.7236, Valid acc: 0.2037\n",
      "Epoch 16:\n",
      "Train loss: 1.7170, Train acc: 0.2297\n",
      "Valid loss: 1.7202, Valid acc: 0.2037\n",
      "Epoch 17:\n",
      "Train loss: 1.7143, Train acc: 0.2281\n",
      "Valid loss: 1.7168, Valid acc: 0.2037\n",
      "Epoch 18:\n",
      "Train loss: 1.7107, Train acc: 0.2293\n",
      "Valid loss: 1.7136, Valid acc: 0.2037\n",
      "Epoch 19:\n",
      "Train loss: 1.7087, Train acc: 0.2283\n",
      "Valid loss: 1.7104, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7777, Train acc: 0.1742\n",
      "Valid loss: 1.7855, Valid acc: 0.1505\n",
      "Epoch 2:\n",
      "Train loss: 1.7717, Train acc: 0.1802\n",
      "Valid loss: 1.7790, Valid acc: 0.1541\n",
      "Epoch 3:\n",
      "Train loss: 1.7656, Train acc: 0.1843\n",
      "Valid loss: 1.7728, Valid acc: 0.1532\n",
      "Epoch 4:\n",
      "Train loss: 1.7589, Train acc: 0.1859\n",
      "Valid loss: 1.7671, Valid acc: 0.1596\n",
      "Epoch 5:\n",
      "Train loss: 1.7542, Train acc: 0.2208\n",
      "Valid loss: 1.7616, Valid acc: 0.2073\n",
      "Epoch 6:\n",
      "Train loss: 1.7491, Train acc: 0.2228\n",
      "Valid loss: 1.7564, Valid acc: 0.2092\n",
      "Epoch 7:\n",
      "Train loss: 1.7437, Train acc: 0.2263\n",
      "Valid loss: 1.7515, Valid acc: 0.2083\n",
      "Epoch 8:\n",
      "Train loss: 1.7391, Train acc: 0.2267\n",
      "Valid loss: 1.7468, Valid acc: 0.2092\n",
      "Epoch 9:\n",
      "Train loss: 1.7363, Train acc: 0.2272\n",
      "Valid loss: 1.7423, Valid acc: 0.2092\n",
      "Epoch 10:\n",
      "Train loss: 1.7307, Train acc: 0.2258\n",
      "Valid loss: 1.7379, Valid acc: 0.2092\n",
      "Epoch 11:\n",
      "Train loss: 1.7274, Train acc: 0.2254\n",
      "Valid loss: 1.7337, Valid acc: 0.2092\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7812, Train acc: 0.2182\n",
      "Valid loss: 1.7809, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.7732, Train acc: 0.2215\n",
      "Valid loss: 1.7737, Valid acc: 0.2450\n",
      "Epoch 3:\n",
      "Train loss: 1.7666, Train acc: 0.2242\n",
      "Valid loss: 1.7669, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.7609, Train acc: 0.2270\n",
      "Valid loss: 1.7605, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.7545, Train acc: 0.2281\n",
      "Valid loss: 1.7544, Valid acc: 0.2477\n",
      "Epoch 6:\n",
      "Train loss: 1.7488, Train acc: 0.2286\n",
      "Valid loss: 1.7487, Valid acc: 0.2477\n",
      "Epoch 7:\n",
      "Train loss: 1.7443, Train acc: 0.2293\n",
      "Valid loss: 1.7432, Valid acc: 0.2477\n",
      "Epoch 8:\n",
      "Train loss: 1.7392, Train acc: 0.2258\n",
      "Valid loss: 1.7380, Valid acc: 0.2486\n",
      "Epoch 9:\n",
      "Train loss: 1.7343, Train acc: 0.2242\n",
      "Valid loss: 1.7329, Valid acc: 0.2477\n",
      "Epoch 10:\n",
      "Train loss: 1.7291, Train acc: 0.2265\n",
      "Valid loss: 1.7281, Valid acc: 0.2459\n",
      "Epoch 11:\n",
      "Train loss: 1.7248, Train acc: 0.2334\n",
      "Valid loss: 1.7235, Valid acc: 0.2009\n",
      "Epoch 12:\n",
      "Train loss: 1.7201, Train acc: 0.2251\n",
      "Valid loss: 1.7191, Valid acc: 0.2028\n",
      "Epoch 13:\n",
      "Train loss: 1.7168, Train acc: 0.2283\n",
      "Valid loss: 1.7148, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7866, Train acc: 0.2212\n",
      "Valid loss: 1.8021, Valid acc: 0.2000\n",
      "Epoch 2:\n",
      "Train loss: 1.7830, Train acc: 0.2228\n",
      "Valid loss: 1.7974, Valid acc: 0.2009\n",
      "Epoch 3:\n",
      "Train loss: 1.7775, Train acc: 0.2244\n",
      "Valid loss: 1.7928, Valid acc: 0.2018\n",
      "Epoch 4:\n",
      "Train loss: 1.7733, Train acc: 0.2231\n",
      "Valid loss: 1.7885, Valid acc: 0.2018\n",
      "Epoch 5:\n",
      "Train loss: 1.7702, Train acc: 0.2233\n",
      "Valid loss: 1.7843, Valid acc: 0.2018\n",
      "Epoch 6:\n",
      "Train loss: 1.7671, Train acc: 0.2244\n",
      "Valid loss: 1.7802, Valid acc: 0.2009\n",
      "Epoch 7:\n",
      "Train loss: 1.7627, Train acc: 0.2219\n",
      "Valid loss: 1.7763, Valid acc: 0.2018\n",
      "Epoch 8:\n",
      "Train loss: 1.7589, Train acc: 0.2235\n",
      "Valid loss: 1.7724, Valid acc: 0.2028\n",
      "Epoch 9:\n",
      "Train loss: 1.7563, Train acc: 0.2251\n",
      "Valid loss: 1.7687, Valid acc: 0.2028\n",
      "Epoch 10:\n",
      "Train loss: 1.7522, Train acc: 0.2267\n",
      "Valid loss: 1.7651, Valid acc: 0.2028\n",
      "Epoch 11:\n",
      "Train loss: 1.7484, Train acc: 0.2240\n",
      "Valid loss: 1.7616, Valid acc: 0.2028\n",
      "Epoch 12:\n",
      "Train loss: 1.7467, Train acc: 0.2233\n",
      "Valid loss: 1.7581, Valid acc: 0.2028\n",
      "Epoch 13:\n",
      "Train loss: 1.7436, Train acc: 0.2240\n",
      "Valid loss: 1.7548, Valid acc: 0.2028\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7824, Train acc: 0.2107\n",
      "Valid loss: 1.7683, Valid acc: 0.2413\n",
      "Epoch 2:\n",
      "Train loss: 1.7781, Train acc: 0.2169\n",
      "Valid loss: 1.7643, Valid acc: 0.2394\n",
      "Epoch 3:\n",
      "Train loss: 1.7745, Train acc: 0.2160\n",
      "Valid loss: 1.7605, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.7701, Train acc: 0.2137\n",
      "Valid loss: 1.7568, Valid acc: 0.2266\n",
      "Epoch 5:\n",
      "Train loss: 1.7666, Train acc: 0.2205\n",
      "Valid loss: 1.7532, Valid acc: 0.2358\n",
      "Epoch 6:\n",
      "Train loss: 1.7640, Train acc: 0.2254\n",
      "Valid loss: 1.7498, Valid acc: 0.2376\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7902, Train acc: 0.1671\n",
      "Valid loss: 1.7883, Valid acc: 0.1523\n",
      "Epoch 2:\n",
      "Train loss: 1.7862, Train acc: 0.1786\n",
      "Valid loss: 1.7840, Valid acc: 0.2450\n",
      "Epoch 3:\n",
      "Train loss: 1.7821, Train acc: 0.2130\n",
      "Valid loss: 1.7799, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.7781, Train acc: 0.2153\n",
      "Valid loss: 1.7759, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.7747, Train acc: 0.2162\n",
      "Valid loss: 1.7721, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.7709, Train acc: 0.2194\n",
      "Valid loss: 1.7684, Valid acc: 0.2440\n",
      "Epoch 7:\n",
      "Train loss: 1.7678, Train acc: 0.2272\n",
      "Valid loss: 1.7650, Valid acc: 0.2477\n",
      "Epoch 8:\n",
      "Train loss: 1.7646, Train acc: 0.2336\n",
      "Valid loss: 1.7616, Valid acc: 0.2083\n",
      "Epoch 9:\n",
      "Train loss: 1.7613, Train acc: 0.2244\n",
      "Valid loss: 1.7583, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7933, Train acc: 0.2075\n",
      "Valid loss: 1.7906, Valid acc: 0.2349\n",
      "Epoch 2:\n",
      "Train loss: 1.7919, Train acc: 0.2075\n",
      "Valid loss: 1.7885, Valid acc: 0.2349\n",
      "Epoch 3:\n",
      "Train loss: 1.7875, Train acc: 0.2075\n",
      "Valid loss: 1.7865, Valid acc: 0.2349\n",
      "Epoch 4:\n",
      "Train loss: 1.7870, Train acc: 0.2077\n",
      "Valid loss: 1.7846, Valid acc: 0.2349\n",
      "Epoch 5:\n",
      "Train loss: 1.7850, Train acc: 0.2077\n",
      "Valid loss: 1.7827, Valid acc: 0.2349\n",
      "Epoch 6:\n",
      "Train loss: 1.7817, Train acc: 0.2077\n",
      "Valid loss: 1.7808, Valid acc: 0.2349\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7726, Train acc: 0.1710\n",
      "Valid loss: 1.7701, Valid acc: 0.1440\n",
      "Epoch 2:\n",
      "Train loss: 1.7686, Train acc: 0.1715\n",
      "Valid loss: 1.7683, Valid acc: 0.1440\n",
      "Epoch 3:\n",
      "Train loss: 1.7673, Train acc: 0.1706\n",
      "Valid loss: 1.7665, Valid acc: 0.1440\n",
      "Epoch 4:\n",
      "Train loss: 1.7659, Train acc: 0.1710\n",
      "Valid loss: 1.7648, Valid acc: 0.1431\n",
      "Epoch 5:\n",
      "Train loss: 1.7646, Train acc: 0.1804\n",
      "Valid loss: 1.7631, Valid acc: 0.2321\n",
      "Epoch 6:\n",
      "Train loss: 1.7632, Train acc: 0.2116\n",
      "Valid loss: 1.7613, Valid acc: 0.2321\n",
      "Epoch 7:\n",
      "Train loss: 1.7594, Train acc: 0.2111\n",
      "Valid loss: 1.7596, Valid acc: 0.2330\n",
      "Epoch 8:\n",
      "Train loss: 1.7596, Train acc: 0.2091\n",
      "Valid loss: 1.7580, Valid acc: 0.2330\n",
      "Epoch 9:\n",
      "Train loss: 1.7569, Train acc: 0.2100\n",
      "Valid loss: 1.7564, Valid acc: 0.2339\n",
      "Epoch 10:\n",
      "Train loss: 1.7555, Train acc: 0.2116\n",
      "Valid loss: 1.7549, Valid acc: 0.2339\n",
      "Epoch 11:\n",
      "Train loss: 1.7539, Train acc: 0.2098\n",
      "Valid loss: 1.7533, Valid acc: 0.2339\n",
      "Epoch 12:\n",
      "Train loss: 1.7526, Train acc: 0.2100\n",
      "Valid loss: 1.7518, Valid acc: 0.2339\n",
      "Epoch 13:\n",
      "Train loss: 1.7513, Train acc: 0.2093\n",
      "Valid loss: 1.7502, Valid acc: 0.2339\n",
      "Epoch 14:\n",
      "Train loss: 1.7487, Train acc: 0.2116\n",
      "Valid loss: 1.7487, Valid acc: 0.2312\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.8064, Train acc: 0.0167\n",
      "Valid loss: 1.8030, Valid acc: 0.0193\n",
      "Epoch 2:\n",
      "Train loss: 1.8038, Train acc: 0.0170\n",
      "Valid loss: 1.8007, Valid acc: 0.0193\n",
      "Epoch 3:\n",
      "Train loss: 1.8023, Train acc: 0.0165\n",
      "Valid loss: 1.7985, Valid acc: 0.0193\n",
      "Epoch 4:\n",
      "Train loss: 1.7991, Train acc: 0.0177\n",
      "Valid loss: 1.7962, Valid acc: 0.2229\n",
      "Epoch 5:\n",
      "Train loss: 1.7975, Train acc: 0.2029\n",
      "Valid loss: 1.7941, Valid acc: 0.2339\n",
      "Epoch 6:\n",
      "Train loss: 1.7952, Train acc: 0.2056\n",
      "Valid loss: 1.7919, Valid acc: 0.2349\n",
      "Epoch 7:\n",
      "Train loss: 1.7925, Train acc: 0.2075\n",
      "Valid loss: 1.7898, Valid acc: 0.2349\n",
      "Epoch 8:\n",
      "Train loss: 1.7910, Train acc: 0.2070\n",
      "Valid loss: 1.7878, Valid acc: 0.2339\n",
      "Epoch 9:\n",
      "Train loss: 1.7885, Train acc: 0.2082\n",
      "Valid loss: 1.7858, Valid acc: 0.2349\n",
      "Epoch 10:\n",
      "Train loss: 1.7860, Train acc: 0.2082\n",
      "Valid loss: 1.7838, Valid acc: 0.2367\n",
      "Epoch 11:\n",
      "Train loss: 1.7846, Train acc: 0.2084\n",
      "Valid loss: 1.7818, Valid acc: 0.2367\n",
      "Epoch 12:\n",
      "Train loss: 1.7831, Train acc: 0.2109\n",
      "Valid loss: 1.7800, Valid acc: 0.2376\n",
      "Epoch 13:\n",
      "Train loss: 1.7816, Train acc: 0.2105\n",
      "Valid loss: 1.7781, Valid acc: 0.2376\n",
      "Epoch 14:\n",
      "Train loss: 1.7791, Train acc: 0.2091\n",
      "Valid loss: 1.7763, Valid acc: 0.2376\n",
      "Epoch 15:\n",
      "Train loss: 1.7765, Train acc: 0.2107\n",
      "Valid loss: 1.7744, Valid acc: 0.2358\n",
      "Epoch 16:\n",
      "Train loss: 1.7750, Train acc: 0.2127\n",
      "Valid loss: 1.7726, Valid acc: 0.2376\n",
      "Epoch 17:\n",
      "Train loss: 1.7744, Train acc: 0.2141\n",
      "Valid loss: 1.7709, Valid acc: 0.2394\n",
      "Epoch 18:\n",
      "Train loss: 1.7720, Train acc: 0.2164\n",
      "Valid loss: 1.7692, Valid acc: 0.2404\n",
      "Epoch 19:\n",
      "Train loss: 1.7697, Train acc: 0.2238\n",
      "Valid loss: 1.7674, Valid acc: 0.2431\n",
      "Epoch 20:\n",
      "Train loss: 1.7683, Train acc: 0.2208\n",
      "Valid loss: 1.7657, Valid acc: 0.2431\n",
      "Epoch 21:\n",
      "Train loss: 1.7658, Train acc: 0.2199\n",
      "Valid loss: 1.7640, Valid acc: 0.2431\n",
      "Epoch 22:\n",
      "Train loss: 1.7659, Train acc: 0.2224\n",
      "Valid loss: 1.7624, Valid acc: 0.2431\n",
      "Epoch 23:\n",
      "Train loss: 1.7635, Train acc: 0.2228\n",
      "Valid loss: 1.7608, Valid acc: 0.2431\n",
      "Epoch 24:\n",
      "Train loss: 1.7610, Train acc: 0.2247\n",
      "Valid loss: 1.7591, Valid acc: 0.2431\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7409, Train acc: 0.2299\n",
      "Valid loss: 1.7094, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6867, Train acc: 0.2354\n",
      "Valid loss: 1.6724, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6632, Train acc: 0.2345\n",
      "Valid loss: 1.6580, Valid acc: 0.2450\n",
      "Epoch 4:\n",
      "Train loss: 1.6560, Train acc: 0.2281\n",
      "Valid loss: 1.6529, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6520, Train acc: 0.2327\n",
      "Valid loss: 1.6506, Valid acc: 0.2450\n",
      "Epoch 6:\n",
      "Train loss: 1.6493, Train acc: 0.2325\n",
      "Valid loss: 1.6492, Valid acc: 0.2450\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6949, Train acc: 0.2201\n",
      "Valid loss: 1.6496, Valid acc: 0.2495\n",
      "Epoch 2:\n",
      "Train loss: 1.6516, Train acc: 0.2357\n",
      "Valid loss: 1.6458, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6486, Train acc: 0.2249\n",
      "Valid loss: 1.6460, Valid acc: 0.2505\n",
      "Epoch 4:\n",
      "Train loss: 1.6461, Train acc: 0.2256\n",
      "Valid loss: 1.6474, Valid acc: 0.2110\n",
      "Epoch 5:\n",
      "Train loss: 1.6449, Train acc: 0.2247\n",
      "Valid loss: 1.6462, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6440, Train acc: 0.2309\n",
      "Valid loss: 1.6470, Valid acc: 0.2413\n",
      "Epoch 7:\n",
      "Train loss: 1.6451, Train acc: 0.2274\n",
      "Valid loss: 1.6451, Valid acc: 0.2440\n",
      "Epoch 8:\n",
      "Train loss: 1.6436, Train acc: 0.2274\n",
      "Valid loss: 1.6451, Valid acc: 0.2450\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6634, Train acc: 0.2260\n",
      "Valid loss: 1.6471, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6484, Train acc: 0.2325\n",
      "Valid loss: 1.6529, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6478, Train acc: 0.2233\n",
      "Valid loss: 1.6473, Valid acc: 0.2367\n",
      "Epoch 4:\n",
      "Train loss: 1.6440, Train acc: 0.2345\n",
      "Valid loss: 1.6461, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6432, Train acc: 0.2425\n",
      "Valid loss: 1.6461, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6418, Train acc: 0.2341\n",
      "Valid loss: 1.6500, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6426, Train acc: 0.2334\n",
      "Valid loss: 1.6439, Valid acc: 0.2321\n",
      "Epoch 8:\n",
      "Train loss: 1.6414, Train acc: 0.2345\n",
      "Valid loss: 1.6451, Valid acc: 0.2312\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7157, Train acc: 0.2283\n",
      "Valid loss: 1.6867, Valid acc: 0.2404\n",
      "Epoch 2:\n",
      "Train loss: 1.6789, Train acc: 0.2322\n",
      "Valid loss: 1.6616, Valid acc: 0.2422\n",
      "Epoch 3:\n",
      "Train loss: 1.6634, Train acc: 0.2306\n",
      "Valid loss: 1.6525, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.6563, Train acc: 0.2315\n",
      "Valid loss: 1.6489, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6571, Train acc: 0.2343\n",
      "Valid loss: 1.6461, Valid acc: 0.2422\n",
      "Epoch 6:\n",
      "Train loss: 1.6555, Train acc: 0.2311\n",
      "Valid loss: 1.6451, Valid acc: 0.2404\n",
      "Epoch 7:\n",
      "Train loss: 1.6503, Train acc: 0.2354\n",
      "Valid loss: 1.6445, Valid acc: 0.2404\n",
      "Epoch 8:\n",
      "Train loss: 1.6506, Train acc: 0.2325\n",
      "Valid loss: 1.6437, Valid acc: 0.2404\n",
      "Epoch 9:\n",
      "Train loss: 1.6497, Train acc: 0.2327\n",
      "Valid loss: 1.6427, Valid acc: 0.2404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6994, Train acc: 0.2233\n",
      "Valid loss: 1.6593, Valid acc: 0.2477\n",
      "Epoch 2:\n",
      "Train loss: 1.6543, Train acc: 0.2240\n",
      "Valid loss: 1.6514, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6494, Train acc: 0.2263\n",
      "Valid loss: 1.6500, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.6491, Train acc: 0.2306\n",
      "Valid loss: 1.6496, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6483, Train acc: 0.2318\n",
      "Valid loss: 1.6505, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6456, Train acc: 0.2215\n",
      "Valid loss: 1.6506, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6702, Train acc: 0.2258\n",
      "Valid loss: 1.6420, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6528, Train acc: 0.2295\n",
      "Valid loss: 1.6458, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6479, Train acc: 0.2338\n",
      "Valid loss: 1.6399, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6492, Train acc: 0.2244\n",
      "Valid loss: 1.6445, Valid acc: 0.2394\n",
      "Epoch 5:\n",
      "Train loss: 1.6447, Train acc: 0.2210\n",
      "Valid loss: 1.6419, Valid acc: 0.2440\n",
      "Epoch 6:\n",
      "Train loss: 1.6450, Train acc: 0.2260\n",
      "Valid loss: 1.6403, Valid acc: 0.2596\n",
      "Epoch 7:\n",
      "Train loss: 1.6484, Train acc: 0.2320\n",
      "Valid loss: 1.6438, Valid acc: 0.2339\n",
      "Epoch 8:\n",
      "Train loss: 1.6433, Train acc: 0.2288\n",
      "Valid loss: 1.6397, Valid acc: 0.2440\n",
      "Epoch 9:\n",
      "Train loss: 1.6525, Train acc: 0.2293\n",
      "Valid loss: 1.6441, Valid acc: 0.2376\n",
      "Epoch 10:\n",
      "Train loss: 1.6418, Train acc: 0.2199\n",
      "Valid loss: 1.6435, Valid acc: 0.2046\n",
      "Epoch 11:\n",
      "Train loss: 1.6443, Train acc: 0.2274\n",
      "Valid loss: 1.6387, Valid acc: 0.2404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7589, Train acc: 0.2247\n",
      "Valid loss: 1.7460, Valid acc: 0.2028\n",
      "Epoch 2:\n",
      "Train loss: 1.7341, Train acc: 0.2297\n",
      "Valid loss: 1.7267, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.7139, Train acc: 0.2288\n",
      "Valid loss: 1.7094, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.6984, Train acc: 0.2290\n",
      "Valid loss: 1.6934, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.6875, Train acc: 0.2279\n",
      "Valid loss: 1.6810, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6761, Train acc: 0.2288\n",
      "Valid loss: 1.6727, Valid acc: 0.2037\n",
      "Epoch 7:\n",
      "Train loss: 1.6660, Train acc: 0.2302\n",
      "Valid loss: 1.6659, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6597, Train acc: 0.2290\n",
      "Valid loss: 1.6613, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.6565, Train acc: 0.2283\n",
      "Valid loss: 1.6584, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6563, Train acc: 0.2290\n",
      "Valid loss: 1.6563, Valid acc: 0.2055\n",
      "Epoch 11:\n",
      "Train loss: 1.6532, Train acc: 0.2274\n",
      "Valid loss: 1.6538, Valid acc: 0.2541\n",
      "Epoch 12:\n",
      "Train loss: 1.6527, Train acc: 0.2260\n",
      "Valid loss: 1.6534, Valid acc: 0.2046\n",
      "Epoch 13:\n",
      "Train loss: 1.6525, Train acc: 0.2281\n",
      "Valid loss: 1.6537, Valid acc: 0.2046\n",
      "Epoch 14:\n",
      "Train loss: 1.6514, Train acc: 0.2288\n",
      "Valid loss: 1.6531, Valid acc: 0.2046\n",
      "Epoch 15:\n",
      "Train loss: 1.6516, Train acc: 0.2290\n",
      "Valid loss: 1.6525, Valid acc: 0.2560\n",
      "Epoch 16:\n",
      "Train loss: 1.6483, Train acc: 0.2215\n",
      "Valid loss: 1.6526, Valid acc: 0.2560\n",
      "Epoch 17:\n",
      "Train loss: 1.6531, Train acc: 0.2176\n",
      "Valid loss: 1.6529, Valid acc: 0.2046\n",
      "Epoch 18:\n",
      "Train loss: 1.6513, Train acc: 0.2283\n",
      "Valid loss: 1.6529, Valid acc: 0.2046\n",
      "Epoch 19:\n",
      "Train loss: 1.6475, Train acc: 0.2288\n",
      "Valid loss: 1.6524, Valid acc: 0.2046\n",
      "Epoch 20:\n",
      "Train loss: 1.6456, Train acc: 0.2293\n",
      "Valid loss: 1.6523, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7301, Train acc: 0.2199\n",
      "Valid loss: 1.6773, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6609, Train acc: 0.2288\n",
      "Valid loss: 1.6547, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6546, Train acc: 0.2178\n",
      "Valid loss: 1.6559, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6535, Train acc: 0.2276\n",
      "Valid loss: 1.6522, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6496, Train acc: 0.2254\n",
      "Valid loss: 1.6510, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6495, Train acc: 0.2297\n",
      "Valid loss: 1.6526, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6521, Train acc: 0.2272\n",
      "Valid loss: 1.6517, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6453, Train acc: 0.2263\n",
      "Valid loss: 1.6514, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.6608, Train acc: 0.2293\n",
      "Valid loss: 1.6511, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6489, Train acc: 0.2258\n",
      "Valid loss: 1.6504, Valid acc: 0.2477\n",
      "Epoch 11:\n",
      "Train loss: 1.6526, Train acc: 0.2212\n",
      "Valid loss: 1.6513, Valid acc: 0.2477\n",
      "Epoch 12:\n",
      "Train loss: 1.6578, Train acc: 0.2304\n",
      "Valid loss: 1.6518, Valid acc: 0.2468\n",
      "Epoch 13:\n",
      "Train loss: 1.6503, Train acc: 0.2299\n",
      "Valid loss: 1.6513, Valid acc: 0.2486\n",
      "Epoch 14:\n",
      "Train loss: 1.6504, Train acc: 0.2293\n",
      "Valid loss: 1.6528, Valid acc: 0.2046\n",
      "Epoch 15:\n",
      "Train loss: 1.6505, Train acc: 0.2293\n",
      "Valid loss: 1.6524, Valid acc: 0.2486\n",
      "Epoch 16:\n",
      "Train loss: 1.6502, Train acc: 0.2210\n",
      "Valid loss: 1.6521, Valid acc: 0.2495\n",
      "Epoch 17:\n",
      "Train loss: 1.6486, Train acc: 0.2286\n",
      "Valid loss: 1.6520, Valid acc: 0.2046\n",
      "Epoch 18:\n",
      "Train loss: 1.6477, Train acc: 0.2288\n",
      "Valid loss: 1.6522, Valid acc: 0.2046\n",
      "Epoch 19:\n",
      "Train loss: 1.6460, Train acc: 0.2293\n",
      "Valid loss: 1.6527, Valid acc: 0.2046\n",
      "Epoch 20:\n",
      "Train loss: 1.6501, Train acc: 0.2293\n",
      "Valid loss: 1.6523, Valid acc: 0.2046\n",
      "Epoch 21:\n",
      "Train loss: 1.6478, Train acc: 0.2288\n",
      "Valid loss: 1.6533, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6855, Train acc: 0.2205\n",
      "Valid loss: 1.6533, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6492, Train acc: 0.2233\n",
      "Valid loss: 1.6531, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6493, Train acc: 0.2336\n",
      "Valid loss: 1.6569, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6516, Train acc: 0.2304\n",
      "Valid loss: 1.6499, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6572, Train acc: 0.2260\n",
      "Valid loss: 1.6517, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.6451, Train acc: 0.2244\n",
      "Valid loss: 1.6492, Valid acc: 0.2468\n",
      "Epoch 7:\n",
      "Train loss: 1.6490, Train acc: 0.2242\n",
      "Valid loss: 1.6488, Valid acc: 0.2495\n",
      "Epoch 8:\n",
      "Train loss: 1.6488, Train acc: 0.2242\n",
      "Valid loss: 1.6530, Valid acc: 0.2055\n",
      "Epoch 9:\n",
      "Train loss: 1.6503, Train acc: 0.2293\n",
      "Valid loss: 1.6497, Valid acc: 0.2468\n",
      "Epoch 10:\n",
      "Train loss: 1.6494, Train acc: 0.2281\n",
      "Valid loss: 1.6522, Valid acc: 0.2055\n",
      "Epoch 11:\n",
      "Train loss: 1.6575, Train acc: 0.2256\n",
      "Valid loss: 1.6529, Valid acc: 0.2046\n",
      "Epoch 12:\n",
      "Train loss: 1.6526, Train acc: 0.2293\n",
      "Valid loss: 1.6496, Valid acc: 0.2431\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6762, Train acc: 0.2251\n",
      "Valid loss: 1.6438, Valid acc: 0.2495\n",
      "Epoch 2:\n",
      "Train loss: 1.6467, Train acc: 0.2286\n",
      "Valid loss: 1.6543, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.6374, Train acc: 0.2375\n",
      "Valid loss: 1.5316, Valid acc: 0.3697\n",
      "Epoch 4:\n",
      "Train loss: 1.4974, Train acc: 0.3345\n",
      "Valid loss: 1.4645, Valid acc: 0.3459\n",
      "Epoch 5:\n",
      "Train loss: 1.4345, Train acc: 0.3712\n",
      "Valid loss: 1.4405, Valid acc: 0.3798\n",
      "Epoch 6:\n",
      "Train loss: 1.4216, Train acc: 0.3663\n",
      "Valid loss: 1.4116, Valid acc: 0.4018\n",
      "Epoch 7:\n",
      "Train loss: 1.4636, Train acc: 0.3735\n",
      "Valid loss: 1.4009, Valid acc: 0.3917\n",
      "Epoch 8:\n",
      "Train loss: 1.3879, Train acc: 0.3906\n",
      "Valid loss: 1.4113, Valid acc: 0.3963\n",
      "Epoch 9:\n",
      "Train loss: 1.3892, Train acc: 0.4014\n",
      "Valid loss: 1.3889, Valid acc: 0.4028\n",
      "Epoch 10:\n",
      "Train loss: 1.4183, Train acc: 0.3785\n",
      "Valid loss: 1.4297, Valid acc: 0.3578\n",
      "Epoch 11:\n",
      "Train loss: 1.4305, Train acc: 0.3618\n",
      "Valid loss: 1.4282, Valid acc: 0.3835\n",
      "Epoch 12:\n",
      "Train loss: 1.4279, Train acc: 0.3680\n",
      "Valid loss: 1.4273, Valid acc: 0.3835\n",
      "Epoch 13:\n",
      "Train loss: 1.4272, Train acc: 0.3608\n",
      "Valid loss: 1.4200, Valid acc: 0.3826\n",
      "Epoch 14:\n",
      "Train loss: 1.4241, Train acc: 0.3636\n",
      "Valid loss: 1.4324, Valid acc: 0.3826\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6664, Train acc: 0.2219\n",
      "Valid loss: 1.6462, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6505, Train acc: 0.2224\n",
      "Valid loss: 1.6484, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6350, Train acc: 0.2405\n",
      "Valid loss: 1.5183, Valid acc: 0.3046\n",
      "Epoch 4:\n",
      "Train loss: 1.6299, Train acc: 0.2492\n",
      "Valid loss: 1.6375, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.6447, Train acc: 0.2265\n",
      "Valid loss: 1.6089, Valid acc: 0.2569\n",
      "Epoch 6:\n",
      "Train loss: 1.5215, Train acc: 0.3391\n",
      "Valid loss: 1.4916, Valid acc: 0.3165\n",
      "Epoch 7:\n",
      "Train loss: 1.5153, Train acc: 0.3398\n",
      "Valid loss: 1.5253, Valid acc: 0.3303\n",
      "Epoch 8:\n",
      "Train loss: 1.4993, Train acc: 0.3223\n",
      "Valid loss: 1.5012, Valid acc: 0.3303\n",
      "Epoch 9:\n",
      "Train loss: 1.4852, Train acc: 0.3372\n",
      "Valid loss: 1.4992, Valid acc: 0.3413\n",
      "Epoch 10:\n",
      "Train loss: 1.4634, Train acc: 0.3556\n",
      "Valid loss: 1.4602, Valid acc: 0.3633\n",
      "Epoch 11:\n",
      "Train loss: 1.4056, Train acc: 0.3769\n",
      "Valid loss: 1.4027, Valid acc: 0.4000\n",
      "Epoch 12:\n",
      "Train loss: 1.3639, Train acc: 0.3975\n",
      "Valid loss: 1.4515, Valid acc: 0.3936\n",
      "Epoch 13:\n",
      "Train loss: 1.4807, Train acc: 0.3567\n",
      "Valid loss: 1.4881, Valid acc: 0.3440\n",
      "Epoch 14:\n",
      "Train loss: 1.4799, Train acc: 0.3370\n",
      "Valid loss: 1.5004, Valid acc: 0.3450\n",
      "Epoch 15:\n",
      "Train loss: 1.4738, Train acc: 0.3297\n",
      "Valid loss: 1.4729, Valid acc: 0.3450\n",
      "Epoch 16:\n",
      "Train loss: 1.4644, Train acc: 0.3356\n",
      "Valid loss: 1.4811, Valid acc: 0.3422\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6702, Train acc: 0.2244\n",
      "Valid loss: 1.6506, Valid acc: 0.2101\n",
      "Epoch 2:\n",
      "Train loss: 1.6532, Train acc: 0.2306\n",
      "Valid loss: 1.6315, Valid acc: 0.2532\n",
      "Epoch 3:\n",
      "Train loss: 1.6483, Train acc: 0.2336\n",
      "Valid loss: 1.6397, Valid acc: 0.2385\n",
      "Epoch 4:\n",
      "Train loss: 1.6339, Train acc: 0.2503\n",
      "Valid loss: 1.5374, Valid acc: 0.3661\n",
      "Epoch 5:\n",
      "Train loss: 1.6524, Train acc: 0.2338\n",
      "Valid loss: 1.6489, Valid acc: 0.2358\n",
      "Epoch 6:\n",
      "Train loss: 1.6418, Train acc: 0.2336\n",
      "Valid loss: 1.6349, Valid acc: 0.2422\n",
      "Epoch 7:\n",
      "Train loss: 1.5930, Train acc: 0.2884\n",
      "Valid loss: 1.6170, Valid acc: 0.2908\n",
      "Epoch 8:\n",
      "Train loss: 1.5405, Train acc: 0.2859\n",
      "Valid loss: 1.5600, Valid acc: 0.2477\n",
      "Epoch 9:\n",
      "Train loss: 1.6474, Train acc: 0.2249\n",
      "Valid loss: 1.6492, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6954, Train acc: 0.2331\n",
      "Valid loss: 1.6656, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6540, Train acc: 0.2350\n",
      "Valid loss: 1.6509, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6501, Train acc: 0.2293\n",
      "Valid loss: 1.6427, Valid acc: 0.2422\n",
      "Epoch 4:\n",
      "Train loss: 1.6442, Train acc: 0.2226\n",
      "Valid loss: 1.6335, Valid acc: 0.2532\n",
      "Epoch 5:\n",
      "Train loss: 1.6451, Train acc: 0.2244\n",
      "Valid loss: 1.6373, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.5998, Train acc: 0.2797\n",
      "Valid loss: 1.4545, Valid acc: 0.4000\n",
      "Epoch 7:\n",
      "Train loss: 1.4583, Train acc: 0.3670\n",
      "Valid loss: 1.4472, Valid acc: 0.3954\n",
      "Epoch 8:\n",
      "Train loss: 1.4254, Train acc: 0.3673\n",
      "Valid loss: 1.4172, Valid acc: 0.3945\n",
      "Epoch 9:\n",
      "Train loss: 1.4294, Train acc: 0.3927\n",
      "Valid loss: 1.4246, Valid acc: 0.4000\n",
      "Epoch 10:\n",
      "Train loss: 1.4049, Train acc: 0.3936\n",
      "Valid loss: 1.4855, Valid acc: 0.3862\n",
      "Epoch 11:\n",
      "Train loss: 1.4079, Train acc: 0.3936\n",
      "Valid loss: 1.4873, Valid acc: 0.3872\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6787, Train acc: 0.2189\n",
      "Valid loss: 1.6495, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.6495, Train acc: 0.2274\n",
      "Valid loss: 1.6557, Valid acc: 0.2394\n",
      "Epoch 3:\n",
      "Train loss: 1.6337, Train acc: 0.2588\n",
      "Valid loss: 1.5445, Valid acc: 0.3697\n",
      "Epoch 4:\n",
      "Train loss: 1.4704, Train acc: 0.3812\n",
      "Valid loss: 1.4545, Valid acc: 0.4092\n",
      "Epoch 5:\n",
      "Train loss: 1.4086, Train acc: 0.4003\n",
      "Valid loss: 1.4486, Valid acc: 0.3073\n",
      "Epoch 6:\n",
      "Train loss: 1.4230, Train acc: 0.3806\n",
      "Valid loss: 1.4595, Valid acc: 0.4119\n",
      "Epoch 7:\n",
      "Train loss: 1.3949, Train acc: 0.3900\n",
      "Valid loss: 1.3928, Valid acc: 0.3908\n",
      "Epoch 8:\n",
      "Train loss: 1.3964, Train acc: 0.3968\n",
      "Valid loss: 1.4414, Valid acc: 0.4073\n",
      "Epoch 9:\n",
      "Train loss: 1.4151, Train acc: 0.3751\n",
      "Valid loss: 1.4516, Valid acc: 0.3165\n",
      "Epoch 10:\n",
      "Train loss: 1.3962, Train acc: 0.3803\n",
      "Valid loss: 1.4431, Valid acc: 0.3972\n",
      "Epoch 11:\n",
      "Train loss: 1.4062, Train acc: 0.3909\n",
      "Valid loss: 1.4156, Valid acc: 0.4101\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6727, Train acc: 0.2251\n",
      "Valid loss: 1.6408, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6533, Train acc: 0.2164\n",
      "Valid loss: 1.6407, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6473, Train acc: 0.2437\n",
      "Valid loss: 1.6330, Valid acc: 0.2642\n",
      "Epoch 4:\n",
      "Train loss: 1.5883, Train acc: 0.2836\n",
      "Valid loss: 1.5031, Valid acc: 0.2725\n",
      "Epoch 5:\n",
      "Train loss: 1.5092, Train acc: 0.3287\n",
      "Valid loss: 1.4640, Valid acc: 0.3495\n",
      "Epoch 6:\n",
      "Train loss: 1.4743, Train acc: 0.3322\n",
      "Valid loss: 1.4743, Valid acc: 0.3459\n",
      "Epoch 7:\n",
      "Train loss: 1.4684, Train acc: 0.3407\n",
      "Valid loss: 1.5027, Valid acc: 0.2569\n",
      "Epoch 8:\n",
      "Train loss: 1.4780, Train acc: 0.3354\n",
      "Valid loss: 1.4724, Valid acc: 0.3495\n",
      "Epoch 9:\n",
      "Train loss: 1.4755, Train acc: 0.3372\n",
      "Valid loss: 1.4618, Valid acc: 0.3468\n",
      "Epoch 10:\n",
      "Train loss: 1.4673, Train acc: 0.3352\n",
      "Valid loss: 1.4604, Valid acc: 0.3486\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7189, Train acc: 0.2233\n",
      "Valid loss: 1.6599, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6597, Train acc: 0.2196\n",
      "Valid loss: 1.6489, Valid acc: 0.2376\n",
      "Epoch 3:\n",
      "Train loss: 1.6473, Train acc: 0.2166\n",
      "Valid loss: 1.6520, Valid acc: 0.2367\n",
      "Epoch 4:\n",
      "Train loss: 1.6541, Train acc: 0.2205\n",
      "Valid loss: 1.6506, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6536, Train acc: 0.2260\n",
      "Valid loss: 1.6544, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.6513, Train acc: 0.2231\n",
      "Valid loss: 1.6531, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6504, Train acc: 0.2176\n",
      "Valid loss: 1.6531, Valid acc: 0.2495\n",
      "Epoch 8:\n",
      "Train loss: 1.6499, Train acc: 0.2297\n",
      "Valid loss: 1.6516, Valid acc: 0.2422\n",
      "Epoch 9:\n",
      "Train loss: 1.6487, Train acc: 0.2125\n",
      "Valid loss: 1.6524, Valid acc: 0.2495\n",
      "Epoch 10:\n",
      "Train loss: 1.6574, Train acc: 0.2249\n",
      "Valid loss: 1.6503, Valid acc: 0.2486\n",
      "Epoch 11:\n",
      "Train loss: 1.6476, Train acc: 0.2221\n",
      "Valid loss: 1.6541, Valid acc: 0.2018\n",
      "Epoch 12:\n",
      "Train loss: 1.6500, Train acc: 0.2315\n",
      "Valid loss: 1.6537, Valid acc: 0.2468\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6863, Train acc: 0.2217\n",
      "Valid loss: 1.6592, Valid acc: 0.2385\n",
      "Epoch 2:\n",
      "Train loss: 1.6642, Train acc: 0.2251\n",
      "Valid loss: 1.6510, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6508, Train acc: 0.2384\n",
      "Valid loss: 1.6622, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6554, Train acc: 0.2221\n",
      "Valid loss: 1.6580, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6472, Train acc: 0.2221\n",
      "Valid loss: 1.6490, Valid acc: 0.2486\n",
      "Epoch 6:\n",
      "Train loss: 1.6513, Train acc: 0.2485\n",
      "Valid loss: 1.6394, Valid acc: 0.2789\n",
      "Epoch 7:\n",
      "Train loss: 1.5578, Train acc: 0.3306\n",
      "Valid loss: 1.4759, Valid acc: 0.3413\n",
      "Epoch 8:\n",
      "Train loss: 1.4686, Train acc: 0.3693\n",
      "Valid loss: 1.4722, Valid acc: 0.3954\n",
      "Epoch 9:\n",
      "Train loss: 1.6199, Train acc: 0.3067\n",
      "Valid loss: 1.5724, Valid acc: 0.2853\n",
      "Epoch 10:\n",
      "Train loss: 1.5586, Train acc: 0.3129\n",
      "Valid loss: 1.5610, Valid acc: 0.2853\n",
      "Epoch 11:\n",
      "Train loss: 1.5560, Train acc: 0.3083\n",
      "Valid loss: 1.5537, Valid acc: 0.3110\n",
      "Epoch 12:\n",
      "Train loss: 1.5435, Train acc: 0.3219\n",
      "Valid loss: 1.5623, Valid acc: 0.3119\n",
      "Epoch 13:\n",
      "Train loss: 1.5272, Train acc: 0.3237\n",
      "Valid loss: 1.5138, Valid acc: 0.3624\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6860, Train acc: 0.2164\n",
      "Valid loss: 1.6610, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6696, Train acc: 0.2160\n",
      "Valid loss: 1.6586, Valid acc: 0.2018\n",
      "Epoch 3:\n",
      "Train loss: 1.6551, Train acc: 0.2288\n",
      "Valid loss: 1.6601, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6509, Train acc: 0.2299\n",
      "Valid loss: 1.6755, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6500, Train acc: 0.2272\n",
      "Valid loss: 1.6562, Valid acc: 0.2018\n",
      "Epoch 6:\n",
      "Train loss: 1.6544, Train acc: 0.2182\n",
      "Valid loss: 1.6599, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6592, Train acc: 0.2180\n",
      "Valid loss: 1.6528, Valid acc: 0.2560\n",
      "Epoch 2:\n",
      "Train loss: 1.6491, Train acc: 0.2345\n",
      "Valid loss: 1.6447, Valid acc: 0.2450\n",
      "Epoch 3:\n",
      "Train loss: 1.6490, Train acc: 0.2320\n",
      "Valid loss: 1.6528, Valid acc: 0.2477\n",
      "Epoch 4:\n",
      "Train loss: 1.6435, Train acc: 0.2329\n",
      "Valid loss: 1.6415, Valid acc: 0.2789\n",
      "Epoch 5:\n",
      "Train loss: 1.4504, Train acc: 0.3890\n",
      "Valid loss: 1.3830, Valid acc: 0.4275\n",
      "Epoch 6:\n",
      "Train loss: 1.3127, Train acc: 0.4289\n",
      "Valid loss: 1.2835, Valid acc: 0.4596\n",
      "Epoch 7:\n",
      "Train loss: 1.1565, Train acc: 0.4972\n",
      "Valid loss: 1.3352, Valid acc: 0.4073\n",
      "Epoch 8:\n",
      "Train loss: 1.0515, Train acc: 0.5491\n",
      "Valid loss: 1.2254, Valid acc: 0.4716\n",
      "Epoch 9:\n",
      "Train loss: 0.9590, Train acc: 0.5763\n",
      "Valid loss: 1.2761, Valid acc: 0.5000\n",
      "Epoch 10:\n",
      "Train loss: 0.8870, Train acc: 0.5981\n",
      "Valid loss: 1.1164, Valid acc: 0.5394\n",
      "Epoch 11:\n",
      "Train loss: 0.8163, Train acc: 0.6506\n",
      "Valid loss: 1.1418, Valid acc: 0.5211\n",
      "Epoch 12:\n",
      "Train loss: 0.7070, Train acc: 0.7199\n",
      "Valid loss: 1.1247, Valid acc: 0.6110\n",
      "Epoch 13:\n",
      "Train loss: 0.6361, Train acc: 0.7439\n",
      "Valid loss: 1.2622, Valid acc: 0.5615\n",
      "Epoch 14:\n",
      "Train loss: 0.5669, Train acc: 0.7685\n",
      "Valid loss: 1.0458, Valid acc: 0.6459\n",
      "Epoch 15:\n",
      "Train loss: 0.6095, Train acc: 0.7632\n",
      "Valid loss: 1.0791, Valid acc: 0.6339\n",
      "Epoch 16:\n",
      "Train loss: 0.5492, Train acc: 0.7893\n",
      "Valid loss: 1.2565, Valid acc: 0.6202\n",
      "Epoch 17:\n",
      "Train loss: 0.5374, Train acc: 0.8127\n",
      "Valid loss: 1.0527, Valid acc: 0.6706\n",
      "Epoch 18:\n",
      "Train loss: 0.4484, Train acc: 0.8668\n",
      "Valid loss: 1.2240, Valid acc: 0.6771\n",
      "Epoch 19:\n",
      "Train loss: 0.6879, Train acc: 0.7646\n",
      "Valid loss: 1.1439, Valid acc: 0.6587\n",
      "Epoch 20:\n",
      "Train loss: 0.4148, Train acc: 0.8952\n",
      "Valid loss: 1.3668, Valid acc: 0.6541\n",
      "Epoch 21:\n",
      "Train loss: 0.2991, Train acc: 0.9319\n",
      "Valid loss: 1.1958, Valid acc: 0.6927\n",
      "Epoch 22:\n",
      "Train loss: 0.6261, Train acc: 0.8265\n",
      "Valid loss: 1.1693, Valid acc: 0.6890\n",
      "Epoch 23:\n",
      "Train loss: 0.4377, Train acc: 0.8867\n",
      "Valid loss: 1.1244, Valid acc: 0.7119\n",
      "Epoch 24:\n",
      "Train loss: 0.3621, Train acc: 0.9129\n",
      "Valid loss: 1.0335, Valid acc: 0.7339\n",
      "Epoch 25:\n",
      "Train loss: 0.2272, Train acc: 0.9535\n",
      "Valid loss: 1.1789, Valid acc: 0.7128\n",
      "Epoch 26:\n",
      "Train loss: 0.3371, Train acc: 0.9218\n",
      "Valid loss: 1.2372, Valid acc: 0.7266\n",
      "Epoch 27:\n",
      "Train loss: 0.2429, Train acc: 0.9482\n",
      "Valid loss: 1.1507, Valid acc: 0.7330\n",
      "Epoch 28:\n",
      "Train loss: 0.3964, Train acc: 0.9026\n",
      "Valid loss: 1.8292, Valid acc: 0.5706\n",
      "Epoch 29:\n",
      "Train loss: 0.3542, Train acc: 0.9175\n",
      "Valid loss: 1.1580, Valid acc: 0.7505\n",
      "Epoch 30:\n",
      "Train loss: 0.2927, Train acc: 0.9354\n",
      "Valid loss: 1.0876, Valid acc: 0.7431\n",
      "Epoch 31:\n",
      "Train loss: 0.2585, Train acc: 0.9420\n",
      "Valid loss: 1.2378, Valid acc: 0.7394\n",
      "Epoch 32:\n",
      "Train loss: 0.4224, Train acc: 0.8982\n",
      "Valid loss: 1.1646, Valid acc: 0.7431\n",
      "Epoch 33:\n",
      "Train loss: 0.2102, Train acc: 0.9576\n",
      "Valid loss: 1.7079, Valid acc: 0.6431\n",
      "Epoch 34:\n",
      "Train loss: 0.5934, Train acc: 0.8001\n",
      "Valid loss: 1.2422, Valid acc: 0.6073\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6612, Train acc: 0.2260\n",
      "Valid loss: 1.6487, Valid acc: 0.2495\n",
      "Epoch 2:\n",
      "Train loss: 1.6468, Train acc: 0.2430\n",
      "Valid loss: 1.6425, Valid acc: 0.2679\n",
      "Epoch 3:\n",
      "Train loss: 1.5245, Train acc: 0.3455\n",
      "Valid loss: 1.5244, Valid acc: 0.3110\n",
      "Epoch 4:\n",
      "Train loss: 1.5188, Train acc: 0.3446\n",
      "Valid loss: 1.4634, Valid acc: 0.3954\n",
      "Epoch 5:\n",
      "Train loss: 1.4712, Train acc: 0.3721\n",
      "Valid loss: 1.4319, Valid acc: 0.3881\n",
      "Epoch 6:\n",
      "Train loss: 1.4640, Train acc: 0.3712\n",
      "Valid loss: 1.4844, Valid acc: 0.3862\n",
      "Epoch 7:\n",
      "Train loss: 1.4680, Train acc: 0.3858\n",
      "Valid loss: 1.5330, Valid acc: 0.3505\n",
      "Epoch 8:\n",
      "Train loss: 1.4407, Train acc: 0.3927\n",
      "Valid loss: 1.4444, Valid acc: 0.3908\n",
      "Epoch 9:\n",
      "Train loss: 1.4294, Train acc: 0.3897\n",
      "Valid loss: 1.4648, Valid acc: 0.3908\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6700, Train acc: 0.2251\n",
      "Valid loss: 1.6617, Valid acc: 0.2422\n",
      "Epoch 2:\n",
      "Train loss: 1.6490, Train acc: 0.2258\n",
      "Valid loss: 1.6483, Valid acc: 0.2339\n",
      "Epoch 3:\n",
      "Train loss: 1.6477, Train acc: 0.2265\n",
      "Valid loss: 1.6619, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6178, Train acc: 0.2724\n",
      "Valid loss: 1.6031, Valid acc: 0.3138\n",
      "Epoch 5:\n",
      "Train loss: 1.6388, Train acc: 0.2620\n",
      "Valid loss: 1.6603, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6434, Train acc: 0.2499\n",
      "Valid loss: 1.6550, Valid acc: 0.2385\n",
      "Epoch 7:\n",
      "Train loss: 1.6460, Train acc: 0.2295\n",
      "Valid loss: 1.6425, Valid acc: 0.2486\n",
      "Epoch 8:\n",
      "Train loss: 1.6392, Train acc: 0.2405\n",
      "Valid loss: 1.6654, Valid acc: 0.2119\n",
      "Epoch 9:\n",
      "Train loss: 1.6364, Train acc: 0.2414\n",
      "Valid loss: 1.6962, Valid acc: 0.2028\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6625, Train acc: 0.2302\n",
      "Valid loss: 1.6520, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6519, Train acc: 0.2270\n",
      "Valid loss: 1.6441, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6480, Train acc: 0.2359\n",
      "Valid loss: 1.6560, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6477, Train acc: 0.2231\n",
      "Valid loss: 1.6391, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.6499, Train acc: 0.2267\n",
      "Valid loss: 1.6450, Valid acc: 0.2523\n",
      "Epoch 6:\n",
      "Train loss: 1.6480, Train acc: 0.2341\n",
      "Valid loss: 1.6430, Valid acc: 0.2541\n",
      "Epoch 7:\n",
      "Train loss: 1.5814, Train acc: 0.3042\n",
      "Valid loss: 1.5591, Valid acc: 0.3844\n",
      "Epoch 8:\n",
      "Train loss: 1.4201, Train acc: 0.3996\n",
      "Valid loss: 1.4115, Valid acc: 0.4092\n",
      "Epoch 9:\n",
      "Train loss: 1.3672, Train acc: 0.4046\n",
      "Valid loss: 1.4388, Valid acc: 0.3972\n",
      "Epoch 10:\n",
      "Train loss: 1.2797, Train acc: 0.4360\n",
      "Valid loss: 1.3622, Valid acc: 0.4174\n",
      "Epoch 11:\n",
      "Train loss: 1.1829, Train acc: 0.4537\n",
      "Valid loss: 1.3687, Valid acc: 0.3312\n",
      "Epoch 12:\n",
      "Train loss: 1.1310, Train acc: 0.4920\n",
      "Valid loss: 1.4129, Valid acc: 0.4349\n",
      "Epoch 13:\n",
      "Train loss: 1.0315, Train acc: 0.5440\n",
      "Valid loss: 1.2601, Valid acc: 0.3881\n",
      "Epoch 14:\n",
      "Train loss: 0.9114, Train acc: 0.6128\n",
      "Valid loss: 1.1868, Valid acc: 0.4789\n",
      "Epoch 15:\n",
      "Train loss: 0.9181, Train acc: 0.6268\n",
      "Valid loss: 1.1482, Valid acc: 0.5064\n",
      "Epoch 16:\n",
      "Train loss: 0.7516, Train acc: 0.7130\n",
      "Valid loss: 1.1012, Valid acc: 0.5798\n",
      "Epoch 17:\n",
      "Train loss: 0.6334, Train acc: 0.7838\n",
      "Valid loss: 1.0876, Valid acc: 0.6431\n",
      "Epoch 18:\n",
      "Train loss: 0.5379, Train acc: 0.8308\n",
      "Valid loss: 1.2046, Valid acc: 0.6174\n",
      "Epoch 19:\n",
      "Train loss: 0.4982, Train acc: 0.8544\n",
      "Valid loss: 1.0938, Valid acc: 0.6514\n",
      "Epoch 20:\n",
      "Train loss: 0.3949, Train acc: 0.9003\n",
      "Valid loss: 1.1140, Valid acc: 0.6706\n",
      "Epoch 21:\n",
      "Train loss: 0.3231, Train acc: 0.9182\n",
      "Valid loss: 1.0462, Valid acc: 0.7055\n",
      "Epoch 22:\n",
      "Train loss: 0.3134, Train acc: 0.9209\n",
      "Valid loss: 1.0556, Valid acc: 0.7128\n",
      "Epoch 23:\n",
      "Train loss: 0.3676, Train acc: 0.9127\n",
      "Valid loss: 1.1796, Valid acc: 0.6917\n",
      "Epoch 24:\n",
      "Train loss: 0.2401, Train acc: 0.9480\n",
      "Valid loss: 1.3502, Valid acc: 0.6587\n",
      "Epoch 25:\n",
      "Train loss: 0.3078, Train acc: 0.9227\n",
      "Valid loss: 1.1906, Valid acc: 0.6872\n",
      "Epoch 26:\n",
      "Train loss: 0.2234, Train acc: 0.9493\n",
      "Valid loss: 1.2354, Valid acc: 0.6807\n",
      "Epoch 27:\n",
      "Train loss: 0.2015, Train acc: 0.9541\n",
      "Valid loss: 1.2751, Valid acc: 0.6991\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6675, Train acc: 0.2088\n",
      "Valid loss: 1.6535, Valid acc: 0.2413\n",
      "Epoch 2:\n",
      "Train loss: 1.6523, Train acc: 0.2341\n",
      "Valid loss: 1.6719, Valid acc: 0.2055\n",
      "Epoch 3:\n",
      "Train loss: 1.6496, Train acc: 0.2249\n",
      "Valid loss: 1.6435, Valid acc: 0.2422\n",
      "Epoch 4:\n",
      "Train loss: 1.6449, Train acc: 0.2350\n",
      "Valid loss: 1.6792, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6119, Train acc: 0.2834\n",
      "Valid loss: 1.5696, Valid acc: 0.3468\n",
      "Epoch 6:\n",
      "Train loss: 1.6428, Train acc: 0.2444\n",
      "Valid loss: 1.6442, Valid acc: 0.2431\n",
      "Epoch 7:\n",
      "Train loss: 1.6535, Train acc: 0.2338\n",
      "Valid loss: 1.6459, Valid acc: 0.2440\n",
      "Epoch 8:\n",
      "Train loss: 1.6515, Train acc: 0.2247\n",
      "Valid loss: 1.6290, Valid acc: 0.2459\n",
      "Epoch 9:\n",
      "Train loss: 1.6514, Train acc: 0.2233\n",
      "Valid loss: 1.6589, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6467, Train acc: 0.2212\n",
      "Valid loss: 1.6400, Valid acc: 0.2450\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6678, Train acc: 0.2240\n",
      "Valid loss: 1.6616, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6555, Train acc: 0.2205\n",
      "Valid loss: 1.6397, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6526, Train acc: 0.2221\n",
      "Valid loss: 1.6654, Valid acc: 0.1514\n",
      "Epoch 4:\n",
      "Train loss: 1.6457, Train acc: 0.2221\n",
      "Valid loss: 1.6808, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6472, Train acc: 0.2315\n",
      "Valid loss: 1.6299, Valid acc: 0.1945\n",
      "Epoch 6:\n",
      "Train loss: 1.6590, Train acc: 0.2432\n",
      "Valid loss: 1.8640, Valid acc: 0.2404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6806, Train acc: 0.2166\n",
      "Valid loss: 1.6739, Valid acc: 0.2349\n",
      "Epoch 2:\n",
      "Train loss: 1.6500, Train acc: 0.2247\n",
      "Valid loss: 1.6568, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6475, Train acc: 0.2318\n",
      "Valid loss: 1.6533, Valid acc: 0.2385\n",
      "Epoch 4:\n",
      "Train loss: 1.6646, Train acc: 0.2304\n",
      "Valid loss: 1.6743, Valid acc: 0.2385\n",
      "Epoch 5:\n",
      "Train loss: 1.6501, Train acc: 0.2224\n",
      "Valid loss: 1.6572, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6575, Train acc: 0.2219\n",
      "Valid loss: 1.6628, Valid acc: 0.2385\n",
      "Epoch 7:\n",
      "Train loss: 1.6558, Train acc: 0.2235\n",
      "Valid loss: 1.6490, Valid acc: 0.2394\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6704, Train acc: 0.2150\n",
      "Valid loss: 1.6864, Valid acc: 0.1459\n",
      "Epoch 2:\n",
      "Train loss: 1.6620, Train acc: 0.2219\n",
      "Valid loss: 1.6882, Valid acc: 0.1495\n",
      "Epoch 3:\n",
      "Train loss: 1.6533, Train acc: 0.2238\n",
      "Valid loss: 1.6544, Valid acc: 0.2431\n",
      "Epoch 4:\n",
      "Train loss: 1.6515, Train acc: 0.2215\n",
      "Valid loss: 1.6539, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6523, Train acc: 0.2233\n",
      "Valid loss: 1.6703, Valid acc: 0.2358\n",
      "Epoch 6:\n",
      "Train loss: 1.6507, Train acc: 0.2210\n",
      "Valid loss: 1.6506, Valid acc: 0.2394\n",
      "Epoch 7:\n",
      "Train loss: 1.6436, Train acc: 0.2350\n",
      "Valid loss: 1.6554, Valid acc: 0.2257\n",
      "Epoch 8:\n",
      "Train loss: 1.6526, Train acc: 0.2432\n",
      "Valid loss: 1.6627, Valid acc: 0.2349\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.0005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6892, Train acc: 0.2208\n",
      "Valid loss: 1.6740, Valid acc: 0.2358\n",
      "Epoch 2:\n",
      "Train loss: 1.6538, Train acc: 0.2187\n",
      "Valid loss: 1.6985, Valid acc: 0.1468\n",
      "Epoch 3:\n",
      "Train loss: 1.6598, Train acc: 0.2242\n",
      "Valid loss: 1.6812, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.6543, Train acc: 0.2189\n",
      "Valid loss: 1.6577, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6504, Train acc: 0.2256\n",
      "Valid loss: 1.6672, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.6485, Train acc: 0.2318\n",
      "Valid loss: 1.6609, Valid acc: 0.2450\n",
      "Epoch 7:\n",
      "Train loss: 1.6474, Train acc: 0.2299\n",
      "Valid loss: 1.6603, Valid acc: 0.2266\n",
      "Epoch 8:\n",
      "Train loss: 1.6521, Train acc: 0.2162\n",
      "Valid loss: 1.6613, Valid acc: 0.2037\n",
      "Epoch 9:\n",
      "Train loss: 1.6446, Train acc: 0.2297\n",
      "Valid loss: 1.6694, Valid acc: 0.2422\n",
      "Epoch 10:\n",
      "Train loss: 1.6520, Train acc: 0.2281\n",
      "Valid loss: 1.6571, Valid acc: 0.1982\n",
      "Epoch 11:\n",
      "Train loss: 1.6445, Train acc: 0.2286\n",
      "Valid loss: 1.6521, Valid acc: 0.2459\n",
      "Epoch 12:\n",
      "Train loss: 1.6464, Train acc: 0.2164\n",
      "Valid loss: 1.6796, Valid acc: 0.2028\n",
      "Epoch 13:\n",
      "Train loss: 1.6453, Train acc: 0.2235\n",
      "Valid loss: 1.6509, Valid acc: 0.2495\n",
      "Epoch 14:\n",
      "Train loss: 1.6546, Train acc: 0.2258\n",
      "Valid loss: 1.6543, Valid acc: 0.2018\n",
      "Epoch 15:\n",
      "Train loss: 1.6398, Train acc: 0.2354\n",
      "Valid loss: 1.6930, Valid acc: 0.2037\n",
      "Epoch 16:\n",
      "Train loss: 1.6382, Train acc: 0.2276\n",
      "Valid loss: 1.7838, Valid acc: 0.2046\n",
      "Epoch 17:\n",
      "Train loss: 1.6371, Train acc: 0.2705\n",
      "Valid loss: 1.7067, Valid acc: 0.2037\n",
      "Epoch 18:\n",
      "Train loss: 1.6580, Train acc: 0.2380\n",
      "Valid loss: 1.6691, Valid acc: 0.2110\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7894, Train acc: 0.2221\n",
      "Valid loss: 1.7686, Valid acc: 0.2486\n",
      "Epoch 2:\n",
      "Train loss: 1.7717, Train acc: 0.2279\n",
      "Valid loss: 1.7541, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.7592, Train acc: 0.2210\n",
      "Valid loss: 1.7418, Valid acc: 0.2514\n",
      "Epoch 4:\n",
      "Train loss: 1.7457, Train acc: 0.2249\n",
      "Valid loss: 1.7312, Valid acc: 0.2514\n",
      "Epoch 5:\n",
      "Train loss: 1.7359, Train acc: 0.2201\n",
      "Valid loss: 1.7219, Valid acc: 0.2505\n",
      "Epoch 6:\n",
      "Train loss: 1.7265, Train acc: 0.2217\n",
      "Valid loss: 1.7137, Valid acc: 0.2505\n",
      "Epoch 7:\n",
      "Train loss: 1.7170, Train acc: 0.2256\n",
      "Valid loss: 1.7064, Valid acc: 0.2514\n",
      "Epoch 8:\n",
      "Train loss: 1.7103, Train acc: 0.2286\n",
      "Valid loss: 1.7000, Valid acc: 0.2560\n",
      "Epoch 9:\n",
      "Train loss: 1.7031, Train acc: 0.2270\n",
      "Valid loss: 1.6943, Valid acc: 0.2606\n",
      "Epoch 10:\n",
      "Train loss: 1.6965, Train acc: 0.2377\n",
      "Valid loss: 1.6893, Valid acc: 0.2431\n",
      "Epoch 11:\n",
      "Train loss: 1.6920, Train acc: 0.2283\n",
      "Valid loss: 1.6849, Valid acc: 0.2413\n",
      "Epoch 12:\n",
      "Train loss: 1.6874, Train acc: 0.2329\n",
      "Valid loss: 1.6809, Valid acc: 0.2431\n",
      "Epoch 13:\n",
      "Train loss: 1.6818, Train acc: 0.2343\n",
      "Valid loss: 1.6775, Valid acc: 0.2450\n",
      "Epoch 14:\n",
      "Train loss: 1.6796, Train acc: 0.2336\n",
      "Valid loss: 1.6745, Valid acc: 0.2404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.8000, Train acc: 0.1261\n",
      "Valid loss: 1.7880, Valid acc: 0.1945\n",
      "Epoch 2:\n",
      "Train loss: 1.7803, Train acc: 0.2201\n",
      "Valid loss: 1.7691, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.7634, Train acc: 0.2144\n",
      "Valid loss: 1.7530, Valid acc: 0.2404\n",
      "Epoch 4:\n",
      "Train loss: 1.7498, Train acc: 0.2155\n",
      "Valid loss: 1.7388, Valid acc: 0.2413\n",
      "Epoch 5:\n",
      "Train loss: 1.7364, Train acc: 0.2187\n",
      "Valid loss: 1.7259, Valid acc: 0.2422\n",
      "Epoch 6:\n",
      "Train loss: 1.7247, Train acc: 0.2224\n",
      "Valid loss: 1.7142, Valid acc: 0.2422\n",
      "Epoch 7:\n",
      "Train loss: 1.7136, Train acc: 0.2272\n",
      "Valid loss: 1.7035, Valid acc: 0.2532\n",
      "Epoch 8:\n",
      "Train loss: 1.7040, Train acc: 0.2109\n",
      "Valid loss: 1.6938, Valid acc: 0.2009\n",
      "Epoch 9:\n",
      "Train loss: 1.6950, Train acc: 0.2235\n",
      "Valid loss: 1.6854, Valid acc: 0.2000\n",
      "Epoch 10:\n",
      "Train loss: 1.6874, Train acc: 0.2233\n",
      "Valid loss: 1.6781, Valid acc: 0.1982\n",
      "Epoch 11:\n",
      "Train loss: 1.6797, Train acc: 0.2226\n",
      "Valid loss: 1.6716, Valid acc: 0.2028\n",
      "Epoch 12:\n",
      "Train loss: 1.6738, Train acc: 0.2215\n",
      "Valid loss: 1.6664, Valid acc: 0.2294\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7868, Train acc: 0.2203\n",
      "Valid loss: 1.7753, Valid acc: 0.2394\n",
      "Epoch 2:\n",
      "Train loss: 1.7722, Train acc: 0.2228\n",
      "Valid loss: 1.7616, Valid acc: 0.2413\n",
      "Epoch 3:\n",
      "Train loss: 1.7592, Train acc: 0.2203\n",
      "Valid loss: 1.7495, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.7474, Train acc: 0.2231\n",
      "Valid loss: 1.7385, Valid acc: 0.2055\n",
      "Epoch 5:\n",
      "Train loss: 1.7364, Train acc: 0.2274\n",
      "Valid loss: 1.7285, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.7265, Train acc: 0.2247\n",
      "Valid loss: 1.7193, Valid acc: 0.2064\n",
      "Epoch 7:\n",
      "Train loss: 1.7177, Train acc: 0.2270\n",
      "Valid loss: 1.7107, Valid acc: 0.2055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7782, Train acc: 0.2086\n",
      "Valid loss: 1.7738, Valid acc: 0.2358\n",
      "Epoch 2:\n",
      "Train loss: 1.7710, Train acc: 0.2093\n",
      "Valid loss: 1.7670, Valid acc: 0.2358\n",
      "Epoch 3:\n",
      "Train loss: 1.7646, Train acc: 0.2084\n",
      "Valid loss: 1.7607, Valid acc: 0.2358\n",
      "Epoch 4:\n",
      "Train loss: 1.7580, Train acc: 0.2093\n",
      "Valid loss: 1.7547, Valid acc: 0.2358\n",
      "Epoch 5:\n",
      "Train loss: 1.7522, Train acc: 0.2072\n",
      "Valid loss: 1.7492, Valid acc: 0.2358\n",
      "Epoch 6:\n",
      "Train loss: 1.7456, Train acc: 0.2095\n",
      "Valid loss: 1.7439, Valid acc: 0.2349\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7868, Train acc: 0.2102\n",
      "Valid loss: 1.7895, Valid acc: 0.2394\n",
      "Epoch 2:\n",
      "Train loss: 1.7790, Train acc: 0.2125\n",
      "Valid loss: 1.7812, Valid acc: 0.2404\n",
      "Epoch 3:\n",
      "Train loss: 1.7709, Train acc: 0.2137\n",
      "Valid loss: 1.7732, Valid acc: 0.2404\n",
      "Epoch 4:\n",
      "Train loss: 1.7644, Train acc: 0.2150\n",
      "Valid loss: 1.7658, Valid acc: 0.2422\n",
      "Epoch 5:\n",
      "Train loss: 1.7572, Train acc: 0.2171\n",
      "Valid loss: 1.7588, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.7508, Train acc: 0.2210\n",
      "Valid loss: 1.7521, Valid acc: 0.2422\n",
      "Epoch 7:\n",
      "Train loss: 1.7444, Train acc: 0.2166\n",
      "Valid loss: 1.7459, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.7382, Train acc: 0.2215\n",
      "Valid loss: 1.7400, Valid acc: 0.2018\n",
      "Epoch 9:\n",
      "Train loss: 1.7332, Train acc: 0.2242\n",
      "Valid loss: 1.7342, Valid acc: 0.2018\n",
      "Epoch 10:\n",
      "Train loss: 1.7284, Train acc: 0.2267\n",
      "Valid loss: 1.7288, Valid acc: 0.2009\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7846, Train acc: 0.1933\n",
      "Valid loss: 1.7701, Valid acc: 0.2376\n",
      "Epoch 2:\n",
      "Train loss: 1.7772, Train acc: 0.2091\n",
      "Valid loss: 1.7634, Valid acc: 0.2394\n",
      "Epoch 3:\n",
      "Train loss: 1.7704, Train acc: 0.2095\n",
      "Valid loss: 1.7571, Valid acc: 0.2376\n",
      "Epoch 4:\n",
      "Train loss: 1.7638, Train acc: 0.2144\n",
      "Valid loss: 1.7512, Valid acc: 0.2367\n",
      "Epoch 5:\n",
      "Train loss: 1.7593, Train acc: 0.2116\n",
      "Valid loss: 1.7457, Valid acc: 0.2385\n",
      "Epoch 6:\n",
      "Train loss: 1.7534, Train acc: 0.2139\n",
      "Valid loss: 1.7403, Valid acc: 0.2394\n",
      "Epoch 7:\n",
      "Train loss: 1.7479, Train acc: 0.2171\n",
      "Valid loss: 1.7354, Valid acc: 0.2413\n",
      "Epoch 8:\n",
      "Train loss: 1.7434, Train acc: 0.2072\n",
      "Valid loss: 1.7306, Valid acc: 0.2009\n",
      "Epoch 9:\n",
      "Train loss: 1.7373, Train acc: 0.2160\n",
      "Valid loss: 1.7259, Valid acc: 0.2009\n",
      "Epoch 10:\n",
      "Train loss: 1.7340, Train acc: 0.2201\n",
      "Valid loss: 1.7215, Valid acc: 0.2018\n",
      "Epoch 11:\n",
      "Train loss: 1.7293, Train acc: 0.2233\n",
      "Valid loss: 1.7172, Valid acc: 0.2073\n",
      "Epoch 12:\n",
      "Train loss: 1.7244, Train acc: 0.2242\n",
      "Valid loss: 1.7130, Valid acc: 0.2083\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.8162, Train acc: 0.1696\n",
      "Valid loss: 1.8199, Valid acc: 0.1440\n",
      "Epoch 2:\n",
      "Train loss: 1.8084, Train acc: 0.1706\n",
      "Valid loss: 1.8135, Valid acc: 0.1450\n",
      "Epoch 3:\n",
      "Train loss: 1.8030, Train acc: 0.1706\n",
      "Valid loss: 1.8074, Valid acc: 0.1450\n",
      "Epoch 4:\n",
      "Train loss: 1.7967, Train acc: 0.1703\n",
      "Valid loss: 1.8015, Valid acc: 0.1450\n",
      "Epoch 5:\n",
      "Train loss: 1.7899, Train acc: 0.1699\n",
      "Valid loss: 1.7960, Valid acc: 0.1450\n",
      "Epoch 6:\n",
      "Train loss: 1.7844, Train acc: 0.1703\n",
      "Valid loss: 1.7907, Valid acc: 0.1450\n",
      "Epoch 7:\n",
      "Train loss: 1.7795, Train acc: 0.1699\n",
      "Valid loss: 1.7857, Valid acc: 0.1468\n",
      "Epoch 8:\n",
      "Train loss: 1.7748, Train acc: 0.1696\n",
      "Valid loss: 1.7808, Valid acc: 0.1477\n",
      "Epoch 9:\n",
      "Train loss: 1.7701, Train acc: 0.1726\n",
      "Valid loss: 1.7760, Valid acc: 0.1477\n",
      "Epoch 10:\n",
      "Train loss: 1.7671, Train acc: 0.1958\n",
      "Valid loss: 1.7715, Valid acc: 0.2009\n",
      "Epoch 11:\n",
      "Train loss: 1.7626, Train acc: 0.2254\n",
      "Valid loss: 1.7672, Valid acc: 0.1991\n",
      "Epoch 12:\n",
      "Train loss: 1.7586, Train acc: 0.2221\n",
      "Valid loss: 1.7630, Valid acc: 0.1982\n",
      "Epoch 13:\n",
      "Train loss: 1.7541, Train acc: 0.2238\n",
      "Valid loss: 1.7589, Valid acc: 0.1982\n",
      "Epoch 14:\n",
      "Train loss: 1.7476, Train acc: 0.2247\n",
      "Valid loss: 1.7550, Valid acc: 0.1982\n",
      "Epoch 15:\n",
      "Train loss: 1.7457, Train acc: 0.2233\n",
      "Valid loss: 1.7511, Valid acc: 0.1991\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7826, Train acc: 0.1694\n",
      "Valid loss: 1.7947, Valid acc: 0.1450\n",
      "Epoch 2:\n",
      "Train loss: 1.7791, Train acc: 0.1708\n",
      "Valid loss: 1.7899, Valid acc: 0.1450\n",
      "Epoch 3:\n",
      "Train loss: 1.7750, Train acc: 0.1710\n",
      "Valid loss: 1.7850, Valid acc: 0.1450\n",
      "Epoch 4:\n",
      "Train loss: 1.7704, Train acc: 0.1717\n",
      "Valid loss: 1.7804, Valid acc: 0.1477\n",
      "Epoch 5:\n",
      "Train loss: 1.7657, Train acc: 0.2022\n",
      "Valid loss: 1.7759, Valid acc: 0.1982\n",
      "Epoch 6:\n",
      "Train loss: 1.7635, Train acc: 0.2226\n",
      "Valid loss: 1.7715, Valid acc: 0.1972\n",
      "Epoch 7:\n",
      "Train loss: 1.7601, Train acc: 0.2235\n",
      "Valid loss: 1.7672, Valid acc: 0.2000\n",
      "Epoch 8:\n",
      "Train loss: 1.7549, Train acc: 0.2242\n",
      "Valid loss: 1.7630, Valid acc: 0.2000\n",
      "Epoch 9:\n",
      "Train loss: 1.7500, Train acc: 0.2256\n",
      "Valid loss: 1.7589, Valid acc: 0.2000\n",
      "Epoch 10:\n",
      "Train loss: 1.7474, Train acc: 0.2263\n",
      "Valid loss: 1.7549, Valid acc: 0.2000\n",
      "Epoch 11:\n",
      "Train loss: 1.7444, Train acc: 0.2254\n",
      "Valid loss: 1.7511, Valid acc: 0.2009\n",
      "Epoch 12:\n",
      "Train loss: 1.7405, Train acc: 0.2249\n",
      "Valid loss: 1.7472, Valid acc: 0.2009\n",
      "Epoch 13:\n",
      "Train loss: 1.7380, Train acc: 0.2231\n",
      "Valid loss: 1.7435, Valid acc: 0.2009\n",
      "Epoch 14:\n",
      "Train loss: 1.7320, Train acc: 0.2267\n",
      "Valid loss: 1.7398, Valid acc: 0.2009\n",
      "Epoch 15:\n",
      "Train loss: 1.7297, Train acc: 0.2263\n",
      "Valid loss: 1.7362, Valid acc: 0.2009\n",
      "Epoch 16:\n",
      "Train loss: 1.7258, Train acc: 0.2270\n",
      "Valid loss: 1.7325, Valid acc: 0.2009\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7981, Train acc: 0.2093\n",
      "Valid loss: 1.7960, Valid acc: 0.2349\n",
      "Epoch 2:\n",
      "Train loss: 1.7937, Train acc: 0.2079\n",
      "Valid loss: 1.7914, Valid acc: 0.2349\n",
      "Epoch 3:\n",
      "Train loss: 1.7899, Train acc: 0.2088\n",
      "Valid loss: 1.7870, Valid acc: 0.2349\n",
      "Epoch 4:\n",
      "Train loss: 1.7840, Train acc: 0.2095\n",
      "Valid loss: 1.7827, Valid acc: 0.2349\n",
      "Epoch 5:\n",
      "Train loss: 1.7809, Train acc: 0.2093\n",
      "Valid loss: 1.7786, Valid acc: 0.2349\n",
      "Epoch 6:\n",
      "Train loss: 1.7770, Train acc: 0.2093\n",
      "Valid loss: 1.7746, Valid acc: 0.2349\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6854, Train acc: 0.2267\n",
      "Valid loss: 1.6526, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6522, Train acc: 0.2219\n",
      "Valid loss: 1.6495, Valid acc: 0.2450\n",
      "Epoch 3:\n",
      "Train loss: 1.6475, Train acc: 0.2348\n",
      "Valid loss: 1.6462, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6469, Train acc: 0.2258\n",
      "Valid loss: 1.6454, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6458, Train acc: 0.2329\n",
      "Valid loss: 1.6458, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.6479, Train acc: 0.2297\n",
      "Valid loss: 1.6468, Valid acc: 0.2440\n",
      "Epoch 7:\n",
      "Train loss: 1.6473, Train acc: 0.2274\n",
      "Valid loss: 1.6464, Valid acc: 0.2450\n",
      "Epoch 8:\n",
      "Train loss: 1.6492, Train acc: 0.2313\n",
      "Valid loss: 1.6475, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6605, Train acc: 0.2354\n",
      "Valid loss: 1.6434, Valid acc: 0.2394\n",
      "Epoch 2:\n",
      "Train loss: 1.6469, Train acc: 0.2302\n",
      "Valid loss: 1.6419, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6425, Train acc: 0.2315\n",
      "Valid loss: 1.6411, Valid acc: 0.2679\n",
      "Epoch 4:\n",
      "Train loss: 1.6431, Train acc: 0.2325\n",
      "Valid loss: 1.6423, Valid acc: 0.2578\n",
      "Epoch 5:\n",
      "Train loss: 1.6424, Train acc: 0.2343\n",
      "Valid loss: 1.6428, Valid acc: 0.2358\n",
      "Epoch 6:\n",
      "Train loss: 1.6405, Train acc: 0.2398\n",
      "Valid loss: 1.6405, Valid acc: 0.2550\n",
      "Epoch 7:\n",
      "Train loss: 1.6366, Train acc: 0.2666\n",
      "Valid loss: 1.6396, Valid acc: 0.2569\n",
      "Epoch 8:\n",
      "Train loss: 1.5958, Train acc: 0.2980\n",
      "Valid loss: 1.4953, Valid acc: 0.3798\n",
      "Epoch 9:\n",
      "Train loss: 1.3802, Train acc: 0.4090\n",
      "Valid loss: 1.3769, Valid acc: 0.4183\n",
      "Epoch 10:\n",
      "Train loss: 1.2753, Train acc: 0.4436\n",
      "Valid loss: 1.3012, Valid acc: 0.4532\n",
      "Epoch 11:\n",
      "Train loss: 1.1874, Train acc: 0.4890\n",
      "Valid loss: 1.2544, Valid acc: 0.4954\n",
      "Epoch 12:\n",
      "Train loss: 1.1220, Train acc: 0.5369\n",
      "Valid loss: 1.1972, Valid acc: 0.5046\n",
      "Epoch 13:\n",
      "Train loss: 1.0559, Train acc: 0.5674\n",
      "Valid loss: 1.1218, Valid acc: 0.5385\n",
      "Epoch 14:\n",
      "Train loss: 1.0248, Train acc: 0.5862\n",
      "Valid loss: 1.1002, Valid acc: 0.5505\n",
      "Epoch 15:\n",
      "Train loss: 0.9844, Train acc: 0.5954\n",
      "Valid loss: 1.0790, Valid acc: 0.5514\n",
      "Epoch 16:\n",
      "Train loss: 0.9481, Train acc: 0.6227\n",
      "Valid loss: 1.0734, Valid acc: 0.5468\n",
      "Epoch 17:\n",
      "Train loss: 0.9135, Train acc: 0.6309\n",
      "Valid loss: 1.0329, Valid acc: 0.5734\n",
      "Epoch 18:\n",
      "Train loss: 0.8912, Train acc: 0.6465\n",
      "Valid loss: 1.0132, Valid acc: 0.5752\n",
      "Epoch 19:\n",
      "Train loss: 0.8597, Train acc: 0.6598\n",
      "Valid loss: 1.0275, Valid acc: 0.5550\n",
      "Epoch 20:\n",
      "Train loss: 0.8395, Train acc: 0.6692\n",
      "Valid loss: 0.9914, Valid acc: 0.5872\n",
      "Epoch 21:\n",
      "Train loss: 0.8101, Train acc: 0.6862\n",
      "Valid loss: 0.9823, Valid acc: 0.5826\n",
      "Epoch 22:\n",
      "Train loss: 0.7958, Train acc: 0.6978\n",
      "Valid loss: 0.9677, Valid acc: 0.5899\n",
      "Epoch 23:\n",
      "Train loss: 0.7736, Train acc: 0.7050\n",
      "Valid loss: 0.9612, Valid acc: 0.5991\n",
      "Epoch 24:\n",
      "Train loss: 0.7500, Train acc: 0.7162\n",
      "Valid loss: 0.9650, Valid acc: 0.5991\n",
      "Epoch 25:\n",
      "Train loss: 0.7335, Train acc: 0.7244\n",
      "Valid loss: 0.9396, Valid acc: 0.6110\n",
      "Epoch 26:\n",
      "Train loss: 0.7181, Train acc: 0.7247\n",
      "Valid loss: 0.9432, Valid acc: 0.5899\n",
      "Epoch 27:\n",
      "Train loss: 0.6968, Train acc: 0.7400\n",
      "Valid loss: 0.9296, Valid acc: 0.6128\n",
      "Epoch 28:\n",
      "Train loss: 0.6832, Train acc: 0.7446\n",
      "Valid loss: 0.9436, Valid acc: 0.6000\n",
      "Epoch 29:\n",
      "Train loss: 0.6631, Train acc: 0.7522\n",
      "Valid loss: 0.9526, Valid acc: 0.5963\n",
      "Epoch 30:\n",
      "Train loss: 0.6570, Train acc: 0.7510\n",
      "Valid loss: 0.9220, Valid acc: 0.6174\n",
      "Epoch 31:\n",
      "Train loss: 0.6403, Train acc: 0.7581\n",
      "Valid loss: 0.9180, Valid acc: 0.6275\n",
      "Epoch 32:\n",
      "Train loss: 0.6314, Train acc: 0.7636\n",
      "Valid loss: 0.9271, Valid acc: 0.6083\n",
      "Epoch 33:\n",
      "Train loss: 0.6197, Train acc: 0.7639\n",
      "Valid loss: 0.8958, Valid acc: 0.6459\n",
      "Epoch 34:\n",
      "Train loss: 0.6091, Train acc: 0.7641\n",
      "Valid loss: 0.9216, Valid acc: 0.6138\n",
      "Epoch 35:\n",
      "Train loss: 0.5895, Train acc: 0.7753\n",
      "Valid loss: 0.8970, Valid acc: 0.6312\n",
      "Epoch 36:\n",
      "Train loss: 0.5803, Train acc: 0.7781\n",
      "Valid loss: 0.9288, Valid acc: 0.6229\n",
      "Epoch 37:\n",
      "Train loss: 0.5781, Train acc: 0.7767\n",
      "Valid loss: 0.8788, Valid acc: 0.6422\n",
      "Epoch 38:\n",
      "Train loss: 0.5632, Train acc: 0.7749\n",
      "Valid loss: 0.8925, Valid acc: 0.6303\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6606, Train acc: 0.2217\n",
      "Valid loss: 1.6419, Valid acc: 0.2495\n",
      "Epoch 2:\n",
      "Train loss: 1.6477, Train acc: 0.2345\n",
      "Valid loss: 1.6472, Valid acc: 0.2505\n",
      "Epoch 3:\n",
      "Train loss: 1.6450, Train acc: 0.2295\n",
      "Valid loss: 1.6423, Valid acc: 0.2339\n",
      "Epoch 4:\n",
      "Train loss: 1.6417, Train acc: 0.2263\n",
      "Valid loss: 1.6410, Valid acc: 0.2697\n",
      "Epoch 5:\n",
      "Train loss: 1.6210, Train acc: 0.2492\n",
      "Valid loss: 1.6120, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.4925, Train acc: 0.3388\n",
      "Valid loss: 1.5164, Valid acc: 0.3431\n",
      "Epoch 7:\n",
      "Train loss: 1.2349, Train acc: 0.4739\n",
      "Valid loss: 1.2014, Valid acc: 0.5147\n",
      "Epoch 8:\n",
      "Train loss: 1.0772, Train acc: 0.5624\n",
      "Valid loss: 1.0905, Valid acc: 0.5688\n",
      "Epoch 9:\n",
      "Train loss: 1.0082, Train acc: 0.5997\n",
      "Valid loss: 1.2724, Valid acc: 0.4826\n",
      "Epoch 10:\n",
      "Train loss: 0.9527, Train acc: 0.6231\n",
      "Valid loss: 1.0718, Valid acc: 0.5807\n",
      "Epoch 11:\n",
      "Train loss: 0.9068, Train acc: 0.6625\n",
      "Valid loss: 1.0029, Valid acc: 0.6358\n",
      "Epoch 12:\n",
      "Train loss: 0.8741, Train acc: 0.6726\n",
      "Valid loss: 0.9796, Valid acc: 0.6339\n",
      "Epoch 13:\n",
      "Train loss: 0.8305, Train acc: 0.7144\n",
      "Valid loss: 0.9687, Valid acc: 0.6450\n",
      "Epoch 14:\n",
      "Train loss: 0.8109, Train acc: 0.7228\n",
      "Valid loss: 0.9952, Valid acc: 0.6376\n",
      "Epoch 15:\n",
      "Train loss: 0.7685, Train acc: 0.7428\n",
      "Valid loss: 1.0169, Valid acc: 0.6156\n",
      "Epoch 16:\n",
      "Train loss: 0.7323, Train acc: 0.7671\n",
      "Valid loss: 0.9182, Valid acc: 0.6963\n",
      "Epoch 17:\n",
      "Train loss: 0.7016, Train acc: 0.7847\n",
      "Valid loss: 0.9276, Valid acc: 0.6945\n",
      "Epoch 18:\n",
      "Train loss: 0.6765, Train acc: 0.8010\n",
      "Valid loss: 0.9011, Valid acc: 0.6890\n",
      "Epoch 19:\n",
      "Train loss: 0.6441, Train acc: 0.8129\n",
      "Valid loss: 0.9136, Valid acc: 0.6963\n",
      "Epoch 20:\n",
      "Train loss: 0.6121, Train acc: 0.8345\n",
      "Valid loss: 0.9183, Valid acc: 0.6633\n",
      "Epoch 21:\n",
      "Train loss: 0.5849, Train acc: 0.8448\n",
      "Valid loss: 0.8541, Valid acc: 0.7239\n",
      "Epoch 22:\n",
      "Train loss: 0.5451, Train acc: 0.8611\n",
      "Valid loss: 0.8308, Valid acc: 0.7413\n",
      "Epoch 23:\n",
      "Train loss: 0.5299, Train acc: 0.8668\n",
      "Valid loss: 0.8036, Valid acc: 0.7486\n",
      "Epoch 24:\n",
      "Train loss: 0.5020, Train acc: 0.8723\n",
      "Valid loss: 0.8189, Valid acc: 0.7532\n",
      "Epoch 25:\n",
      "Train loss: 0.4797, Train acc: 0.8812\n",
      "Valid loss: 0.8042, Valid acc: 0.7468\n",
      "Epoch 26:\n",
      "Train loss: 0.4599, Train acc: 0.8870\n",
      "Valid loss: 0.8224, Valid acc: 0.7450\n",
      "Epoch 27:\n",
      "Train loss: 0.4359, Train acc: 0.8906\n",
      "Valid loss: 0.7853, Valid acc: 0.7606\n",
      "Epoch 28:\n",
      "Train loss: 0.4296, Train acc: 0.8966\n",
      "Valid loss: 0.7890, Valid acc: 0.7596\n",
      "Epoch 29:\n",
      "Train loss: 0.4141, Train acc: 0.9000\n",
      "Valid loss: 0.7777, Valid acc: 0.7651\n",
      "Epoch 30:\n",
      "Train loss: 0.4124, Train acc: 0.9019\n",
      "Valid loss: 0.7659, Valid acc: 0.7706\n",
      "Epoch 31:\n",
      "Train loss: 0.3832, Train acc: 0.9074\n",
      "Valid loss: 0.7966, Valid acc: 0.7486\n",
      "Epoch 32:\n",
      "Train loss: 0.3737, Train acc: 0.9113\n",
      "Valid loss: 0.7727, Valid acc: 0.7798\n",
      "Epoch 33:\n",
      "Train loss: 0.3754, Train acc: 0.9094\n",
      "Valid loss: 0.7820, Valid acc: 0.7734\n",
      "Epoch 34:\n",
      "Train loss: 0.3640, Train acc: 0.9221\n",
      "Valid loss: 1.0406, Valid acc: 0.7321\n",
      "Epoch 35:\n",
      "Train loss: 0.3673, Train acc: 0.9106\n",
      "Valid loss: 0.7678, Valid acc: 0.7844\n",
      "Epoch 36:\n",
      "Train loss: 0.3228, Train acc: 0.9310\n",
      "Valid loss: 0.8112, Valid acc: 0.7688\n",
      "Epoch 37:\n",
      "Train loss: 0.3242, Train acc: 0.9266\n",
      "Valid loss: 0.7658, Valid acc: 0.7835\n",
      "Epoch 38:\n",
      "Train loss: 0.3141, Train acc: 0.9321\n",
      "Valid loss: 0.7752, Valid acc: 0.7771\n",
      "Epoch 39:\n",
      "Train loss: 0.3078, Train acc: 0.9319\n",
      "Valid loss: 0.7746, Valid acc: 0.7826\n",
      "Epoch 40:\n",
      "Train loss: 0.3102, Train acc: 0.9312\n",
      "Valid loss: 0.7696, Valid acc: 0.7853\n",
      "Epoch 41:\n",
      "Train loss: 0.3006, Train acc: 0.9333\n",
      "Valid loss: 0.8055, Valid acc: 0.7881\n",
      "Epoch 42:\n",
      "Train loss: 0.2870, Train acc: 0.9388\n",
      "Valid loss: 0.7708, Valid acc: 0.7872\n",
      "Epoch 43:\n",
      "Train loss: 0.2811, Train acc: 0.9402\n",
      "Valid loss: 0.8027, Valid acc: 0.7844\n",
      "Epoch 44:\n",
      "Train loss: 0.2841, Train acc: 0.9376\n",
      "Valid loss: 0.7639, Valid acc: 0.7908\n",
      "Epoch 45:\n",
      "Train loss: 0.2701, Train acc: 0.9429\n",
      "Valid loss: 0.8180, Valid acc: 0.7780\n",
      "Epoch 46:\n",
      "Train loss: 0.2629, Train acc: 0.9429\n",
      "Valid loss: 0.7654, Valid acc: 0.7991\n",
      "Epoch 47:\n",
      "Train loss: 0.2564, Train acc: 0.9466\n",
      "Valid loss: 0.7933, Valid acc: 0.7872\n",
      "Epoch 48:\n",
      "Train loss: 0.2471, Train acc: 0.9489\n",
      "Valid loss: 0.8092, Valid acc: 0.7899\n",
      "Epoch 49:\n",
      "Train loss: 0.2569, Train acc: 0.9461\n",
      "Valid loss: 0.9149, Valid acc: 0.7514\n",
      "Epoch 50:\n",
      "Train loss: 0.2530, Train acc: 0.9452\n",
      "Valid loss: 0.7813, Valid acc: 0.7899\n",
      "Epoch 51:\n",
      "Train loss: 0.2467, Train acc: 0.9486\n",
      "Valid loss: 0.7997, Valid acc: 0.7881\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6879, Train acc: 0.2240\n",
      "Valid loss: 1.6599, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6557, Train acc: 0.2299\n",
      "Valid loss: 1.6536, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.6500, Train acc: 0.2293\n",
      "Valid loss: 1.6496, Valid acc: 0.2055\n",
      "Epoch 4:\n",
      "Train loss: 1.6469, Train acc: 0.2265\n",
      "Valid loss: 1.6509, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6479, Train acc: 0.2224\n",
      "Valid loss: 1.6454, Valid acc: 0.2404\n",
      "Epoch 6:\n",
      "Train loss: 1.6489, Train acc: 0.2315\n",
      "Valid loss: 1.6471, Valid acc: 0.2385\n",
      "Epoch 7:\n",
      "Train loss: 1.6449, Train acc: 0.2258\n",
      "Valid loss: 1.6490, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6773, Train acc: 0.2194\n",
      "Valid loss: 1.6457, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6512, Train acc: 0.2212\n",
      "Valid loss: 1.6450, Valid acc: 0.2028\n",
      "Epoch 3:\n",
      "Train loss: 1.6496, Train acc: 0.2325\n",
      "Valid loss: 1.6403, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6496, Train acc: 0.2263\n",
      "Valid loss: 1.6423, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6484, Train acc: 0.2286\n",
      "Valid loss: 1.6378, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.6458, Train acc: 0.2311\n",
      "Valid loss: 1.6388, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6488, Train acc: 0.2249\n",
      "Valid loss: 1.6383, Valid acc: 0.2349\n",
      "Epoch 8:\n",
      "Train loss: 1.6497, Train acc: 0.2270\n",
      "Valid loss: 1.6390, Valid acc: 0.2055\n",
      "Epoch 9:\n",
      "Train loss: 1.6460, Train acc: 0.2279\n",
      "Valid loss: 1.6398, Valid acc: 0.2385\n",
      "Epoch 10:\n",
      "Train loss: 1.6452, Train acc: 0.2221\n",
      "Valid loss: 1.6419, Valid acc: 0.2440\n",
      "Epoch 11:\n",
      "Train loss: 1.6492, Train acc: 0.2276\n",
      "Valid loss: 1.6409, Valid acc: 0.2431\n",
      "Epoch 12:\n",
      "Train loss: 1.6462, Train acc: 0.2329\n",
      "Valid loss: 1.6417, Valid acc: 0.2046\n",
      "Epoch 13:\n",
      "Train loss: 1.6482, Train acc: 0.2293\n",
      "Valid loss: 1.6412, Valid acc: 0.2440\n",
      "Epoch 14:\n",
      "Train loss: 1.6453, Train acc: 0.2304\n",
      "Valid loss: 1.6391, Valid acc: 0.2450\n",
      "Epoch 15:\n",
      "Train loss: 1.6451, Train acc: 0.2327\n",
      "Valid loss: 1.6404, Valid acc: 0.2404\n",
      "Epoch 16:\n",
      "Train loss: 1.6439, Train acc: 0.2341\n",
      "Valid loss: 1.6436, Valid acc: 0.2394\n",
      "Epoch 17:\n",
      "Train loss: 1.6446, Train acc: 0.2311\n",
      "Valid loss: 1.6437, Valid acc: 0.2046\n",
      "Epoch 18:\n",
      "Train loss: 1.6440, Train acc: 0.2304\n",
      "Valid loss: 1.6449, Valid acc: 0.2046\n",
      "Epoch 19:\n",
      "Train loss: 1.6461, Train acc: 0.2276\n",
      "Valid loss: 1.6429, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6637, Train acc: 0.2306\n",
      "Valid loss: 1.6357, Valid acc: 0.2440\n",
      "Epoch 2:\n",
      "Train loss: 1.6493, Train acc: 0.2318\n",
      "Valid loss: 1.6366, Valid acc: 0.2486\n",
      "Epoch 3:\n",
      "Train loss: 1.6485, Train acc: 0.2297\n",
      "Valid loss: 1.6344, Valid acc: 0.2596\n",
      "Epoch 4:\n",
      "Train loss: 1.6451, Train acc: 0.2336\n",
      "Valid loss: 1.6332, Valid acc: 0.2404\n",
      "Epoch 5:\n",
      "Train loss: 1.6464, Train acc: 0.2460\n",
      "Valid loss: 1.6402, Valid acc: 0.2138\n",
      "Epoch 6:\n",
      "Train loss: 1.6220, Train acc: 0.2707\n",
      "Valid loss: 1.5253, Valid acc: 0.3128\n",
      "Epoch 7:\n",
      "Train loss: 1.4136, Train acc: 0.4191\n",
      "Valid loss: 1.4895, Valid acc: 0.3505\n",
      "Epoch 8:\n",
      "Train loss: 1.2729, Train acc: 0.4982\n",
      "Valid loss: 1.2105, Valid acc: 0.5110\n",
      "Epoch 9:\n",
      "Train loss: 1.1215, Train acc: 0.5479\n",
      "Valid loss: 1.2467, Valid acc: 0.4991\n",
      "Epoch 10:\n",
      "Train loss: 1.0482, Train acc: 0.5851\n",
      "Valid loss: 1.1350, Valid acc: 0.5578\n",
      "Epoch 11:\n",
      "Train loss: 1.0054, Train acc: 0.6025\n",
      "Valid loss: 1.1029, Valid acc: 0.5606\n",
      "Epoch 12:\n",
      "Train loss: 0.9343, Train acc: 0.6419\n",
      "Valid loss: 1.0423, Valid acc: 0.6193\n",
      "Epoch 13:\n",
      "Train loss: 0.8956, Train acc: 0.6678\n",
      "Valid loss: 1.0161, Valid acc: 0.6385\n",
      "Epoch 14:\n",
      "Train loss: 0.8544, Train acc: 0.6887\n",
      "Valid loss: 0.9923, Valid acc: 0.6477\n",
      "Epoch 15:\n",
      "Train loss: 0.8111, Train acc: 0.7164\n",
      "Valid loss: 0.9689, Valid acc: 0.6450\n",
      "Epoch 16:\n",
      "Train loss: 0.7803, Train acc: 0.7393\n",
      "Valid loss: 1.1285, Valid acc: 0.5789\n",
      "Epoch 17:\n",
      "Train loss: 0.7543, Train acc: 0.7485\n",
      "Valid loss: 1.1588, Valid acc: 0.5624\n",
      "Epoch 18:\n",
      "Train loss: 0.7187, Train acc: 0.7701\n",
      "Valid loss: 1.0287, Valid acc: 0.6284\n",
      "Epoch 19:\n",
      "Train loss: 0.6878, Train acc: 0.7811\n",
      "Valid loss: 0.9563, Valid acc: 0.6587\n",
      "Epoch 20:\n",
      "Train loss: 0.6822, Train acc: 0.7909\n",
      "Valid loss: 0.9532, Valid acc: 0.6688\n",
      "Epoch 21:\n",
      "Train loss: 0.6540, Train acc: 0.8079\n",
      "Valid loss: 0.9348, Valid acc: 0.6853\n",
      "Epoch 22:\n",
      "Train loss: 0.6189, Train acc: 0.8219\n",
      "Valid loss: 0.8777, Valid acc: 0.7064\n",
      "Epoch 23:\n",
      "Train loss: 0.6028, Train acc: 0.8244\n",
      "Valid loss: 1.0241, Valid acc: 0.6183\n",
      "Epoch 24:\n",
      "Train loss: 0.5861, Train acc: 0.8388\n",
      "Valid loss: 0.9145, Valid acc: 0.6991\n",
      "Epoch 25:\n",
      "Train loss: 0.5750, Train acc: 0.8384\n",
      "Valid loss: 0.9116, Valid acc: 0.6945\n",
      "Epoch 26:\n",
      "Train loss: 0.5353, Train acc: 0.8581\n",
      "Valid loss: 0.8651, Valid acc: 0.7183\n",
      "Epoch 27:\n",
      "Train loss: 0.5293, Train acc: 0.8547\n",
      "Valid loss: 0.9257, Valid acc: 0.6936\n",
      "Epoch 28:\n",
      "Train loss: 0.5046, Train acc: 0.8650\n",
      "Valid loss: 0.9269, Valid acc: 0.7018\n",
      "Epoch 29:\n",
      "Train loss: 0.4913, Train acc: 0.8709\n",
      "Valid loss: 0.9114, Valid acc: 0.7138\n",
      "Epoch 30:\n",
      "Train loss: 0.4878, Train acc: 0.8737\n",
      "Valid loss: 0.8600, Valid acc: 0.7211\n",
      "Epoch 31:\n",
      "Train loss: 0.4505, Train acc: 0.8890\n",
      "Valid loss: 0.8403, Valid acc: 0.7358\n",
      "Epoch 32:\n",
      "Train loss: 0.4347, Train acc: 0.8939\n",
      "Valid loss: 0.8464, Valid acc: 0.7459\n",
      "Epoch 33:\n",
      "Train loss: 0.4270, Train acc: 0.8941\n",
      "Valid loss: 0.9100, Valid acc: 0.7211\n",
      "Epoch 34:\n",
      "Train loss: 0.4209, Train acc: 0.8996\n",
      "Valid loss: 0.8403, Valid acc: 0.7349\n",
      "Epoch 35:\n",
      "Train loss: 0.4241, Train acc: 0.8973\n",
      "Valid loss: 0.8360, Valid acc: 0.7505\n",
      "Epoch 36:\n",
      "Train loss: 0.3940, Train acc: 0.9033\n",
      "Valid loss: 0.8332, Valid acc: 0.7459\n",
      "Epoch 37:\n",
      "Train loss: 0.3951, Train acc: 0.9099\n",
      "Valid loss: 0.8803, Valid acc: 0.7367\n",
      "Epoch 38:\n",
      "Train loss: 0.3666, Train acc: 0.9163\n",
      "Valid loss: 0.8276, Valid acc: 0.7550\n",
      "Epoch 39:\n",
      "Train loss: 0.3750, Train acc: 0.9138\n",
      "Valid loss: 0.8638, Valid acc: 0.7385\n",
      "Epoch 40:\n",
      "Train loss: 0.3563, Train acc: 0.9209\n",
      "Valid loss: 0.8270, Valid acc: 0.7514\n",
      "Epoch 41:\n",
      "Train loss: 0.3507, Train acc: 0.9225\n",
      "Valid loss: 0.8181, Valid acc: 0.7697\n",
      "Epoch 42:\n",
      "Train loss: 0.3453, Train acc: 0.9246\n",
      "Valid loss: 0.8133, Valid acc: 0.7725\n",
      "Epoch 43:\n",
      "Train loss: 0.3492, Train acc: 0.9250\n",
      "Valid loss: 0.8276, Valid acc: 0.7642\n",
      "Epoch 44:\n",
      "Train loss: 0.3638, Train acc: 0.9147\n",
      "Valid loss: 0.9782, Valid acc: 0.7211\n",
      "Epoch 45:\n",
      "Train loss: 0.3257, Train acc: 0.9303\n",
      "Valid loss: 0.8113, Valid acc: 0.7706\n",
      "Epoch 46:\n",
      "Train loss: 0.3149, Train acc: 0.9337\n",
      "Valid loss: 0.8517, Valid acc: 0.7606\n",
      "Epoch 47:\n",
      "Train loss: 0.3174, Train acc: 0.9303\n",
      "Valid loss: 0.8027, Valid acc: 0.7734\n",
      "Epoch 48:\n",
      "Train loss: 0.3070, Train acc: 0.9360\n",
      "Valid loss: 0.8104, Valid acc: 0.7771\n",
      "Epoch 49:\n",
      "Train loss: 0.2961, Train acc: 0.9395\n",
      "Valid loss: 0.8299, Valid acc: 0.7734\n",
      "Epoch 50:\n",
      "Train loss: 0.3177, Train acc: 0.9292\n",
      "Valid loss: 0.8164, Valid acc: 0.7725\n",
      "Epoch 51:\n",
      "Train loss: 0.2985, Train acc: 0.9392\n",
      "Valid loss: 0.8065, Valid acc: 0.7771\n",
      "Epoch 52:\n",
      "Train loss: 0.2867, Train acc: 0.9404\n",
      "Valid loss: 0.8121, Valid acc: 0.7817\n",
      "Epoch 53:\n",
      "Train loss: 0.3318, Train acc: 0.9278\n",
      "Valid loss: 0.9392, Valid acc: 0.7339\n",
      "Epoch 54:\n",
      "Train loss: 0.2888, Train acc: 0.9388\n",
      "Valid loss: 0.8270, Valid acc: 0.7670\n",
      "Epoch 55:\n",
      "Train loss: 0.2728, Train acc: 0.9464\n",
      "Valid loss: 0.8094, Valid acc: 0.7807\n",
      "Epoch 56:\n",
      "Train loss: 0.2695, Train acc: 0.9475\n",
      "Valid loss: 0.8194, Valid acc: 0.7817\n",
      "Epoch 57:\n",
      "Train loss: 0.2702, Train acc: 0.9448\n",
      "Valid loss: 0.8239, Valid acc: 0.7826\n",
      "Epoch 58:\n",
      "Train loss: 0.2696, Train acc: 0.9450\n",
      "Valid loss: 0.8107, Valid acc: 0.7817\n",
      "Epoch 59:\n",
      "Train loss: 0.2600, Train acc: 0.9489\n",
      "Valid loss: 0.8111, Valid acc: 0.7881\n",
      "Epoch 60:\n",
      "Train loss: 0.2694, Train acc: 0.9464\n",
      "Valid loss: 0.8103, Valid acc: 0.7844\n",
      "Epoch 61:\n",
      "Train loss: 0.2634, Train acc: 0.9470\n",
      "Valid loss: 0.8210, Valid acc: 0.7826\n",
      "Epoch 62:\n",
      "Train loss: 0.2574, Train acc: 0.9482\n",
      "Valid loss: 0.8345, Valid acc: 0.7835\n",
      "Epoch 63:\n",
      "Train loss: 0.2607, Train acc: 0.9512\n",
      "Valid loss: 0.8271, Valid acc: 0.7835\n",
      "Epoch 64:\n",
      "Train loss: 0.2464, Train acc: 0.9523\n",
      "Valid loss: 0.8136, Valid acc: 0.7881\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7164, Train acc: 0.2050\n",
      "Valid loss: 1.6662, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6580, Train acc: 0.2228\n",
      "Valid loss: 1.6539, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6535, Train acc: 0.2295\n",
      "Valid loss: 1.6522, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6480, Train acc: 0.2240\n",
      "Valid loss: 1.6527, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.6542, Train acc: 0.2290\n",
      "Valid loss: 1.6523, Valid acc: 0.2037\n",
      "Epoch 6:\n",
      "Train loss: 1.6530, Train acc: 0.2265\n",
      "Valid loss: 1.6527, Valid acc: 0.2037\n",
      "Epoch 7:\n",
      "Train loss: 1.6466, Train acc: 0.2274\n",
      "Valid loss: 1.6529, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.6493, Train acc: 0.2270\n",
      "Valid loss: 1.6525, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6984, Train acc: 0.2155\n",
      "Valid loss: 1.6598, Valid acc: 0.2385\n",
      "Epoch 2:\n",
      "Train loss: 1.6477, Train acc: 0.2102\n",
      "Valid loss: 1.6592, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6539, Train acc: 0.2203\n",
      "Valid loss: 1.6498, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6511, Train acc: 0.2263\n",
      "Valid loss: 1.6525, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6532, Train acc: 0.2341\n",
      "Valid loss: 1.6516, Valid acc: 0.2018\n",
      "Epoch 6:\n",
      "Train loss: 1.6490, Train acc: 0.2331\n",
      "Valid loss: 1.6508, Valid acc: 0.2468\n",
      "Epoch 7:\n",
      "Train loss: 1.6542, Train acc: 0.2226\n",
      "Valid loss: 1.6540, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6468, Train acc: 0.2242\n",
      "Valid loss: 1.6537, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6672, Train acc: 0.2182\n",
      "Valid loss: 1.6686, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6479, Train acc: 0.2299\n",
      "Valid loss: 1.6734, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6613, Train acc: 0.2235\n",
      "Valid loss: 1.6585, Valid acc: 0.2413\n",
      "Epoch 4:\n",
      "Train loss: 1.6475, Train acc: 0.2283\n",
      "Valid loss: 1.6595, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6473, Train acc: 0.2343\n",
      "Valid loss: 1.6623, Valid acc: 0.2486\n",
      "Epoch 6:\n",
      "Train loss: 1.6510, Train acc: 0.2297\n",
      "Valid loss: 1.6491, Valid acc: 0.2394\n",
      "Epoch 7:\n",
      "Train loss: 1.6501, Train acc: 0.2166\n",
      "Valid loss: 1.6526, Valid acc: 0.2431\n",
      "Epoch 8:\n",
      "Train loss: 1.6446, Train acc: 0.2288\n",
      "Valid loss: 1.6520, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.6475, Train acc: 0.2194\n",
      "Valid loss: 1.6520, Valid acc: 0.2486\n",
      "Epoch 10:\n",
      "Train loss: 1.6544, Train acc: 0.2311\n",
      "Valid loss: 1.6540, Valid acc: 0.2422\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6626, Train acc: 0.2299\n",
      "Valid loss: 1.6647, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6114, Train acc: 0.2762\n",
      "Valid loss: 1.6580, Valid acc: 0.2688\n",
      "Epoch 3:\n",
      "Train loss: 1.4708, Train acc: 0.3492\n",
      "Valid loss: 1.4680, Valid acc: 0.3578\n",
      "Epoch 4:\n",
      "Train loss: 1.4469, Train acc: 0.3530\n",
      "Valid loss: 1.4570, Valid acc: 0.3624\n",
      "Epoch 5:\n",
      "Train loss: 1.4872, Train acc: 0.3230\n",
      "Valid loss: 1.5921, Valid acc: 0.2459\n",
      "Epoch 6:\n",
      "Train loss: 1.4888, Train acc: 0.3113\n",
      "Valid loss: 1.5033, Valid acc: 0.3440\n",
      "Epoch 7:\n",
      "Train loss: 1.5741, Train acc: 0.2806\n",
      "Valid loss: 1.6424, Valid acc: 0.2486\n",
      "Epoch 8:\n",
      "Train loss: 1.6504, Train acc: 0.2235\n",
      "Valid loss: 1.6551, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.6488, Train acc: 0.2375\n",
      "Valid loss: 1.6452, Valid acc: 0.2486\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6687, Train acc: 0.2258\n",
      "Valid loss: 1.6536, Valid acc: 0.2486\n",
      "Epoch 2:\n",
      "Train loss: 1.6522, Train acc: 0.2265\n",
      "Valid loss: 1.6515, Valid acc: 0.2385\n",
      "Epoch 3:\n",
      "Train loss: 1.6440, Train acc: 0.2432\n",
      "Valid loss: 1.6588, Valid acc: 0.2495\n",
      "Epoch 4:\n",
      "Train loss: 1.6287, Train acc: 0.2593\n",
      "Valid loss: 1.5580, Valid acc: 0.3138\n",
      "Epoch 5:\n",
      "Train loss: 1.4891, Train acc: 0.3457\n",
      "Valid loss: 1.4427, Valid acc: 0.4037\n",
      "Epoch 6:\n",
      "Train loss: 1.5932, Train acc: 0.2882\n",
      "Valid loss: 1.6661, Valid acc: 0.2450\n",
      "Epoch 7:\n",
      "Train loss: 1.6447, Train acc: 0.2398\n",
      "Valid loss: 1.6627, Valid acc: 0.2358\n",
      "Epoch 8:\n",
      "Train loss: 1.6425, Train acc: 0.2313\n",
      "Valid loss: 1.6713, Valid acc: 0.2147\n",
      "Epoch 9:\n",
      "Train loss: 1.6386, Train acc: 0.2421\n",
      "Valid loss: 1.6620, Valid acc: 0.2367\n",
      "Epoch 10:\n",
      "Train loss: 1.6370, Train acc: 0.2359\n",
      "Valid loss: 1.6548, Valid acc: 0.2385\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6703, Train acc: 0.2148\n",
      "Valid loss: 1.6531, Valid acc: 0.2073\n",
      "Epoch 2:\n",
      "Train loss: 1.6534, Train acc: 0.2164\n",
      "Valid loss: 1.6484, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6173, Train acc: 0.2742\n",
      "Valid loss: 1.6265, Valid acc: 0.3367\n",
      "Epoch 4:\n",
      "Train loss: 1.6488, Train acc: 0.2444\n",
      "Valid loss: 1.6442, Valid acc: 0.2486\n",
      "Epoch 5:\n",
      "Train loss: 1.6805, Train acc: 0.2217\n",
      "Valid loss: 1.6632, Valid acc: 0.2477\n",
      "Epoch 6:\n",
      "Train loss: 1.6789, Train acc: 0.2242\n",
      "Valid loss: 1.6740, Valid acc: 0.2477\n",
      "Epoch 7:\n",
      "Train loss: 1.6589, Train acc: 0.2219\n",
      "Valid loss: 1.6497, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.6547, Train acc: 0.2276\n",
      "Valid loss: 1.6370, Valid acc: 0.2486\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6825, Train acc: 0.2297\n",
      "Valid loss: 1.6435, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6497, Train acc: 0.2247\n",
      "Valid loss: 1.6491, Valid acc: 0.2440\n",
      "Epoch 3:\n",
      "Train loss: 1.6522, Train acc: 0.2256\n",
      "Valid loss: 1.6548, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6558, Train acc: 0.2162\n",
      "Valid loss: 1.6531, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6511, Train acc: 0.2263\n",
      "Valid loss: 1.6583, Valid acc: 0.2404\n",
      "Epoch 6:\n",
      "Train loss: 1.6493, Train acc: 0.2199\n",
      "Valid loss: 1.6397, Valid acc: 0.2459\n",
      "Epoch 7:\n",
      "Train loss: 1.6027, Train acc: 0.2905\n",
      "Valid loss: 1.6634, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6561, Train acc: 0.2217\n",
      "Valid loss: 1.6432, Valid acc: 0.2468\n",
      "Epoch 9:\n",
      "Train loss: 1.6409, Train acc: 0.2274\n",
      "Valid loss: 1.6277, Valid acc: 0.2523\n",
      "Epoch 10:\n",
      "Train loss: 1.6387, Train acc: 0.2370\n",
      "Valid loss: 1.6438, Valid acc: 0.2468\n",
      "Epoch 11:\n",
      "Train loss: 1.6405, Train acc: 0.2322\n",
      "Valid loss: 1.6252, Valid acc: 0.2514\n",
      "Epoch 12:\n",
      "Train loss: 1.6563, Train acc: 0.2270\n",
      "Valid loss: 1.6431, Valid acc: 0.2073\n",
      "Epoch 13:\n",
      "Train loss: 1.6507, Train acc: 0.2279\n",
      "Valid loss: 1.6446, Valid acc: 0.2468\n",
      "Epoch 14:\n",
      "Train loss: 1.6496, Train acc: 0.2127\n",
      "Valid loss: 1.6381, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6668, Train acc: 0.2338\n",
      "Valid loss: 1.6400, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6526, Train acc: 0.2267\n",
      "Valid loss: 1.6511, Valid acc: 0.2422\n",
      "Epoch 3:\n",
      "Train loss: 1.6527, Train acc: 0.2288\n",
      "Valid loss: 1.6458, Valid acc: 0.2514\n",
      "Epoch 4:\n",
      "Train loss: 1.6528, Train acc: 0.2469\n",
      "Valid loss: 1.6586, Valid acc: 0.2495\n",
      "Epoch 5:\n",
      "Train loss: 1.6493, Train acc: 0.2166\n",
      "Valid loss: 1.6466, Valid acc: 0.2385\n",
      "Epoch 6:\n",
      "Train loss: 1.6427, Train acc: 0.2254\n",
      "Valid loss: 1.6433, Valid acc: 0.2505\n",
      "Epoch 7:\n",
      "Train loss: 1.6488, Train acc: 0.2380\n",
      "Valid loss: 1.6457, Valid acc: 0.2450\n",
      "Epoch 8:\n",
      "Train loss: 1.6386, Train acc: 0.2297\n",
      "Valid loss: 1.6467, Valid acc: 0.2037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6655, Train acc: 0.2251\n",
      "Valid loss: 1.6494, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.6261, Train acc: 0.2779\n",
      "Valid loss: 1.5566, Valid acc: 0.3578\n",
      "Epoch 3:\n",
      "Train loss: 1.6047, Train acc: 0.3012\n",
      "Valid loss: 1.6033, Valid acc: 0.3064\n",
      "Epoch 4:\n",
      "Train loss: 1.6065, Train acc: 0.3102\n",
      "Valid loss: 1.6571, Valid acc: 0.2349\n",
      "Epoch 5:\n",
      "Train loss: 1.6694, Train acc: 0.2283\n",
      "Valid loss: 1.6629, Valid acc: 0.2413\n",
      "Epoch 6:\n",
      "Train loss: 1.6589, Train acc: 0.2265\n",
      "Valid loss: 1.6492, Valid acc: 0.2394\n",
      "Epoch 7:\n",
      "Train loss: 1.6554, Train acc: 0.2235\n",
      "Valid loss: 1.6871, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6945, Train acc: 0.2309\n",
      "Valid loss: 1.6578, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6518, Train acc: 0.2260\n",
      "Valid loss: 1.6548, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6537, Train acc: 0.2199\n",
      "Valid loss: 1.6510, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6570, Train acc: 0.2217\n",
      "Valid loss: 1.6482, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6531, Train acc: 0.2125\n",
      "Valid loss: 1.6574, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6527, Train acc: 0.2201\n",
      "Valid loss: 1.6545, Valid acc: 0.2046\n",
      "Epoch 7:\n",
      "Train loss: 1.6606, Train acc: 0.2251\n",
      "Valid loss: 1.6552, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6542, Train acc: 0.2238\n",
      "Valid loss: 1.6545, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6774, Train acc: 0.2304\n",
      "Valid loss: 1.6675, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6668, Train acc: 0.2311\n",
      "Valid loss: 1.6572, Valid acc: 0.2385\n",
      "Epoch 3:\n",
      "Train loss: 1.6540, Train acc: 0.2130\n",
      "Valid loss: 1.6540, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6533, Train acc: 0.2215\n",
      "Valid loss: 1.6572, Valid acc: 0.2486\n",
      "Epoch 5:\n",
      "Train loss: 1.6526, Train acc: 0.2318\n",
      "Valid loss: 1.6578, Valid acc: 0.2028\n",
      "Epoch 6:\n",
      "Train loss: 1.6564, Train acc: 0.2279\n",
      "Valid loss: 1.6514, Valid acc: 0.2339\n",
      "Epoch 7:\n",
      "Train loss: 1.6532, Train acc: 0.2189\n",
      "Valid loss: 1.6551, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.6574, Train acc: 0.2322\n",
      "Valid loss: 1.6505, Valid acc: 0.2505\n",
      "Epoch 9:\n",
      "Train loss: 1.6507, Train acc: 0.2405\n",
      "Valid loss: 1.6259, Valid acc: 0.2706\n",
      "Epoch 10:\n",
      "Train loss: 1.5309, Train acc: 0.3496\n",
      "Valid loss: 1.5054, Valid acc: 0.3706\n",
      "Epoch 11:\n",
      "Train loss: 1.4890, Train acc: 0.3608\n",
      "Valid loss: 1.5153, Valid acc: 0.3486\n",
      "Epoch 12:\n",
      "Train loss: 1.4886, Train acc: 0.3276\n",
      "Valid loss: 1.5245, Valid acc: 0.3248\n",
      "Epoch 13:\n",
      "Train loss: 1.4987, Train acc: 0.3251\n",
      "Valid loss: 1.5367, Valid acc: 0.3229\n",
      "Epoch 14:\n",
      "Train loss: 1.4850, Train acc: 0.3416\n",
      "Valid loss: 1.4530, Valid acc: 0.3422\n",
      "Epoch 15:\n",
      "Train loss: 1.4588, Train acc: 0.3547\n",
      "Valid loss: 1.4493, Valid acc: 0.3789\n",
      "Epoch 16:\n",
      "Train loss: 1.4522, Train acc: 0.3535\n",
      "Valid loss: 1.4472, Valid acc: 0.3798\n",
      "Epoch 17:\n",
      "Train loss: 1.5130, Train acc: 0.3414\n",
      "Valid loss: 1.5548, Valid acc: 0.2550\n",
      "Epoch 18:\n",
      "Train loss: 1.5634, Train acc: 0.2983\n",
      "Valid loss: 1.6640, Valid acc: 0.2046\n",
      "Epoch 19:\n",
      "Train loss: 1.6607, Train acc: 0.2260\n",
      "Valid loss: 1.6614, Valid acc: 0.2046\n",
      "Epoch 20:\n",
      "Train loss: 1.6593, Train acc: 0.2185\n",
      "Valid loss: 1.6548, Valid acc: 0.2376\n",
      "Epoch 21:\n",
      "Train loss: 1.6470, Train acc: 0.2286\n",
      "Valid loss: 1.6539, Valid acc: 0.2046\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6809, Train acc: 0.2178\n",
      "Valid loss: 1.6533, Valid acc: 0.2404\n",
      "Epoch 2:\n",
      "Train loss: 1.6630, Train acc: 0.2247\n",
      "Valid loss: 1.6472, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.6538, Train acc: 0.2137\n",
      "Valid loss: 1.6623, Valid acc: 0.2587\n",
      "Epoch 4:\n",
      "Train loss: 1.6514, Train acc: 0.2313\n",
      "Valid loss: 1.6524, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6600, Train acc: 0.2242\n",
      "Valid loss: 1.6530, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6605, Train acc: 0.2254\n",
      "Valid loss: 1.6583, Valid acc: 0.2037\n",
      "Epoch 7:\n",
      "Train loss: 1.6531, Train acc: 0.2244\n",
      "Valid loss: 1.6539, Valid acc: 0.2018\n",
      "Epoch 8:\n",
      "Train loss: 1.6465, Train acc: 0.2217\n",
      "Valid loss: 1.6532, Valid acc: 0.2064\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6652, Train acc: 0.2192\n",
      "Valid loss: 1.6474, Valid acc: 0.2486\n",
      "Epoch 2:\n",
      "Train loss: 1.6467, Train acc: 0.2455\n",
      "Valid loss: 1.6612, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.5375, Train acc: 0.3242\n",
      "Valid loss: 1.4855, Valid acc: 0.3624\n",
      "Epoch 4:\n",
      "Train loss: 1.4600, Train acc: 0.3790\n",
      "Valid loss: 1.4773, Valid acc: 0.3706\n",
      "Epoch 5:\n",
      "Train loss: 1.4372, Train acc: 0.3904\n",
      "Valid loss: 1.4312, Valid acc: 0.3972\n",
      "Epoch 6:\n",
      "Train loss: 1.4109, Train acc: 0.3936\n",
      "Valid loss: 1.4902, Valid acc: 0.3697\n",
      "Epoch 7:\n",
      "Train loss: 1.3685, Train acc: 0.4122\n",
      "Valid loss: 1.4272, Valid acc: 0.3917\n",
      "Epoch 8:\n",
      "Train loss: 1.3674, Train acc: 0.4017\n",
      "Valid loss: 1.3804, Valid acc: 0.4000\n",
      "Epoch 9:\n",
      "Train loss: 1.3016, Train acc: 0.4168\n",
      "Valid loss: 1.3897, Valid acc: 0.3275\n",
      "Epoch 10:\n",
      "Train loss: 1.3851, Train acc: 0.3831\n",
      "Valid loss: 1.4620, Valid acc: 0.3009\n",
      "Epoch 11:\n",
      "Train loss: 1.4067, Train acc: 0.3764\n",
      "Valid loss: 1.4627, Valid acc: 0.3541\n",
      "Epoch 12:\n",
      "Train loss: 1.4024, Train acc: 0.3776\n",
      "Valid loss: 1.4535, Valid acc: 0.3697\n",
      "Epoch 13:\n",
      "Train loss: 1.4204, Train acc: 0.3872\n",
      "Valid loss: 1.4648, Valid acc: 0.3000\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6675, Train acc: 0.2281\n",
      "Valid loss: 1.6680, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6519, Train acc: 0.2217\n",
      "Valid loss: 1.6453, Valid acc: 0.2569\n",
      "Epoch 3:\n",
      "Train loss: 1.6482, Train acc: 0.2331\n",
      "Valid loss: 1.6638, Valid acc: 0.2083\n",
      "Epoch 4:\n",
      "Train loss: 1.6385, Train acc: 0.2519\n",
      "Valid loss: 1.5113, Valid acc: 0.3862\n",
      "Epoch 5:\n",
      "Train loss: 1.5212, Train acc: 0.3569\n",
      "Valid loss: 1.5955, Valid acc: 0.3505\n",
      "Epoch 6:\n",
      "Train loss: 1.4981, Train acc: 0.3723\n",
      "Valid loss: 1.4995, Valid acc: 0.3303\n",
      "Epoch 7:\n",
      "Train loss: 1.5225, Train acc: 0.3434\n",
      "Valid loss: 1.6468, Valid acc: 0.2055\n",
      "Epoch 8:\n",
      "Train loss: 1.6393, Train acc: 0.2244\n",
      "Valid loss: 1.6426, Valid acc: 0.1991\n",
      "Epoch 9:\n",
      "Train loss: 1.6251, Train acc: 0.2430\n",
      "Valid loss: 1.4854, Valid acc: 0.3440\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7186, Train acc: 0.2148\n",
      "Valid loss: 1.8118, Valid acc: 0.1569\n",
      "Epoch 2:\n",
      "Train loss: 1.6794, Train acc: 0.2144\n",
      "Valid loss: 1.6899, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.6750, Train acc: 0.2160\n",
      "Valid loss: 1.7001, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6779, Train acc: 0.2201\n",
      "Valid loss: 1.7546, Valid acc: 0.2394\n",
      "Epoch 5:\n",
      "Train loss: 1.6686, Train acc: 0.2192\n",
      "Valid loss: 1.6834, Valid acc: 0.1688\n",
      "Epoch 6:\n",
      "Train loss: 1.6692, Train acc: 0.2160\n",
      "Valid loss: 1.6826, Valid acc: 0.1963\n",
      "Epoch 7:\n",
      "Train loss: 1.6673, Train acc: 0.2270\n",
      "Valid loss: 1.6506, Valid acc: 0.2431\n",
      "Epoch 8:\n",
      "Train loss: 1.6717, Train acc: 0.2235\n",
      "Valid loss: 1.7220, Valid acc: 0.1954\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6602, Train acc: 0.2272\n",
      "Valid loss: 1.7702, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6520, Train acc: 0.2306\n",
      "Valid loss: 1.6608, Valid acc: 0.2688\n",
      "Epoch 3:\n",
      "Train loss: 1.6009, Train acc: 0.3038\n",
      "Valid loss: 1.6555, Valid acc: 0.2963\n",
      "Epoch 4:\n",
      "Train loss: 1.4666, Train acc: 0.3682\n",
      "Valid loss: 1.5301, Valid acc: 0.3917\n",
      "Epoch 5:\n",
      "Train loss: 1.3883, Train acc: 0.3968\n",
      "Valid loss: 1.3982, Valid acc: 0.4083\n",
      "Epoch 6:\n",
      "Train loss: 1.3878, Train acc: 0.4014\n",
      "Valid loss: 1.4254, Valid acc: 0.4028\n",
      "Epoch 7:\n",
      "Train loss: 1.3888, Train acc: 0.3998\n",
      "Valid loss: 1.3764, Valid acc: 0.4128\n",
      "Epoch 8:\n",
      "Train loss: 1.3866, Train acc: 0.3884\n",
      "Valid loss: 1.3840, Valid acc: 0.3156\n",
      "Epoch 9:\n",
      "Train loss: 1.3926, Train acc: 0.3716\n",
      "Valid loss: 1.3891, Valid acc: 0.4092\n",
      "Epoch 10:\n",
      "Train loss: 1.3778, Train acc: 0.3964\n",
      "Valid loss: 1.3642, Valid acc: 0.4055\n",
      "Epoch 11:\n",
      "Train loss: 1.3670, Train acc: 0.3909\n",
      "Valid loss: 1.3994, Valid acc: 0.4092\n",
      "Epoch 12:\n",
      "Train loss: 1.3751, Train acc: 0.3696\n",
      "Valid loss: 1.4113, Valid acc: 0.3872\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6750, Train acc: 0.2173\n",
      "Valid loss: 1.6678, Valid acc: 0.1514\n",
      "Epoch 2:\n",
      "Train loss: 1.6531, Train acc: 0.2299\n",
      "Valid loss: 1.6520, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6450, Train acc: 0.2311\n",
      "Valid loss: 1.7191, Valid acc: 0.2101\n",
      "Epoch 4:\n",
      "Train loss: 1.6488, Train acc: 0.2350\n",
      "Valid loss: 1.6603, Valid acc: 0.2495\n",
      "Epoch 5:\n",
      "Train loss: 1.6479, Train acc: 0.2481\n",
      "Valid loss: 1.6645, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6432, Train acc: 0.2409\n",
      "Valid loss: 1.6434, Valid acc: 0.2404\n",
      "Epoch 7:\n",
      "Train loss: 1.6404, Train acc: 0.2350\n",
      "Valid loss: 1.6803, Valid acc: 0.2312\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7807, Train acc: 0.2132\n",
      "Valid loss: 1.6552, Valid acc: 0.2440\n",
      "Epoch 2:\n",
      "Train loss: 1.6742, Train acc: 0.2215\n",
      "Valid loss: 1.7611, Valid acc: 0.2349\n",
      "Epoch 3:\n",
      "Train loss: 1.6693, Train acc: 0.2178\n",
      "Valid loss: 1.7335, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.6706, Train acc: 0.2254\n",
      "Valid loss: 1.6741, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6722, Train acc: 0.2148\n",
      "Valid loss: 1.7155, Valid acc: 0.1606\n",
      "Epoch 6:\n",
      "Train loss: 1.6707, Train acc: 0.2192\n",
      "Valid loss: 1.7398, Valid acc: 0.2385\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6714, Train acc: 0.2027\n",
      "Valid loss: 1.7340, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6507, Train acc: 0.2205\n",
      "Valid loss: 1.6541, Valid acc: 0.2358\n",
      "Epoch 3:\n",
      "Train loss: 1.6541, Train acc: 0.2244\n",
      "Valid loss: 1.6519, Valid acc: 0.2376\n",
      "Epoch 4:\n",
      "Train loss: 1.6472, Train acc: 0.2279\n",
      "Valid loss: 1.6553, Valid acc: 0.2055\n",
      "Epoch 5:\n",
      "Train loss: 1.6483, Train acc: 0.2272\n",
      "Valid loss: 1.6556, Valid acc: 0.2376\n",
      "Epoch 6:\n",
      "Train loss: 1.6484, Train acc: 0.2272\n",
      "Valid loss: 1.6577, Valid acc: 0.2450\n",
      "Epoch 7:\n",
      "Train loss: 1.6470, Train acc: 0.2279\n",
      "Valid loss: 1.6587, Valid acc: 0.2394\n",
      "Epoch 8:\n",
      "Train loss: 1.6527, Train acc: 0.2203\n",
      "Valid loss: 1.6523, Valid acc: 0.2376\n",
      "Epoch 9:\n",
      "Train loss: 1.6438, Train acc: 0.2251\n",
      "Valid loss: 1.6565, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6499, Train acc: 0.2286\n",
      "Valid loss: 1.6582, Valid acc: 0.2550\n",
      "Epoch 11:\n",
      "Train loss: 1.6380, Train acc: 0.2414\n",
      "Valid loss: 1.6151, Valid acc: 0.2954\n",
      "Epoch 12:\n",
      "Train loss: 1.5579, Train acc: 0.3228\n",
      "Valid loss: 1.5344, Valid acc: 0.2853\n",
      "Epoch 13:\n",
      "Train loss: 1.4879, Train acc: 0.3416\n",
      "Valid loss: 1.5030, Valid acc: 0.2697\n",
      "Epoch 14:\n",
      "Train loss: 1.4544, Train acc: 0.3721\n",
      "Valid loss: 1.4740, Valid acc: 0.3890\n",
      "Epoch 15:\n",
      "Train loss: 1.4431, Train acc: 0.3819\n",
      "Valid loss: 1.7695, Valid acc: 0.2798\n",
      "Epoch 16:\n",
      "Train loss: 1.4853, Train acc: 0.3824\n",
      "Valid loss: 1.5096, Valid acc: 0.3761\n",
      "Epoch 17:\n",
      "Train loss: 1.5082, Train acc: 0.3608\n",
      "Valid loss: 1.6324, Valid acc: 0.2413\n",
      "Epoch 18:\n",
      "Train loss: 1.5228, Train acc: 0.3320\n",
      "Valid loss: 1.4523, Valid acc: 0.3917\n",
      "Epoch 19:\n",
      "Train loss: 1.4458, Train acc: 0.3906\n",
      "Valid loss: 1.4574, Valid acc: 0.3119\n",
      "Epoch 20:\n",
      "Train loss: 1.4250, Train acc: 0.3870\n",
      "Valid loss: 1.4693, Valid acc: 0.3183\n",
      "Epoch 21:\n",
      "Train loss: 1.4282, Train acc: 0.3902\n",
      "Valid loss: 1.4440, Valid acc: 0.3927\n",
      "Epoch 22:\n",
      "Train loss: 1.4278, Train acc: 0.3927\n",
      "Valid loss: 1.4528, Valid acc: 0.3908\n",
      "Epoch 23:\n",
      "Train loss: 1.4230, Train acc: 0.3939\n",
      "Valid loss: 1.4494, Valid acc: 0.3899\n",
      "Epoch 24:\n",
      "Train loss: 1.4327, Train acc: 0.3927\n",
      "Valid loss: 1.4653, Valid acc: 0.3064\n",
      "Epoch 25:\n",
      "Train loss: 1.4212, Train acc: 0.3904\n",
      "Valid loss: 1.4581, Valid acc: 0.3908\n",
      "Epoch 26:\n",
      "Train loss: 1.4179, Train acc: 0.3952\n",
      "Valid loss: 1.4546, Valid acc: 0.3908\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6807, Train acc: 0.2210\n",
      "Valid loss: 1.7484, Valid acc: 0.1459\n",
      "Epoch 2:\n",
      "Train loss: 1.6550, Train acc: 0.2238\n",
      "Valid loss: 1.6729, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6766, Train acc: 0.2270\n",
      "Valid loss: 1.7275, Valid acc: 0.1798\n",
      "Epoch 4:\n",
      "Train loss: 1.6758, Train acc: 0.2297\n",
      "Valid loss: 1.7042, Valid acc: 0.2202\n",
      "Epoch 5:\n",
      "Train loss: 1.6601, Train acc: 0.2295\n",
      "Valid loss: 1.6589, Valid acc: 0.2486\n",
      "Epoch 6:\n",
      "Train loss: 1.6621, Train acc: 0.2231\n",
      "Valid loss: 1.6665, Valid acc: 0.2202\n",
      "Epoch 7:\n",
      "Train loss: 1.6502, Train acc: 0.2233\n",
      "Valid loss: 1.6701, Valid acc: 0.2312\n",
      "Epoch 8:\n",
      "Train loss: 1.6498, Train acc: 0.2187\n",
      "Valid loss: 1.6533, Valid acc: 0.2459\n",
      "Epoch 9:\n",
      "Train loss: 1.6456, Train acc: 0.2279\n",
      "Valid loss: 1.6643, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6572, Train acc: 0.2281\n",
      "Valid loss: 1.6826, Valid acc: 0.1982\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.001, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.8146, Train acc: 0.2038\n",
      "Valid loss: 1.7332, Valid acc: 0.2275\n",
      "Epoch 2:\n",
      "Train loss: 1.6754, Train acc: 0.2098\n",
      "Valid loss: 1.7380, Valid acc: 0.2183\n",
      "Epoch 3:\n",
      "Train loss: 1.6865, Train acc: 0.2020\n",
      "Valid loss: 1.7052, Valid acc: 0.2156\n",
      "Epoch 4:\n",
      "Train loss: 1.6671, Train acc: 0.2274\n",
      "Valid loss: 1.8153, Valid acc: 0.2459\n",
      "Epoch 5:\n",
      "Train loss: 1.6936, Train acc: 0.2231\n",
      "Valid loss: 1.7172, Valid acc: 0.2073\n",
      "Epoch 6:\n",
      "Train loss: 1.6876, Train acc: 0.2102\n",
      "Valid loss: 1.7102, Valid acc: 0.2009\n",
      "Epoch 7:\n",
      "Train loss: 1.6807, Train acc: 0.1994\n",
      "Valid loss: 1.7298, Valid acc: 0.2138\n",
      "Epoch 8:\n",
      "Train loss: 1.6766, Train acc: 0.2254\n",
      "Valid loss: 1.7135, Valid acc: 0.2128\n",
      "Epoch 9:\n",
      "Train loss: 1.6799, Train acc: 0.2208\n",
      "Valid loss: 1.7025, Valid acc: 0.2358\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7629, Train acc: 0.2194\n",
      "Valid loss: 1.7277, Valid acc: 0.1972\n",
      "Epoch 2:\n",
      "Train loss: 1.7048, Train acc: 0.2185\n",
      "Valid loss: 1.6832, Valid acc: 0.2009\n",
      "Epoch 3:\n",
      "Train loss: 1.6741, Train acc: 0.2221\n",
      "Valid loss: 1.6604, Valid acc: 0.2018\n",
      "Epoch 4:\n",
      "Train loss: 1.6611, Train acc: 0.2162\n",
      "Valid loss: 1.6531, Valid acc: 0.2064\n",
      "Epoch 5:\n",
      "Train loss: 1.6552, Train acc: 0.2233\n",
      "Valid loss: 1.6502, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6524, Train acc: 0.2272\n",
      "Valid loss: 1.6495, Valid acc: 0.2073\n",
      "Epoch 7:\n",
      "Train loss: 1.6511, Train acc: 0.2258\n",
      "Valid loss: 1.6479, Valid acc: 0.2083\n",
      "Epoch 8:\n",
      "Train loss: 1.6510, Train acc: 0.2290\n",
      "Valid loss: 1.6458, Valid acc: 0.2486\n",
      "Epoch 9:\n",
      "Train loss: 1.6489, Train acc: 0.2320\n",
      "Valid loss: 1.6468, Valid acc: 0.2064\n",
      "Epoch 10:\n",
      "Train loss: 1.6495, Train acc: 0.2286\n",
      "Valid loss: 1.6480, Valid acc: 0.2092\n",
      "Epoch 11:\n",
      "Train loss: 1.6490, Train acc: 0.2235\n",
      "Valid loss: 1.6453, Valid acc: 0.2064\n",
      "Epoch 12:\n",
      "Train loss: 1.6496, Train acc: 0.2233\n",
      "Valid loss: 1.6446, Valid acc: 0.2431\n",
      "Epoch 13:\n",
      "Train loss: 1.6471, Train acc: 0.2231\n",
      "Valid loss: 1.6446, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7570, Train acc: 0.2192\n",
      "Valid loss: 1.7252, Valid acc: 0.2376\n",
      "Epoch 2:\n",
      "Train loss: 1.7054, Train acc: 0.2196\n",
      "Valid loss: 1.6840, Valid acc: 0.2385\n",
      "Epoch 3:\n",
      "Train loss: 1.6764, Train acc: 0.2299\n",
      "Valid loss: 1.6626, Valid acc: 0.2431\n",
      "Epoch 4:\n",
      "Train loss: 1.6616, Train acc: 0.2322\n",
      "Valid loss: 1.6550, Valid acc: 0.2064\n",
      "Epoch 5:\n",
      "Train loss: 1.6547, Train acc: 0.2251\n",
      "Valid loss: 1.6501, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.6514, Train acc: 0.2215\n",
      "Valid loss: 1.6489, Valid acc: 0.2055\n",
      "Epoch 7:\n",
      "Train loss: 1.6489, Train acc: 0.2217\n",
      "Valid loss: 1.6495, Valid acc: 0.2092\n",
      "Epoch 8:\n",
      "Train loss: 1.6473, Train acc: 0.2299\n",
      "Valid loss: 1.6515, Valid acc: 0.2055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7605, Train acc: 0.2247\n",
      "Valid loss: 1.7336, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.7076, Train acc: 0.2267\n",
      "Valid loss: 1.6880, Valid acc: 0.2028\n",
      "Epoch 3:\n",
      "Train loss: 1.6736, Train acc: 0.2295\n",
      "Valid loss: 1.6601, Valid acc: 0.2064\n",
      "Epoch 4:\n",
      "Train loss: 1.6581, Train acc: 0.2274\n",
      "Valid loss: 1.6501, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6508, Train acc: 0.2318\n",
      "Valid loss: 1.6488, Valid acc: 0.2083\n",
      "Epoch 6:\n",
      "Train loss: 1.6486, Train acc: 0.2297\n",
      "Valid loss: 1.6459, Valid acc: 0.2431\n",
      "Epoch 7:\n",
      "Train loss: 1.6496, Train acc: 0.2345\n",
      "Valid loss: 1.6491, Valid acc: 0.2459\n",
      "Epoch 8:\n",
      "Train loss: 1.6478, Train acc: 0.2240\n",
      "Valid loss: 1.6464, Valid acc: 0.2459\n",
      "Epoch 9:\n",
      "Train loss: 1.6483, Train acc: 0.2286\n",
      "Valid loss: 1.6491, Valid acc: 0.2413\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7746, Train acc: 0.1614\n",
      "Valid loss: 1.7535, Valid acc: 0.1624\n",
      "Epoch 2:\n",
      "Train loss: 1.7431, Train acc: 0.2082\n",
      "Valid loss: 1.7253, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.7184, Train acc: 0.2249\n",
      "Valid loss: 1.7042, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.7000, Train acc: 0.2283\n",
      "Valid loss: 1.6871, Valid acc: 0.2037\n",
      "Epoch 5:\n",
      "Train loss: 1.6863, Train acc: 0.2258\n",
      "Valid loss: 1.6737, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.6760, Train acc: 0.2233\n",
      "Valid loss: 1.6642, Valid acc: 0.2092\n",
      "Epoch 7:\n",
      "Train loss: 1.6667, Train acc: 0.2260\n",
      "Valid loss: 1.6575, Valid acc: 0.2394\n",
      "Epoch 8:\n",
      "Train loss: 1.6616, Train acc: 0.2270\n",
      "Valid loss: 1.6530, Valid acc: 0.2404\n",
      "Epoch 9:\n",
      "Train loss: 1.6568, Train acc: 0.2254\n",
      "Valid loss: 1.6502, Valid acc: 0.2413\n",
      "Epoch 10:\n",
      "Train loss: 1.6539, Train acc: 0.2272\n",
      "Valid loss: 1.6492, Valid acc: 0.2046\n",
      "Epoch 11:\n",
      "Train loss: 1.6543, Train acc: 0.2286\n",
      "Valid loss: 1.6469, Valid acc: 0.2394\n",
      "Epoch 12:\n",
      "Train loss: 1.6528, Train acc: 0.2265\n",
      "Valid loss: 1.6466, Valid acc: 0.2431\n",
      "Epoch 13:\n",
      "Train loss: 1.6531, Train acc: 0.2240\n",
      "Valid loss: 1.6460, Valid acc: 0.2394\n",
      "Epoch 14:\n",
      "Train loss: 1.6497, Train acc: 0.2263\n",
      "Valid loss: 1.6465, Valid acc: 0.2046\n",
      "Epoch 15:\n",
      "Train loss: 1.6503, Train acc: 0.2244\n",
      "Valid loss: 1.6457, Valid acc: 0.2404\n",
      "Epoch 16:\n",
      "Train loss: 1.6529, Train acc: 0.2205\n",
      "Valid loss: 1.6457, Valid acc: 0.2046\n",
      "Epoch 17:\n",
      "Train loss: 1.6501, Train acc: 0.2281\n",
      "Valid loss: 1.6446, Valid acc: 0.2385\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7705, Train acc: 0.2219\n",
      "Valid loss: 1.7510, Valid acc: 0.1991\n",
      "Epoch 2:\n",
      "Train loss: 1.7442, Train acc: 0.2205\n",
      "Valid loss: 1.7261, Valid acc: 0.2000\n",
      "Epoch 3:\n",
      "Train loss: 1.7220, Train acc: 0.2231\n",
      "Valid loss: 1.7053, Valid acc: 0.2000\n",
      "Epoch 4:\n",
      "Train loss: 1.7023, Train acc: 0.2242\n",
      "Valid loss: 1.6867, Valid acc: 0.2046\n",
      "Epoch 5:\n",
      "Train loss: 1.6872, Train acc: 0.2247\n",
      "Valid loss: 1.6731, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.6766, Train acc: 0.2244\n",
      "Valid loss: 1.6628, Valid acc: 0.2064\n",
      "Epoch 7:\n",
      "Train loss: 1.6670, Train acc: 0.2272\n",
      "Valid loss: 1.6552, Valid acc: 0.2083\n",
      "Epoch 8:\n",
      "Train loss: 1.6595, Train acc: 0.2205\n",
      "Valid loss: 1.6506, Valid acc: 0.2083\n",
      "Epoch 9:\n",
      "Train loss: 1.6566, Train acc: 0.2242\n",
      "Valid loss: 1.6479, Valid acc: 0.2064\n",
      "Epoch 10:\n",
      "Train loss: 1.6551, Train acc: 0.2219\n",
      "Valid loss: 1.6458, Valid acc: 0.2468\n",
      "Epoch 11:\n",
      "Train loss: 1.6532, Train acc: 0.2306\n",
      "Valid loss: 1.6448, Valid acc: 0.2477\n",
      "Epoch 12:\n",
      "Train loss: 1.6508, Train acc: 0.2208\n",
      "Valid loss: 1.6432, Valid acc: 0.2367\n",
      "Epoch 13:\n",
      "Train loss: 1.6517, Train acc: 0.2201\n",
      "Valid loss: 1.6438, Valid acc: 0.2477\n",
      "Epoch 14:\n",
      "Train loss: 1.6500, Train acc: 0.2217\n",
      "Valid loss: 1.6434, Valid acc: 0.2092\n",
      "Epoch 15:\n",
      "Train loss: 1.6504, Train acc: 0.2258\n",
      "Valid loss: 1.6432, Valid acc: 0.2468\n",
      "Epoch 16:\n",
      "Train loss: 1.6495, Train acc: 0.2192\n",
      "Valid loss: 1.6441, Valid acc: 0.2092\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7782, Train acc: 0.2304\n",
      "Valid loss: 1.7667, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.7483, Train acc: 0.2299\n",
      "Valid loss: 1.7389, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.7227, Train acc: 0.2295\n",
      "Valid loss: 1.7142, Valid acc: 0.2450\n",
      "Epoch 4:\n",
      "Train loss: 1.7033, Train acc: 0.2208\n",
      "Valid loss: 1.6927, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6838, Train acc: 0.2274\n",
      "Valid loss: 1.6738, Valid acc: 0.2119\n",
      "Epoch 6:\n",
      "Train loss: 1.6693, Train acc: 0.2178\n",
      "Valid loss: 1.6616, Valid acc: 0.2064\n",
      "Epoch 7:\n",
      "Train loss: 1.6609, Train acc: 0.2276\n",
      "Valid loss: 1.6539, Valid acc: 0.2101\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7782, Train acc: 0.2263\n",
      "Valid loss: 1.7679, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.7597, Train acc: 0.2254\n",
      "Valid loss: 1.7503, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.7436, Train acc: 0.2281\n",
      "Valid loss: 1.7359, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.7299, Train acc: 0.2279\n",
      "Valid loss: 1.7229, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.7180, Train acc: 0.2276\n",
      "Valid loss: 1.7118, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.7082, Train acc: 0.2299\n",
      "Valid loss: 1.7017, Valid acc: 0.2468\n",
      "Epoch 7:\n",
      "Train loss: 1.6971, Train acc: 0.2313\n",
      "Valid loss: 1.6925, Valid acc: 0.2018\n",
      "Epoch 8:\n",
      "Train loss: 1.6888, Train acc: 0.2263\n",
      "Valid loss: 1.6846, Valid acc: 0.2000\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.7735, Train acc: 0.2247\n",
      "Valid loss: 1.7590, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.7569, Train acc: 0.2247\n",
      "Valid loss: 1.7444, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.7448, Train acc: 0.2251\n",
      "Valid loss: 1.7310, Valid acc: 0.2431\n",
      "Epoch 4:\n",
      "Train loss: 1.7296, Train acc: 0.2244\n",
      "Valid loss: 1.7189, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.7186, Train acc: 0.2244\n",
      "Valid loss: 1.7078, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.7092, Train acc: 0.2244\n",
      "Valid loss: 1.6982, Valid acc: 0.2431\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.sgd.SGD'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7841, Train acc: 0.2203\n",
      "Valid loss: 1.7758, Valid acc: 0.2000\n",
      "Epoch 2:\n",
      "Train loss: 1.7638, Train acc: 0.2224\n",
      "Valid loss: 1.7577, Valid acc: 0.2000\n",
      "Epoch 3:\n",
      "Train loss: 1.7504, Train acc: 0.2276\n",
      "Valid loss: 1.7431, Valid acc: 0.2000\n",
      "Epoch 4:\n",
      "Train loss: 1.7331, Train acc: 0.2263\n",
      "Valid loss: 1.7292, Valid acc: 0.2009\n",
      "Epoch 5:\n",
      "Train loss: 1.7203, Train acc: 0.2270\n",
      "Valid loss: 1.7166, Valid acc: 0.2009\n",
      "Epoch 6:\n",
      "Train loss: 1.7088, Train acc: 0.2251\n",
      "Valid loss: 1.7057, Valid acc: 0.2009\n",
      "Epoch 7:\n",
      "Train loss: 1.6997, Train acc: 0.2235\n",
      "Valid loss: 1.6960, Valid acc: 0.2009\n",
      "Epoch 8:\n",
      "Train loss: 1.6889, Train acc: 0.2242\n",
      "Valid loss: 1.6870, Valid acc: 0.2009\n",
      "Epoch 9:\n",
      "Train loss: 1.6828, Train acc: 0.2260\n",
      "Valid loss: 1.6792, Valid acc: 0.2009\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6580, Train acc: 0.2368\n",
      "Valid loss: 1.6538, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6465, Train acc: 0.2315\n",
      "Valid loss: 1.6430, Valid acc: 0.2450\n",
      "Epoch 3:\n",
      "Train loss: 1.6428, Train acc: 0.2327\n",
      "Valid loss: 1.6528, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.6464, Train acc: 0.2354\n",
      "Valid loss: 1.6419, Valid acc: 0.2642\n",
      "Epoch 5:\n",
      "Train loss: 1.6071, Train acc: 0.2737\n",
      "Valid loss: 1.4584, Valid acc: 0.3725\n",
      "Epoch 6:\n",
      "Train loss: 1.3455, Train acc: 0.4200\n",
      "Valid loss: 1.3014, Valid acc: 0.4523\n",
      "Epoch 7:\n",
      "Train loss: 1.1455, Train acc: 0.5293\n",
      "Valid loss: 1.1775, Valid acc: 0.5147\n",
      "Epoch 8:\n",
      "Train loss: 1.0153, Train acc: 0.5995\n",
      "Valid loss: 1.3048, Valid acc: 0.4817\n",
      "Epoch 9:\n",
      "Train loss: 0.9066, Train acc: 0.6481\n",
      "Valid loss: 1.0379, Valid acc: 0.5908\n",
      "Epoch 10:\n",
      "Train loss: 0.8075, Train acc: 0.7006\n",
      "Valid loss: 0.9702, Valid acc: 0.6128\n",
      "Epoch 11:\n",
      "Train loss: 0.7260, Train acc: 0.7290\n",
      "Valid loss: 0.9892, Valid acc: 0.6138\n",
      "Epoch 12:\n",
      "Train loss: 0.6644, Train acc: 0.7529\n",
      "Valid loss: 0.9906, Valid acc: 0.6193\n",
      "Epoch 13:\n",
      "Train loss: 0.6059, Train acc: 0.7694\n",
      "Valid loss: 0.9300, Valid acc: 0.6349\n",
      "Epoch 14:\n",
      "Train loss: 0.5700, Train acc: 0.7783\n",
      "Valid loss: 1.0388, Valid acc: 0.6193\n",
      "Epoch 15:\n",
      "Train loss: 0.5587, Train acc: 0.7824\n",
      "Valid loss: 1.0037, Valid acc: 0.6202\n",
      "Epoch 16:\n",
      "Train loss: 0.5174, Train acc: 0.7941\n",
      "Valid loss: 0.9246, Valid acc: 0.6486\n",
      "Epoch 17:\n",
      "Train loss: 0.4911, Train acc: 0.7999\n",
      "Valid loss: 0.9629, Valid acc: 0.6486\n",
      "Epoch 18:\n",
      "Train loss: 0.4806, Train acc: 0.8012\n",
      "Valid loss: 0.9623, Valid acc: 0.6468\n",
      "Epoch 19:\n",
      "Train loss: 0.4707, Train acc: 0.8031\n",
      "Valid loss: 0.9813, Valid acc: 0.6495\n",
      "Epoch 20:\n",
      "Train loss: 0.4573, Train acc: 0.8049\n",
      "Valid loss: 0.9834, Valid acc: 0.6569\n",
      "Epoch 21:\n",
      "Train loss: 0.4496, Train acc: 0.8072\n",
      "Valid loss: 0.9656, Valid acc: 0.6615\n",
      "Epoch 22:\n",
      "Train loss: 0.4514, Train acc: 0.8035\n",
      "Valid loss: 0.9700, Valid acc: 0.6587\n",
      "Epoch 23:\n",
      "Train loss: 0.4289, Train acc: 0.8099\n",
      "Valid loss: 1.0079, Valid acc: 0.6495\n",
      "Epoch 24:\n",
      "Train loss: 0.4170, Train acc: 0.8145\n",
      "Valid loss: 0.9692, Valid acc: 0.6633\n",
      "Epoch 25:\n",
      "Train loss: 0.4048, Train acc: 0.8166\n",
      "Valid loss: 1.0786, Valid acc: 0.6349\n",
      "Epoch 26:\n",
      "Train loss: 0.4123, Train acc: 0.8136\n",
      "Valid loss: 0.9833, Valid acc: 0.6615\n",
      "Epoch 27:\n",
      "Train loss: 0.3978, Train acc: 0.8180\n",
      "Valid loss: 0.9764, Valid acc: 0.6716\n",
      "Epoch 28:\n",
      "Train loss: 0.3947, Train acc: 0.8189\n",
      "Valid loss: 1.0214, Valid acc: 0.6569\n",
      "Epoch 29:\n",
      "Train loss: 0.4034, Train acc: 0.8150\n",
      "Valid loss: 1.0205, Valid acc: 0.6615\n",
      "Epoch 30:\n",
      "Train loss: 0.5360, Train acc: 0.7843\n",
      "Valid loss: 1.0641, Valid acc: 0.6523\n",
      "Epoch 31:\n",
      "Train loss: 0.4213, Train acc: 0.8111\n",
      "Valid loss: 1.0136, Valid acc: 0.6550\n",
      "Epoch 32:\n",
      "Train loss: 0.3930, Train acc: 0.8191\n",
      "Valid loss: 0.9957, Valid acc: 0.6670\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6580, Train acc: 0.2231\n",
      "Valid loss: 1.6733, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6450, Train acc: 0.2334\n",
      "Valid loss: 1.6554, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.5744, Train acc: 0.2980\n",
      "Valid loss: 1.4361, Valid acc: 0.4009\n",
      "Epoch 4:\n",
      "Train loss: 1.5244, Train acc: 0.3597\n",
      "Valid loss: 1.6089, Valid acc: 0.3422\n",
      "Epoch 5:\n",
      "Train loss: 1.4972, Train acc: 0.3670\n",
      "Valid loss: 1.4613, Valid acc: 0.3752\n",
      "Epoch 6:\n",
      "Train loss: 1.4831, Train acc: 0.3443\n",
      "Valid loss: 1.4830, Valid acc: 0.3661\n",
      "Epoch 7:\n",
      "Train loss: 1.4765, Train acc: 0.3519\n",
      "Valid loss: 1.4666, Valid acc: 0.3899\n",
      "Epoch 8:\n",
      "Train loss: 1.4701, Train acc: 0.3508\n",
      "Valid loss: 1.4689, Valid acc: 0.3725\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6692, Train acc: 0.2180\n",
      "Valid loss: 1.6437, Valid acc: 0.2413\n",
      "Epoch 2:\n",
      "Train loss: 1.6464, Train acc: 0.2343\n",
      "Valid loss: 1.6434, Valid acc: 0.2349\n",
      "Epoch 3:\n",
      "Train loss: 1.6419, Train acc: 0.2478\n",
      "Valid loss: 1.6477, Valid acc: 0.2367\n",
      "Epoch 4:\n",
      "Train loss: 1.6223, Train acc: 0.2687\n",
      "Valid loss: 1.6049, Valid acc: 0.3734\n",
      "Epoch 5:\n",
      "Train loss: 1.5277, Train acc: 0.3455\n",
      "Valid loss: 1.5764, Valid acc: 0.3239\n",
      "Epoch 6:\n",
      "Train loss: 1.5009, Train acc: 0.3505\n",
      "Valid loss: 1.5124, Valid acc: 0.3661\n",
      "Epoch 7:\n",
      "Train loss: 1.5089, Train acc: 0.3310\n",
      "Valid loss: 1.5099, Valid acc: 0.3330\n",
      "Epoch 8:\n",
      "Train loss: 1.4930, Train acc: 0.3402\n",
      "Valid loss: 1.5177, Valid acc: 0.3349\n",
      "Epoch 9:\n",
      "Train loss: 1.4901, Train acc: 0.3317\n",
      "Valid loss: 1.5065, Valid acc: 0.3514\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6610, Train acc: 0.2164\n",
      "Valid loss: 1.6573, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6496, Train acc: 0.2295\n",
      "Valid loss: 1.6519, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6496, Train acc: 0.2320\n",
      "Valid loss: 1.6448, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6461, Train acc: 0.2368\n",
      "Valid loss: 1.6456, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6448, Train acc: 0.2276\n",
      "Valid loss: 1.6510, Valid acc: 0.2541\n",
      "Epoch 6:\n",
      "Train loss: 1.6305, Train acc: 0.2604\n",
      "Valid loss: 1.4973, Valid acc: 0.3927\n",
      "Epoch 7:\n",
      "Train loss: 1.4292, Train acc: 0.4053\n",
      "Valid loss: 1.4413, Valid acc: 0.4046\n",
      "Epoch 8:\n",
      "Train loss: 1.3575, Train acc: 0.4131\n",
      "Valid loss: 1.3972, Valid acc: 0.4211\n",
      "Epoch 9:\n",
      "Train loss: 1.3002, Train acc: 0.4287\n",
      "Valid loss: 1.4526, Valid acc: 0.4009\n",
      "Epoch 10:\n",
      "Train loss: 1.1709, Train acc: 0.4888\n",
      "Valid loss: 1.4674, Valid acc: 0.4193\n",
      "Epoch 11:\n",
      "Train loss: 1.0590, Train acc: 0.5481\n",
      "Valid loss: 1.3037, Valid acc: 0.4890\n",
      "Epoch 12:\n",
      "Train loss: 0.9746, Train acc: 0.5818\n",
      "Valid loss: 1.3520, Valid acc: 0.4734\n",
      "Epoch 13:\n",
      "Train loss: 0.9077, Train acc: 0.6009\n",
      "Valid loss: 1.3726, Valid acc: 0.4963\n",
      "Epoch 14:\n",
      "Train loss: 0.8517, Train acc: 0.6201\n",
      "Valid loss: 1.2944, Valid acc: 0.5092\n",
      "Epoch 15:\n",
      "Train loss: 0.8319, Train acc: 0.6263\n",
      "Valid loss: 1.1926, Valid acc: 0.5394\n",
      "Epoch 16:\n",
      "Train loss: 0.8052, Train acc: 0.6325\n",
      "Valid loss: 1.4069, Valid acc: 0.4872\n",
      "Epoch 17:\n",
      "Train loss: 0.7998, Train acc: 0.6309\n",
      "Valid loss: 1.2131, Valid acc: 0.5330\n",
      "Epoch 18:\n",
      "Train loss: 0.7755, Train acc: 0.6405\n",
      "Valid loss: 1.5655, Valid acc: 0.4578\n",
      "Epoch 19:\n",
      "Train loss: 0.7611, Train acc: 0.6428\n",
      "Valid loss: 1.2959, Valid acc: 0.5257\n",
      "Epoch 20:\n",
      "Train loss: 0.7876, Train acc: 0.6362\n",
      "Valid loss: 1.1776, Valid acc: 0.5459\n",
      "Epoch 21:\n",
      "Train loss: 0.7383, Train acc: 0.6486\n",
      "Valid loss: 1.2951, Valid acc: 0.5321\n",
      "Epoch 22:\n",
      "Train loss: 0.7339, Train acc: 0.6492\n",
      "Valid loss: 1.2854, Valid acc: 0.5303\n",
      "Epoch 23:\n",
      "Train loss: 0.7311, Train acc: 0.6474\n",
      "Valid loss: 1.2890, Valid acc: 0.5367\n",
      "Epoch 24:\n",
      "Train loss: 0.7361, Train acc: 0.6460\n",
      "Valid loss: 1.2706, Valid acc: 0.5422\n",
      "Epoch 25:\n",
      "Train loss: 0.7345, Train acc: 0.6483\n",
      "Valid loss: 1.3801, Valid acc: 0.5165\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6699, Train acc: 0.2242\n",
      "Valid loss: 1.6536, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6531, Train acc: 0.2210\n",
      "Valid loss: 1.6492, Valid acc: 0.2064\n",
      "Epoch 3:\n",
      "Train loss: 1.6501, Train acc: 0.2288\n",
      "Valid loss: 1.6551, Valid acc: 0.2055\n",
      "Epoch 4:\n",
      "Train loss: 1.6439, Train acc: 0.2485\n",
      "Valid loss: 1.6368, Valid acc: 0.2697\n",
      "Epoch 5:\n",
      "Train loss: 1.6378, Train acc: 0.2524\n",
      "Valid loss: 1.6801, Valid acc: 0.1505\n",
      "Epoch 6:\n",
      "Train loss: 1.5716, Train acc: 0.2918\n",
      "Valid loss: 1.4395, Valid acc: 0.3872\n",
      "Epoch 7:\n",
      "Train loss: 1.4770, Train acc: 0.3622\n",
      "Valid loss: 1.5144, Valid acc: 0.3761\n",
      "Epoch 8:\n",
      "Train loss: 1.4820, Train acc: 0.3498\n",
      "Valid loss: 1.4767, Valid acc: 0.3477\n",
      "Epoch 9:\n",
      "Train loss: 1.4466, Train acc: 0.3576\n",
      "Valid loss: 1.4668, Valid acc: 0.3963\n",
      "Epoch 10:\n",
      "Train loss: 1.4292, Train acc: 0.3831\n",
      "Valid loss: 1.4753, Valid acc: 0.3872\n",
      "Epoch 11:\n",
      "Train loss: 1.3970, Train acc: 0.4026\n",
      "Valid loss: 1.4372, Valid acc: 0.3954\n",
      "Epoch 12:\n",
      "Train loss: 1.4059, Train acc: 0.3879\n",
      "Valid loss: 1.4363, Valid acc: 0.3881\n",
      "Epoch 13:\n",
      "Train loss: 1.4021, Train acc: 0.3865\n",
      "Valid loss: 1.4385, Valid acc: 0.3853\n",
      "Epoch 14:\n",
      "Train loss: 1.4099, Train acc: 0.3801\n",
      "Valid loss: 1.5024, Valid acc: 0.3881\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6755, Train acc: 0.2279\n",
      "Valid loss: 1.6662, Valid acc: 0.2064\n",
      "Epoch 2:\n",
      "Train loss: 1.6479, Train acc: 0.2348\n",
      "Valid loss: 1.6325, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.6449, Train acc: 0.2384\n",
      "Valid loss: 1.6390, Valid acc: 0.2156\n",
      "Epoch 4:\n",
      "Train loss: 1.6487, Train acc: 0.2272\n",
      "Valid loss: 1.6451, Valid acc: 0.2587\n",
      "Epoch 5:\n",
      "Train loss: 1.5862, Train acc: 0.3221\n",
      "Valid loss: 1.6965, Valid acc: 0.2587\n",
      "Epoch 6:\n",
      "Train loss: 1.6042, Train acc: 0.2671\n",
      "Valid loss: 1.5591, Valid acc: 0.2752\n",
      "Epoch 7:\n",
      "Train loss: 1.5359, Train acc: 0.3276\n",
      "Valid loss: 1.5025, Valid acc: 0.3688\n",
      "Epoch 8:\n",
      "Train loss: 1.5284, Train acc: 0.3572\n",
      "Valid loss: 1.5326, Valid acc: 0.3413\n",
      "Epoch 9:\n",
      "Train loss: 1.5154, Train acc: 0.3526\n",
      "Valid loss: 1.4606, Valid acc: 0.3679\n",
      "Epoch 10:\n",
      "Train loss: 1.5084, Train acc: 0.3475\n",
      "Valid loss: 1.4903, Valid acc: 0.3073\n",
      "Epoch 11:\n",
      "Train loss: 1.6063, Train acc: 0.2822\n",
      "Valid loss: 1.6625, Valid acc: 0.2046\n",
      "Epoch 12:\n",
      "Train loss: 1.6474, Train acc: 0.2350\n",
      "Valid loss: 1.6301, Valid acc: 0.2431\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6669, Train acc: 0.2231\n",
      "Valid loss: 1.6573, Valid acc: 0.2459\n",
      "Epoch 2:\n",
      "Train loss: 1.6514, Train acc: 0.2286\n",
      "Valid loss: 1.6592, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.6486, Train acc: 0.2244\n",
      "Valid loss: 1.6498, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6509, Train acc: 0.2228\n",
      "Valid loss: 1.6592, Valid acc: 0.2486\n",
      "Epoch 5:\n",
      "Train loss: 1.6476, Train acc: 0.2313\n",
      "Valid loss: 1.6539, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6612, Train acc: 0.2208\n",
      "Valid loss: 1.6517, Valid acc: 0.2385\n",
      "Epoch 7:\n",
      "Train loss: 1.6453, Train acc: 0.2320\n",
      "Valid loss: 1.6594, Valid acc: 0.2037\n",
      "Epoch 8:\n",
      "Train loss: 1.6486, Train acc: 0.2194\n",
      "Valid loss: 1.6550, Valid acc: 0.2046\n",
      "Epoch 9:\n",
      "Train loss: 1.6454, Train acc: 0.2251\n",
      "Valid loss: 1.6520, Valid acc: 0.2459\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6685, Train acc: 0.2125\n",
      "Valid loss: 1.7122, Valid acc: 0.2028\n",
      "Epoch 2:\n",
      "Train loss: 1.6577, Train acc: 0.2249\n",
      "Valid loss: 1.6715, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6575, Train acc: 0.2240\n",
      "Valid loss: 1.6515, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.6539, Train acc: 0.2256\n",
      "Valid loss: 1.6541, Valid acc: 0.2468\n",
      "Epoch 5:\n",
      "Train loss: 1.6514, Train acc: 0.2219\n",
      "Valid loss: 1.6580, Valid acc: 0.2404\n",
      "Epoch 6:\n",
      "Train loss: 1.6599, Train acc: 0.2272\n",
      "Valid loss: 1.6567, Valid acc: 0.2495\n",
      "Epoch 7:\n",
      "Train loss: 1.6464, Train acc: 0.2331\n",
      "Valid loss: 1.6793, Valid acc: 0.2413\n",
      "Epoch 8:\n",
      "Train loss: 1.6597, Train acc: 0.2368\n",
      "Valid loss: 1.6500, Valid acc: 0.2486\n",
      "Epoch 9:\n",
      "Train loss: 1.6412, Train acc: 0.2657\n",
      "Valid loss: 1.6703, Valid acc: 0.2459\n",
      "Epoch 10:\n",
      "Train loss: 1.5682, Train acc: 0.3223\n",
      "Valid loss: 1.7291, Valid acc: 0.2789\n",
      "Epoch 11:\n",
      "Train loss: 1.4724, Train acc: 0.3746\n",
      "Valid loss: 1.5152, Valid acc: 0.3505\n",
      "Epoch 12:\n",
      "Train loss: 1.4906, Train acc: 0.3320\n",
      "Valid loss: 1.4846, Valid acc: 0.3587\n",
      "Epoch 13:\n",
      "Train loss: 1.4758, Train acc: 0.3304\n",
      "Valid loss: 1.4787, Valid acc: 0.3532\n",
      "Epoch 14:\n",
      "Train loss: 1.4634, Train acc: 0.3377\n",
      "Valid loss: 1.4905, Valid acc: 0.3560\n",
      "Epoch 15:\n",
      "Train loss: 1.4614, Train acc: 0.3459\n",
      "Valid loss: 1.4939, Valid acc: 0.3404\n",
      "Epoch 16:\n",
      "Train loss: 1.4523, Train acc: 0.3466\n",
      "Valid loss: 1.4682, Valid acc: 0.3550\n",
      "Epoch 17:\n",
      "Train loss: 1.4554, Train acc: 0.3455\n",
      "Valid loss: 1.4589, Valid acc: 0.3560\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adagrad.Adagrad'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.6801, Train acc: 0.2240\n",
      "Valid loss: 1.6569, Valid acc: 0.2440\n",
      "Epoch 2:\n",
      "Train loss: 1.6519, Train acc: 0.2244\n",
      "Valid loss: 1.7440, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6615, Train acc: 0.2267\n",
      "Valid loss: 1.6771, Valid acc: 0.2404\n",
      "Epoch 4:\n",
      "Train loss: 1.6549, Train acc: 0.2238\n",
      "Valid loss: 1.6748, Valid acc: 0.2431\n",
      "Epoch 5:\n",
      "Train loss: 1.6506, Train acc: 0.2226\n",
      "Valid loss: 1.6584, Valid acc: 0.2468\n",
      "Epoch 6:\n",
      "Train loss: 1.6570, Train acc: 0.2341\n",
      "Valid loss: 1.6933, Valid acc: 0.2486\n",
      "Epoch 7:\n",
      "Train loss: 1.6512, Train acc: 0.2295\n",
      "Valid loss: 1.6503, Valid acc: 0.2440\n",
      "Epoch 8:\n",
      "Train loss: 1.6438, Train acc: 0.2242\n",
      "Valid loss: 1.6665, Valid acc: 0.2431\n",
      "Epoch 9:\n",
      "Train loss: 1.6478, Train acc: 0.2373\n",
      "Valid loss: 1.6594, Valid acc: 0.2046\n",
      "Epoch 10:\n",
      "Train loss: 1.6665, Train acc: 0.2137\n",
      "Valid loss: 1.7143, Valid acc: 0.2046\n",
      "Epoch 11:\n",
      "Train loss: 1.6529, Train acc: 0.2244\n",
      "Valid loss: 1.6538, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6718, Train acc: 0.2137\n",
      "Valid loss: 1.6644, Valid acc: 0.2358\n",
      "Epoch 2:\n",
      "Train loss: 1.6561, Train acc: 0.2263\n",
      "Valid loss: 1.6622, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6527, Train acc: 0.2231\n",
      "Valid loss: 1.6632, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6512, Train acc: 0.2208\n",
      "Valid loss: 1.6465, Valid acc: 0.2440\n",
      "Epoch 5:\n",
      "Train loss: 1.6505, Train acc: 0.2377\n",
      "Valid loss: 1.6386, Valid acc: 0.2495\n",
      "Epoch 6:\n",
      "Train loss: 1.6440, Train acc: 0.2359\n",
      "Valid loss: 1.6800, Valid acc: 0.2128\n",
      "Epoch 7:\n",
      "Train loss: 1.7027, Train acc: 0.2185\n",
      "Valid loss: 1.6954, Valid acc: 0.2450\n",
      "Epoch 8:\n",
      "Train loss: 1.6824, Train acc: 0.2306\n",
      "Valid loss: 1.6664, Valid acc: 0.2394\n",
      "Epoch 9:\n",
      "Train loss: 1.6793, Train acc: 0.2242\n",
      "Valid loss: 1.6995, Valid acc: 0.2404\n",
      "Epoch 10:\n",
      "Train loss: 1.6585, Train acc: 0.2391\n",
      "Valid loss: 1.6741, Valid acc: 0.1560\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6872, Train acc: 0.2015\n",
      "Valid loss: 1.6953, Valid acc: 0.2339\n",
      "Epoch 2:\n",
      "Train loss: 1.6981, Train acc: 0.2210\n",
      "Valid loss: 1.7024, Valid acc: 0.2174\n",
      "Epoch 3:\n",
      "Train loss: 1.7735, Train acc: 0.2203\n",
      "Valid loss: 1.9717, Valid acc: 0.1982\n",
      "Epoch 4:\n",
      "Train loss: 1.7559, Train acc: 0.2281\n",
      "Valid loss: 1.6988, Valid acc: 0.2092\n",
      "Epoch 5:\n",
      "Train loss: 1.7349, Train acc: 0.2201\n",
      "Valid loss: 1.7751, Valid acc: 0.2083\n",
      "Epoch 6:\n",
      "Train loss: 1.7471, Train acc: 0.2031\n",
      "Valid loss: 1.6605, Valid acc: 0.2174\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.7910, Train acc: 0.2109\n",
      "Valid loss: 1.7314, Valid acc: 0.1642\n",
      "Epoch 2:\n",
      "Train loss: 1.8115, Train acc: 0.2137\n",
      "Valid loss: 1.6760, Valid acc: 0.2486\n",
      "Epoch 3:\n",
      "Train loss: 1.8555, Train acc: 0.2118\n",
      "Valid loss: 1.7305, Valid acc: 0.2092\n",
      "Epoch 4:\n",
      "Train loss: 1.7979, Train acc: 0.1981\n",
      "Valid loss: 2.0123, Valid acc: 0.2330\n",
      "Epoch 5:\n",
      "Train loss: 1.8528, Train acc: 0.1978\n",
      "Valid loss: 1.9379, Valid acc: 0.2009\n",
      "Epoch 6:\n",
      "Train loss: 1.7603, Train acc: 0.2185\n",
      "Valid loss: 1.8011, Valid acc: 0.2486\n",
      "Epoch 7:\n",
      "Train loss: 1.8209, Train acc: 0.2050\n",
      "Valid loss: 1.6925, Valid acc: 0.2275\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6727, Train acc: 0.2185\n",
      "Valid loss: 1.6491, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.6560, Train acc: 0.2228\n",
      "Valid loss: 1.6479, Valid acc: 0.2064\n",
      "Epoch 3:\n",
      "Train loss: 1.6557, Train acc: 0.2288\n",
      "Valid loss: 1.6611, Valid acc: 0.2000\n",
      "Epoch 4:\n",
      "Train loss: 1.6558, Train acc: 0.2221\n",
      "Valid loss: 1.6378, Valid acc: 0.2083\n",
      "Epoch 5:\n",
      "Train loss: 1.6475, Train acc: 0.2276\n",
      "Valid loss: 1.6365, Valid acc: 0.2055\n",
      "Epoch 6:\n",
      "Train loss: 1.6540, Train acc: 0.2224\n",
      "Valid loss: 1.6521, Valid acc: 0.2009\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6749, Train acc: 0.2240\n",
      "Valid loss: 1.6662, Valid acc: 0.2147\n",
      "Epoch 2:\n",
      "Train loss: 1.6631, Train acc: 0.2270\n",
      "Valid loss: 1.6630, Valid acc: 0.2174\n",
      "Epoch 3:\n",
      "Train loss: 1.6614, Train acc: 0.2180\n",
      "Valid loss: 1.6625, Valid acc: 0.2028\n",
      "Epoch 4:\n",
      "Train loss: 1.7180, Train acc: 0.2155\n",
      "Valid loss: 1.7714, Valid acc: 0.1761\n",
      "Epoch 5:\n",
      "Train loss: 1.6901, Train acc: 0.2130\n",
      "Valid loss: 1.6688, Valid acc: 0.2330\n",
      "Epoch 6:\n",
      "Train loss: 1.6868, Train acc: 0.2270\n",
      "Valid loss: 1.6843, Valid acc: 0.2459\n",
      "Epoch 7:\n",
      "Train loss: 1.6785, Train acc: 0.2185\n",
      "Valid loss: 1.6955, Valid acc: 0.2339\n",
      "Epoch 8:\n",
      "Train loss: 1.6947, Train acc: 0.2203\n",
      "Valid loss: 1.6706, Valid acc: 0.2202\n",
      "Epoch 9:\n",
      "Train loss: 1.6995, Train acc: 0.2263\n",
      "Valid loss: 1.6992, Valid acc: 0.1679\n",
      "Epoch 10:\n",
      "Train loss: 1.6908, Train acc: 0.2178\n",
      "Valid loss: 1.7217, Valid acc: 0.2156\n",
      "Epoch 11:\n",
      "Train loss: 1.6779, Train acc: 0.2228\n",
      "Valid loss: 1.6686, Valid acc: 0.2138\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.9255, Train acc: 0.1701\n",
      "Valid loss: 1.9556, Valid acc: 0.1606\n",
      "Epoch 2:\n",
      "Train loss: 1.8519, Train acc: 0.1972\n",
      "Valid loss: 1.7405, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.8335, Train acc: 0.2114\n",
      "Valid loss: 1.7618, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.8036, Train acc: 0.2228\n",
      "Valid loss: 1.8148, Valid acc: 0.1688\n",
      "Epoch 5:\n",
      "Train loss: 1.7800, Train acc: 0.2155\n",
      "Valid loss: 1.8039, Valid acc: 0.2495\n",
      "Epoch 6:\n",
      "Train loss: 1.8297, Train acc: 0.2013\n",
      "Valid loss: 1.6918, Valid acc: 0.2275\n",
      "Epoch 7:\n",
      "Train loss: 1.7919, Train acc: 0.2036\n",
      "Valid loss: 1.6655, Valid acc: 0.2587\n",
      "Epoch 8:\n",
      "Train loss: 1.7792, Train acc: 0.2063\n",
      "Valid loss: 1.7052, Valid acc: 0.1982\n",
      "Epoch 9:\n",
      "Train loss: 1.8715, Train acc: 0.2070\n",
      "Valid loss: 2.0115, Valid acc: 0.1532\n",
      "Epoch 10:\n",
      "Train loss: 1.7886, Train acc: 0.2072\n",
      "Valid loss: 1.7887, Valid acc: 0.2257\n",
      "Epoch 11:\n",
      "Train loss: 1.8164, Train acc: 0.2059\n",
      "Valid loss: 1.7198, Valid acc: 0.2560\n",
      "Epoch 12:\n",
      "Train loss: 1.7635, Train acc: 0.2027\n",
      "Valid loss: 1.8469, Valid acc: 0.2413\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.6713, Train acc: 0.2180\n",
      "Valid loss: 1.6537, Valid acc: 0.2367\n",
      "Epoch 2:\n",
      "Train loss: 1.6578, Train acc: 0.2263\n",
      "Valid loss: 1.6522, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6571, Train acc: 0.2242\n",
      "Valid loss: 1.6758, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.6548, Train acc: 0.2205\n",
      "Valid loss: 1.6488, Valid acc: 0.2367\n",
      "Epoch 5:\n",
      "Train loss: 1.6587, Train acc: 0.2088\n",
      "Valid loss: 1.6540, Valid acc: 0.2220\n",
      "Epoch 6:\n",
      "Train loss: 1.6582, Train acc: 0.2238\n",
      "Valid loss: 1.6608, Valid acc: 0.2009\n",
      "Epoch 7:\n",
      "Train loss: 1.6531, Train acc: 0.2178\n",
      "Valid loss: 1.6524, Valid acc: 0.2440\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.6877, Train acc: 0.2153\n",
      "Valid loss: 1.6794, Valid acc: 0.2477\n",
      "Epoch 2:\n",
      "Train loss: 1.6649, Train acc: 0.2171\n",
      "Valid loss: 1.6630, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6584, Train acc: 0.2185\n",
      "Valid loss: 1.6731, Valid acc: 0.2037\n",
      "Epoch 4:\n",
      "Train loss: 1.6561, Train acc: 0.2173\n",
      "Valid loss: 1.6616, Valid acc: 0.2294\n",
      "Epoch 5:\n",
      "Train loss: 1.6632, Train acc: 0.2258\n",
      "Valid loss: 1.6540, Valid acc: 0.2431\n",
      "Epoch 6:\n",
      "Train loss: 1.6570, Train acc: 0.2276\n",
      "Valid loss: 1.6557, Valid acc: 0.2477\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.adam.Adam'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 1.8148, Train acc: 0.2013\n",
      "Valid loss: 1.7414, Valid acc: 0.1505\n",
      "Epoch 2:\n",
      "Train loss: 1.7266, Train acc: 0.2052\n",
      "Valid loss: 1.6691, Valid acc: 0.2514\n",
      "Epoch 3:\n",
      "Train loss: 1.7747, Train acc: 0.1944\n",
      "Valid loss: 1.7290, Valid acc: 0.2284\n",
      "Epoch 4:\n",
      "Train loss: 1.6993, Train acc: 0.2185\n",
      "Valid loss: 1.7136, Valid acc: 0.2009\n",
      "Epoch 5:\n",
      "Train loss: 1.6798, Train acc: 0.2180\n",
      "Valid loss: 1.6788, Valid acc: 0.2303\n",
      "Epoch 6:\n",
      "Train loss: 1.7005, Train acc: 0.2127\n",
      "Valid loss: 1.6762, Valid acc: 0.1881\n",
      "Epoch 7:\n",
      "Train loss: 1.6864, Train acc: 0.2061\n",
      "Valid loss: 1.8026, Valid acc: 0.2321\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7281, Train acc: 0.2263\n",
      "Valid loss: 1.6716, Valid acc: 0.2422\n",
      "Epoch 2:\n",
      "Train loss: 1.6788, Train acc: 0.2286\n",
      "Valid loss: 1.7167, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.6828, Train acc: 0.2203\n",
      "Valid loss: 1.6948, Valid acc: 0.2450\n",
      "Epoch 4:\n",
      "Train loss: 1.6767, Train acc: 0.2302\n",
      "Valid loss: 1.6721, Valid acc: 0.2486\n",
      "Epoch 5:\n",
      "Train loss: 1.6848, Train acc: 0.2210\n",
      "Valid loss: 1.8738, Valid acc: 0.1477\n",
      "Epoch 6:\n",
      "Train loss: 1.6808, Train acc: 0.2182\n",
      "Valid loss: 1.7553, Valid acc: 0.2404\n",
      "Epoch 7:\n",
      "Train loss: 1.6614, Train acc: 0.2421\n",
      "Valid loss: 1.7409, Valid acc: 0.2018\n",
      "Epoch 8:\n",
      "Train loss: 1.6712, Train acc: 0.2173\n",
      "Valid loss: 1.7747, Valid acc: 0.2367\n",
      "Epoch 9:\n",
      "Train loss: 1.6676, Train acc: 0.2249\n",
      "Valid loss: 1.7383, Valid acc: 0.2367\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.8431, Train acc: 0.2634\n",
      "Valid loss: 1.8266, Valid acc: 0.3440\n",
      "Epoch 2:\n",
      "Train loss: 1.6188, Train acc: 0.3049\n",
      "Valid loss: 1.6855, Valid acc: 0.3486\n",
      "Epoch 3:\n",
      "Train loss: 1.5954, Train acc: 0.3113\n",
      "Valid loss: 1.8479, Valid acc: 0.3450\n",
      "Epoch 4:\n",
      "Train loss: 1.6145, Train acc: 0.2930\n",
      "Valid loss: 1.7978, Valid acc: 0.3413\n",
      "Epoch 5:\n",
      "Train loss: 1.6118, Train acc: 0.3031\n",
      "Valid loss: 1.8416, Valid acc: 0.2046\n",
      "Epoch 6:\n",
      "Train loss: 1.5947, Train acc: 0.3090\n",
      "Valid loss: 1.5805, Valid acc: 0.3477\n",
      "Epoch 7:\n",
      "Train loss: 1.5916, Train acc: 0.3099\n",
      "Valid loss: 1.6653, Valid acc: 0.3468\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 32, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 2.2525, Train acc: 0.2001\n",
      "Valid loss: 2.0095, Valid acc: 0.2083\n",
      "Epoch 2:\n",
      "Train loss: 1.8436, Train acc: 0.2625\n",
      "Valid loss: 1.7283, Valid acc: 0.2927\n",
      "Epoch 3:\n",
      "Train loss: 1.8068, Train acc: 0.2719\n",
      "Valid loss: 1.6853, Valid acc: 0.2642\n",
      "Epoch 4:\n",
      "Train loss: 1.8364, Train acc: 0.2481\n",
      "Valid loss: 1.7198, Valid acc: 0.3239\n",
      "Epoch 5:\n",
      "Train loss: 1.7488, Train acc: 0.2588\n",
      "Valid loss: 1.8419, Valid acc: 0.2486\n",
      "Epoch 6:\n",
      "Train loss: 1.7297, Train acc: 0.2613\n",
      "Valid loss: 2.0599, Valid acc: 0.2486\n",
      "Epoch 7:\n",
      "Train loss: 1.7376, Train acc: 0.2744\n",
      "Valid loss: 2.2689, Valid acc: 0.2505\n",
      "Epoch 8:\n",
      "Train loss: 1.7311, Train acc: 0.2746\n",
      "Valid loss: 1.6455, Valid acc: 0.2890\n",
      "Epoch 9:\n",
      "Train loss: 1.7348, Train acc: 0.2666\n",
      "Valid loss: 2.1155, Valid acc: 0.2156\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7606, Train acc: 0.2173\n",
      "Valid loss: 1.7854, Valid acc: 0.1495\n",
      "Epoch 2:\n",
      "Train loss: 1.6714, Train acc: 0.2247\n",
      "Valid loss: 1.6956, Valid acc: 0.2413\n",
      "Epoch 3:\n",
      "Train loss: 1.6749, Train acc: 0.2233\n",
      "Valid loss: 1.6465, Valid acc: 0.2486\n",
      "Epoch 4:\n",
      "Train loss: 1.6701, Train acc: 0.2286\n",
      "Valid loss: 1.7503, Valid acc: 0.1514\n",
      "Epoch 5:\n",
      "Train loss: 1.6660, Train acc: 0.2322\n",
      "Valid loss: 1.9073, Valid acc: 0.2422\n",
      "Epoch 6:\n",
      "Train loss: 1.6612, Train acc: 0.2219\n",
      "Valid loss: 1.6710, Valid acc: 0.2431\n",
      "Epoch 7:\n",
      "Train loss: 1.6764, Train acc: 0.2311\n",
      "Valid loss: 1.6978, Valid acc: 0.2046\n",
      "Epoch 8:\n",
      "Train loss: 1.6692, Train acc: 0.2244\n",
      "Valid loss: 1.7091, Valid acc: 0.2431\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 1.8966, Train acc: 0.2242\n",
      "Valid loss: 1.9522, Valid acc: 0.2284\n",
      "Epoch 2:\n",
      "Train loss: 1.7373, Train acc: 0.2235\n",
      "Valid loss: 1.8432, Valid acc: 0.2578\n",
      "Epoch 3:\n",
      "Train loss: 1.7375, Train acc: 0.2224\n",
      "Valid loss: 1.8254, Valid acc: 0.2587\n",
      "Epoch 4:\n",
      "Train loss: 1.7345, Train acc: 0.2242\n",
      "Valid loss: 1.7897, Valid acc: 0.1927\n",
      "Epoch 5:\n",
      "Train loss: 1.7516, Train acc: 0.2157\n",
      "Valid loss: 2.0371, Valid acc: 0.1725\n",
      "Epoch 6:\n",
      "Train loss: 1.7425, Train acc: 0.2088\n",
      "Valid loss: 1.7383, Valid acc: 0.2651\n",
      "Epoch 7:\n",
      "Train loss: 1.7343, Train acc: 0.2274\n",
      "Valid loss: 1.7772, Valid acc: 0.1954\n",
      "Epoch 8:\n",
      "Train loss: 1.7250, Train acc: 0.2276\n",
      "Valid loss: 1.7464, Valid acc: 0.1853\n",
      "Epoch 9:\n",
      "Train loss: 1.7245, Train acc: 0.2171\n",
      "Valid loss: 1.7520, Valid acc: 0.2560\n",
      "Epoch 10:\n",
      "Train loss: 1.6993, Train acc: 0.2295\n",
      "Valid loss: 2.0215, Valid acc: 0.2073\n",
      "Epoch 11:\n",
      "Train loss: 1.6699, Train acc: 0.2675\n",
      "Valid loss: 1.7307, Valid acc: 0.2761\n",
      "Epoch 12:\n",
      "Train loss: 1.7201, Train acc: 0.2226\n",
      "Valid loss: 2.0675, Valid acc: 0.1514\n",
      "Epoch 13:\n",
      "Train loss: 1.7259, Train acc: 0.2109\n",
      "Valid loss: 1.8573, Valid acc: 0.2486\n",
      "Epoch 14:\n",
      "Train loss: 1.7273, Train acc: 0.2173\n",
      "Valid loss: 1.7978, Valid acc: 0.2046\n",
      "Epoch 15:\n",
      "Train loss: 1.7110, Train acc: 0.2171\n",
      "Valid loss: 2.3274, Valid acc: 0.1505\n",
      "Epoch 16:\n",
      "Train loss: 1.7458, Train acc: 0.2105\n",
      "Valid loss: 2.3408, Valid acc: 0.2349\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 64, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 2.6115, Train acc: 0.1788\n",
      "Valid loss: 2.3107, Valid acc: 0.1899\n",
      "Epoch 2:\n",
      "Train loss: 1.8990, Train acc: 0.2164\n",
      "Valid loss: 2.1519, Valid acc: 0.1468\n",
      "Epoch 3:\n",
      "Train loss: 1.9297, Train acc: 0.2215\n",
      "Valid loss: 2.1797, Valid acc: 0.2110\n",
      "Epoch 4:\n",
      "Train loss: 2.0427, Train acc: 0.1983\n",
      "Valid loss: 1.9746, Valid acc: 0.1495\n",
      "Epoch 5:\n",
      "Train loss: 1.9170, Train acc: 0.2125\n",
      "Valid loss: 1.8173, Valid acc: 0.2110\n",
      "Epoch 6:\n",
      "Train loss: 1.9907, Train acc: 0.1829\n",
      "Valid loss: 1.9661, Valid acc: 0.2385\n",
      "Epoch 7:\n",
      "Train loss: 1.9662, Train acc: 0.1942\n",
      "Valid loss: 2.5961, Valid acc: 0.2358\n",
      "Epoch 8:\n",
      "Train loss: 1.8889, Train acc: 0.2162\n",
      "Valid loss: 2.0571, Valid acc: 0.1523\n",
      "Epoch 9:\n",
      "Train loss: 1.9219, Train acc: 0.2189\n",
      "Valid loss: 2.3729, Valid acc: 0.2064\n",
      "Epoch 10:\n",
      "Train loss: 1.9028, Train acc: 0.2235\n",
      "Valid loss: 2.3756, Valid acc: 0.2064\n",
      "Epoch 11:\n",
      "Train loss: 1.9880, Train acc: 0.1969\n",
      "Valid loss: 2.2212, Valid acc: 0.1853\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 64}\n",
      "Epoch 1:\n",
      "Train loss: 1.7743, Train acc: 0.2095\n",
      "Valid loss: 1.7762, Valid acc: 0.1505\n",
      "Epoch 2:\n",
      "Train loss: 1.6756, Train acc: 0.2079\n",
      "Valid loss: 1.7411, Valid acc: 0.2404\n",
      "Epoch 3:\n",
      "Train loss: 1.6676, Train acc: 0.2336\n",
      "Valid loss: 1.7423, Valid acc: 0.2073\n",
      "Epoch 4:\n",
      "Train loss: 1.6680, Train acc: 0.2189\n",
      "Valid loss: 1.9211, Valid acc: 0.1835\n",
      "Epoch 5:\n",
      "Train loss: 1.6758, Train acc: 0.2228\n",
      "Valid loss: 1.6826, Valid acc: 0.2156\n",
      "Epoch 6:\n",
      "Train loss: 1.6644, Train acc: 0.2251\n",
      "Valid loss: 1.7899, Valid acc: 0.1780\n",
      "Epoch 7:\n",
      "Train loss: 1.6723, Train acc: 0.2121\n",
      "Valid loss: 1.7926, Valid acc: 0.2284\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 128}\n",
      "Epoch 1:\n",
      "Train loss: 2.1873, Train acc: 0.1770\n",
      "Valid loss: 1.9615, Valid acc: 0.2073\n",
      "Epoch 2:\n",
      "Train loss: 1.7493, Train acc: 0.2127\n",
      "Valid loss: 2.0600, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.7161, Train acc: 0.2293\n",
      "Valid loss: 1.7714, Valid acc: 0.2706\n",
      "Epoch 4:\n",
      "Train loss: 1.7295, Train acc: 0.2180\n",
      "Valid loss: 1.7550, Valid acc: 0.2560\n",
      "Epoch 5:\n",
      "Train loss: 1.7436, Train acc: 0.1988\n",
      "Valid loss: 1.9054, Valid acc: 0.2321\n",
      "Epoch 6:\n",
      "Train loss: 1.8802, Train acc: 0.1866\n",
      "Valid loss: 2.1679, Valid acc: 0.1789\n",
      "Epoch 7:\n",
      "Train loss: 1.8176, Train acc: 0.1972\n",
      "Valid loss: 1.9199, Valid acc: 0.1954\n",
      "Epoch 8:\n",
      "Train loss: 1.7441, Train acc: 0.2276\n",
      "Valid loss: 1.7729, Valid acc: 0.1954\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Training with hyperparameters: {'lrs': 0.005, 'optimizers': <class 'torch.optim.rmsprop.RMSprop'>, 'batch_sizes': 128, 'hidden_dims': 256}\n",
      "Epoch 1:\n",
      "Train loss: 3.1081, Train acc: 0.1738\n",
      "Valid loss: 2.2430, Valid acc: 0.1771\n",
      "Epoch 2:\n",
      "Train loss: 2.0393, Train acc: 0.1912\n",
      "Valid loss: 2.0405, Valid acc: 0.2284\n",
      "Epoch 3:\n",
      "Train loss: 1.9215, Train acc: 0.1994\n",
      "Valid loss: 2.0330, Valid acc: 0.1927\n",
      "Epoch 4:\n",
      "Train loss: 1.8584, Train acc: 0.1919\n",
      "Valid loss: 2.5206, Valid acc: 0.2092\n",
      "Epoch 5:\n",
      "Train loss: 1.8183, Train acc: 0.2242\n",
      "Valid loss: 1.9559, Valid acc: 0.1991\n",
      "Epoch 6:\n",
      "Train loss: 1.8712, Train acc: 0.2173\n",
      "Valid loss: 2.4561, Valid acc: 0.1853\n",
      "Epoch 7:\n",
      "Train loss: 1.8630, Train acc: 0.2017\n",
      "Valid loss: 2.2648, Valid acc: 0.2477\n",
      "Epoch 8:\n",
      "Train loss: 1.8427, Train acc: 0.2024\n",
      "Valid loss: 2.2433, Valid acc: 0.1991\n",
      "Epoch 9:\n",
      "Train loss: 1.8923, Train acc: 0.2176\n",
      "Valid loss: 2.2732, Valid acc: 0.2239\n",
      "Epoch 10:\n",
      "Train loss: 1.7777, Train acc: 0.2017\n",
      "Valid loss: 2.0109, Valid acc: 0.2404\n",
      "Epoch 11:\n",
      "Train loss: 1.7889, Train acc: 0.2052\n",
      "Valid loss: 2.1214, Valid acc: 0.1661\n",
      "Epoch 12:\n",
      "Train loss: 1.7838, Train acc: 0.2118\n",
      "Valid loss: 2.2768, Valid acc: 0.2055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Best Hyperparamters:  {'lr': 0.0001, 'optimizer': RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    capturable: False\n",
      "    centered: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      "), 'batch_size': 32, 'hidden_dim': 256, 'epochs ran': 32}\n",
      "Best validation accuracy:  0.8045871559633028\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for simple hyperparameter tuning based on validation accuracy\n",
    "param_grid = {\n",
    "    'lrs': [0.0001, 0.0005, 0.001, 0.005],\n",
    "    'optimizers': [torch.optim.SGD, torch.optim.Adagrad, torch.optim.Adam, torch.optim.RMSprop],\n",
    "    'batch_sizes': [32, 64, 128],\n",
    "    'hidden_dims': [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Run function to find optimal hyperparameters for (a)\n",
    "results, best_hyperparams = find_optimal_hyperparams(param_grid, TEXT.vocab.vectors.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c1a2dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "optimizer",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "batch_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hidden_dim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "best_valid_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "epochs ran",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9e44579b-66a9-4dd2-8f3a-30f254fe6934",
       "rows": [
        [
         "29",
         "0.0001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.8045871559633028",
         "32"
        ],
        [
         "83",
         "0.001",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.001\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.7990825688073394",
         "51"
        ],
        [
         "28",
         "0.0001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.7954128440366972",
         "38"
        ],
        [
         "31",
         "0.0001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "128",
         "0.7944954128440367",
         "58"
        ],
        [
         "23",
         "0.0001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.7935779816513762",
         "27"
        ],
        [
         "32",
         "0.0001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.7908256880733945",
         "37"
        ],
        [
         "86",
         "0.001",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.001\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.7880733944954128",
         "64"
        ],
        [
         "63",
         "0.0005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.7504587155963303",
         "34"
        ],
        [
         "66",
         "0.0005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "64",
         "0.7128440366972477",
         "27"
        ],
        [
         "20",
         "0.0001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.6862385321100918",
         "30"
        ],
        [
         "117",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.671559633027523",
         "32"
        ],
        [
         "19",
         "0.0001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.6688073394495413",
         "23"
        ],
        [
         "35",
         "0.0001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "128",
         "256",
         "0.6651376146788991",
         "35"
        ],
        [
         "82",
         "0.001",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.001\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.6458715596330276",
         "38"
        ],
        [
         "120",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "64",
         "0.5458715596330275",
         "25"
        ],
        [
         "102",
         "0.001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "64",
         "0.41284403669724773",
         "12"
        ],
        [
         "58",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "128",
         "0.41192660550458715",
         "11"
        ],
        [
         "91",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.4036697247706422",
         "10"
        ],
        [
         "54",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.40275229357798165",
         "14"
        ],
        [
         "118",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.40091743119266054",
         "8"
        ],
        [
         "55",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.4",
         "16"
        ],
        [
         "99",
         "0.001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.4",
         "13"
        ],
        [
         "57",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "64",
         "0.4",
         "11"
        ],
        [
         "121",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "128",
         "0.3963302752293578",
         "14"
        ],
        [
         "61",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "128",
         "128",
         "0.39541284403669724",
         "13"
        ],
        [
         "64",
         "0.0005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.39541284403669724",
         "9"
        ],
        [
         "105",
         "0.001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "128",
         "64",
         "0.3926605504587156",
         "26"
        ],
        [
         "100",
         "0.001",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.38623853211009174",
         "9"
        ],
        [
         "97",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "128",
         "128",
         "0.3798165137614679",
         "21"
        ],
        [
         "119",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.37339449541284403",
         "9"
        ],
        [
         "122",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.3688073394495413",
         "12"
        ],
        [
         "56",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.3660550458715596",
         "9"
        ],
        [
         "90",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.3623853211009174",
         "9"
        ],
        [
         "124",
         "0.005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "128",
         "128",
         "0.3587155963302752",
         "17"
        ],
        [
         "95",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.3577981651376147",
         "7"
        ],
        [
         "59",
         "0.0005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.3495412844036697",
         "10"
        ],
        [
         "136",
         "0.005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.3486238532110092",
         "7"
        ],
        [
         "67",
         "0.0005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "128",
         "0.3467889908256881",
         "10"
        ],
        [
         "92",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.336697247706422",
         "8"
        ],
        [
         "137",
         "0.005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.3238532110091743",
         "9"
        ],
        [
         "65",
         "0.0005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "32",
         "256",
         "0.3137614678899083",
         "9"
        ],
        [
         "139",
         "0.005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "64",
         "128",
         "0.2761467889908257",
         "16"
        ],
        [
         "142",
         "0.005",
         "RMSprop (\nParameter Group 0\n    alpha: 0.99\n    capturable: False\n    centered: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.005\n    maximize: False\n    momentum: 0\n    weight_decay: 0\n)",
         "128",
         "128",
         "0.2706422018348624",
         "8"
        ],
        [
         "72",
         "0.001",
         "SGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.26055045871559634",
         "14"
        ],
        [
         "50",
         "0.0005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.0005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.25963302752293577",
         "11"
        ],
        [
         "98",
         "0.001",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)",
         "128",
         "256",
         "0.25871559633027524",
         "8"
        ],
        [
         "131",
         "0.005",
         "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    decoupled_weight_decay: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.005\n    maximize: False\n    weight_decay: 0\n)",
         "64",
         "256",
         "0.25871559633027524",
         "12"
        ],
        [
         "51",
         "0.0005",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.0005\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "128",
         "64",
         "0.25596330275229356",
         "20"
        ],
        [
         "9",
         "0.0001",
         "Adagrad (\nParameter Group 0\n    differentiable: False\n    eps: 1e-10\n    foreach: None\n    fused: None\n    initial_accumulator_value: 0\n    lr: 0.0001\n    lr_decay: 0\n    maximize: False\n    weight_decay: 0\n)",
         "32",
         "64",
         "0.25321100917431194",
         "11"
        ],
        [
         "73",
         "0.001",
         "SGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)",
         "32",
         "128",
         "0.25321100917431194",
         "12"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 144
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>best_valid_acc</th>\n",
       "      <th>epochs ran</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>0.804587</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>0.799083</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>0.795413</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0.794495</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>0.793578</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.200917</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.200917</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.144037</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.020183</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr                                          optimizer  batch_size  \\\n",
       "29   0.0001  RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...          32   \n",
       "83   0.0010  Adagrad (\\nParameter Group 0\\n    differentiab...          32   \n",
       "28   0.0001  RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...          32   \n",
       "31   0.0001  RMSprop (\\nParameter Group 0\\n    alpha: 0.99\\...          64   \n",
       "23   0.0001  Adam (\\nParameter Group 0\\n    amsgrad: False\\...          64   \n",
       "..      ...                                                ...         ...   \n",
       "116  0.0050  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...         128   \n",
       "78   0.0010  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...         128   \n",
       "2    0.0001  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...          32   \n",
       "7    0.0001  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...         128   \n",
       "6    0.0001  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...         128   \n",
       "\n",
       "     hidden_dim  best_valid_acc  epochs ran  \n",
       "29          256        0.804587          32  \n",
       "83          256        0.799083          51  \n",
       "28          128        0.795413          38  \n",
       "31          128        0.794495          58  \n",
       "23          256        0.793578          27  \n",
       "..          ...             ...         ...  \n",
       "116         256        0.200917           9  \n",
       "78           64        0.200917          15  \n",
       "2           256        0.155963          10  \n",
       "7           128        0.144037           6  \n",
       "6            64        0.020183           6  \n",
       "\n",
       "[144 rows x 6 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To put everything in a dataframe for neater results\n",
    "df_results_a = pd.DataFrame(results)\n",
    "df_results_a = df_results_a.sort_values(by='best_valid_acc', ascending=False)\n",
    "df_results_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c920952",
   "metadata": {},
   "source": [
    "### Answer (a):\n",
    "Final Configuration of Best Model \n",
    "Number of training epochs = 37\n",
    "Learning rate = 0.0001\n",
    "Optimizer = Adam\n",
    "Batch Size = 32\n",
    "Hidden dimension = 256\n",
    "\n",
    "Best validation accuracy based on optimal parameters: 0.7899"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68973e1",
   "metadata": {},
   "source": [
    "**(b) Report all the regularization strategies you have tried. Compare the accuracy on the test set among all strategies and the one without any regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eb1be",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "Here, our approach is to try most of the regularization strategies (that does not change the number of layers) that we have learnt during the lectures to avoid overfitting, namely:\n",
    "1. Baseline (as per qn requirement)\n",
    "2. L2 Regularization (Weight Decay)\n",
    "3. Dropout\n",
    "4. Gradient Clipping\n",
    "\n",
    "We first experiment with the regularization strategies individually with parameter tuning, e.g. Dropout ONLY or Gradient Clipping ONLY, and we do a comparison with the baseline to see how no regularisation here would perform compared to having regularization.\n",
    "\n",
    "Thereafter, we try to find the best possible \"strategy\" to use for the later part by trying a combination of the strategies to see which would perform the best after evaluating on test accuracy as per question requirements (e.g. whether dropout + L2 regularisation is better than just dropout, whether dropout + gradient clipping + L2 regularisation outperforms having lesser strategies combined etc.)\n",
    "\n",
    "Note: Even though early stopping is a regularization technique, we do not consider it here as one of the strategies to try out since we already implemented it earlier on. The goal here for us is to try out other strategies that we have not implemented in part (a).\n",
    "\n",
    "Note 2: We did not consider L1 regularisation as a strategy because we found that Adam is the best optimizer for our model in part (a), and since Adam is an adaptive optimizer, trying to use L1 can potentially cause the issue of sparse weights (which we want to avoid here).\n",
    "\n",
    "Note 3: While we did consider trying batch normalisation, we realised it is best used in between 2 hidden layers. Since we only had 1 hidden layer, internal covariate shift is minimal in a shallow network, and we think that the stabilising benefits are limited compared to the unnecessary overhead and noise introduced in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba361b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ideal hyperparameters from 2(a)\n",
    "if best_hyperparams[\"epochs ran\"] <= 50:\n",
    "    no_epoch = 50\n",
    "elif best_hyperparams[\"epochs ran\"] > 50 & best_hyperparams[\"epochs ran\"] <= 100:\n",
    "    no_epoch = 100\n",
    "else:\n",
    "    no_epoch = 200\n",
    "batch_size = best_hyperparams[\"batch_size\"]\n",
    "hidden_dim = best_hyperparams[\"hidden_dim\"]\n",
    "lr = best_hyperparams[\"lr\"]\n",
    "optimizer = best_hyperparams[\"optimizer\"]\n",
    "\n",
    "best_reg_technique = []\n",
    "\n",
    "# Define a function for Regularisation Tests\n",
    "def regularisation_test(weight_decay, dropout, grad_clip, max_norm, reg_technique, optimizer):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialise model\n",
    "    model = ClassifierRNN(TEXT.vocab.vectors.numpy() , hidden_dim, dropout=dropout)\n",
    "\n",
    "    # Initialise optimiser with L2 regularization\n",
    "    optimizer = optimizer.__class__(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if grad_clip == True:\n",
    "        _, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch, grad_clip=True, max_norm=max_norm)\n",
    "    else:\n",
    "        _, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_acc = test_loop(model, test_loader)\n",
    "    print(f\"Test Accuracy {reg_technique} (Dropout: {dropout}, Weight Decay: {weight_decay}, Max Norm: {max_norm}): {test_acc:.4f}\") # Print test accuracy for regularisation strategy\n",
    "\n",
    "    return {\n",
    "        'technique': reg_technique,\n",
    "        'dropout': dropout,\n",
    "        'weight_decay': weight_decay,\n",
    "        'grad_clip': grad_clip,\n",
    "        'max_norm': max_norm,\n",
    "        'best_val_acc': max(valid_accuracies),\n",
    "        'test_acc': test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16e7cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 1.6589, Train acc: 0.2244\n",
      "Valid loss: 1.6503, Valid acc: 0.2477\n",
      "Epoch 2:\n",
      "Train loss: 1.6450, Train acc: 0.2377\n",
      "Valid loss: 1.6527, Valid acc: 0.2294\n",
      "Epoch 3:\n",
      "Train loss: 1.6412, Train acc: 0.2400\n",
      "Valid loss: 1.6436, Valid acc: 0.2587\n",
      "Epoch 4:\n",
      "Train loss: 1.4870, Train acc: 0.3680\n",
      "Valid loss: 1.3672, Valid acc: 0.4450\n",
      "Epoch 5:\n",
      "Train loss: 1.2609, Train acc: 0.5023\n",
      "Valid loss: 1.2810, Valid acc: 0.5073\n",
      "Epoch 6:\n",
      "Train loss: 1.1058, Train acc: 0.5674\n",
      "Valid loss: 1.0726, Valid acc: 0.6028\n",
      "Epoch 7:\n",
      "Train loss: 1.0090, Train acc: 0.6071\n",
      "Valid loss: 1.1062, Valid acc: 0.5862\n",
      "Epoch 8:\n",
      "Train loss: 0.9275, Train acc: 0.6444\n",
      "Valid loss: 1.0496, Valid acc: 0.6018\n",
      "Epoch 9:\n",
      "Train loss: 0.8779, Train acc: 0.6591\n",
      "Valid loss: 0.9986, Valid acc: 0.6339\n",
      "Epoch 10:\n",
      "Train loss: 0.8100, Train acc: 0.6825\n",
      "Valid loss: 1.0564, Valid acc: 0.6147\n",
      "Epoch 11:\n",
      "Train loss: 0.7846, Train acc: 0.6887\n",
      "Valid loss: 0.9225, Valid acc: 0.6514\n",
      "Epoch 12:\n",
      "Train loss: 0.7347, Train acc: 0.7004\n",
      "Valid loss: 1.0294, Valid acc: 0.6339\n",
      "Epoch 13:\n",
      "Train loss: 0.6935, Train acc: 0.7155\n",
      "Valid loss: 0.8959, Valid acc: 0.6642\n",
      "Epoch 14:\n",
      "Train loss: 0.6668, Train acc: 0.7233\n",
      "Valid loss: 0.9321, Valid acc: 0.6514\n",
      "Epoch 15:\n",
      "Train loss: 0.6587, Train acc: 0.7309\n",
      "Valid loss: 0.9449, Valid acc: 0.6413\n",
      "Epoch 16:\n",
      "Train loss: 0.6348, Train acc: 0.7540\n",
      "Valid loss: 0.8593, Valid acc: 0.6688\n",
      "Epoch 17:\n",
      "Train loss: 0.5891, Train acc: 0.7712\n",
      "Valid loss: 0.8358, Valid acc: 0.6908\n",
      "Epoch 18:\n",
      "Train loss: 0.5703, Train acc: 0.8006\n",
      "Valid loss: 0.8321, Valid acc: 0.7312\n",
      "Epoch 19:\n",
      "Train loss: 0.5573, Train acc: 0.8235\n",
      "Valid loss: 0.9355, Valid acc: 0.6890\n",
      "Epoch 20:\n",
      "Train loss: 0.4789, Train acc: 0.8547\n",
      "Valid loss: 0.8460, Valid acc: 0.7459\n",
      "Epoch 21:\n",
      "Train loss: 0.4596, Train acc: 0.8689\n",
      "Valid loss: 0.7741, Valid acc: 0.7761\n",
      "Epoch 22:\n",
      "Train loss: 0.4457, Train acc: 0.8737\n",
      "Valid loss: 0.7773, Valid acc: 0.7798\n",
      "Epoch 23:\n",
      "Train loss: 0.4475, Train acc: 0.8783\n",
      "Valid loss: 0.9078, Valid acc: 0.7358\n",
      "Epoch 24:\n",
      "Train loss: 0.3975, Train acc: 0.8952\n",
      "Valid loss: 0.8474, Valid acc: 0.7679\n",
      "Epoch 25:\n",
      "Train loss: 0.3680, Train acc: 0.9035\n",
      "Valid loss: 0.8553, Valid acc: 0.7661\n",
      "Epoch 26:\n",
      "Train loss: 0.3248, Train acc: 0.9161\n",
      "Valid loss: 0.7393, Valid acc: 0.8073\n",
      "Epoch 27:\n",
      "Train loss: 0.3737, Train acc: 0.9065\n",
      "Valid loss: 0.9361, Valid acc: 0.7477\n",
      "Epoch 28:\n",
      "Train loss: 0.3261, Train acc: 0.9204\n",
      "Valid loss: 0.7797, Valid acc: 0.7835\n",
      "Epoch 29:\n",
      "Train loss: 0.3382, Train acc: 0.9159\n",
      "Valid loss: 0.8938, Valid acc: 0.7560\n",
      "Epoch 30:\n",
      "Train loss: 0.3145, Train acc: 0.9234\n",
      "Valid loss: 1.4331, Valid acc: 0.6266\n",
      "Epoch 31:\n",
      "Train loss: 0.2763, Train acc: 0.9324\n",
      "Valid loss: 0.8102, Valid acc: 0.7853\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Baseline (Dropout: 0.0, Weight Decay: 0.0, Max Norm: 0.0): 0.8220\n"
     ]
    }
   ],
   "source": [
    "# Regularisation - Baseline (No Regularisation)\n",
    "best_reg_technique.append(regularisation_test(weight_decay=0.0,dropout=0.0, grad_clip=False, max_norm=0.0, reg_technique=\"Baseline\", optimizer=optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fcd0ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing L2 Regularization with weight decay: 1e-05\n",
      "Epoch 1:\n",
      "Train loss: 1.6578, Train acc: 0.2325\n",
      "Valid loss: 1.6547, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6464, Train acc: 0.2306\n",
      "Valid loss: 1.6531, Valid acc: 0.2037\n",
      "Epoch 3:\n",
      "Train loss: 1.6497, Train acc: 0.2412\n",
      "Valid loss: 1.6478, Valid acc: 0.2679\n",
      "Epoch 4:\n",
      "Train loss: 1.6443, Train acc: 0.2403\n",
      "Valid loss: 1.6338, Valid acc: 0.2477\n",
      "Epoch 5:\n",
      "Train loss: 1.4696, Train acc: 0.3677\n",
      "Valid loss: 1.4438, Valid acc: 0.3743\n",
      "Epoch 6:\n",
      "Train loss: 1.2631, Train acc: 0.4856\n",
      "Valid loss: 1.3454, Valid acc: 0.4679\n",
      "Epoch 7:\n",
      "Train loss: 1.1305, Train acc: 0.5509\n",
      "Valid loss: 1.1413, Valid acc: 0.5606\n",
      "Epoch 8:\n",
      "Train loss: 1.0307, Train acc: 0.5926\n",
      "Valid loss: 1.0820, Valid acc: 0.5991\n",
      "Epoch 9:\n",
      "Train loss: 0.9465, Train acc: 0.6355\n",
      "Valid loss: 0.9981, Valid acc: 0.6147\n",
      "Epoch 10:\n",
      "Train loss: 0.8883, Train acc: 0.6667\n",
      "Valid loss: 1.0090, Valid acc: 0.6193\n",
      "Epoch 11:\n",
      "Train loss: 0.8316, Train acc: 0.6884\n",
      "Valid loss: 1.2720, Valid acc: 0.5018\n",
      "Epoch 12:\n",
      "Train loss: 0.7883, Train acc: 0.6992\n",
      "Valid loss: 0.9121, Valid acc: 0.6404\n",
      "Epoch 13:\n",
      "Train loss: 0.7479, Train acc: 0.7141\n",
      "Valid loss: 0.9504, Valid acc: 0.6422\n",
      "Epoch 14:\n",
      "Train loss: 0.7152, Train acc: 0.7331\n",
      "Valid loss: 0.9025, Valid acc: 0.6596\n",
      "Epoch 15:\n",
      "Train loss: 0.8286, Train acc: 0.6832\n",
      "Valid loss: 0.8751, Valid acc: 0.6734\n",
      "Epoch 16:\n",
      "Train loss: 0.6435, Train acc: 0.7531\n",
      "Valid loss: 0.8722, Valid acc: 0.6761\n",
      "Epoch 17:\n",
      "Train loss: 0.6423, Train acc: 0.7593\n",
      "Valid loss: 0.8614, Valid acc: 0.6872\n",
      "Epoch 18:\n",
      "Train loss: 0.6058, Train acc: 0.7973\n",
      "Valid loss: 0.8815, Valid acc: 0.6633\n",
      "Epoch 19:\n",
      "Train loss: 0.5346, Train acc: 0.8352\n",
      "Valid loss: 0.9111, Valid acc: 0.6927\n",
      "Epoch 20:\n",
      "Train loss: 0.5867, Train acc: 0.8207\n",
      "Valid loss: 0.8457, Valid acc: 0.7239\n",
      "Epoch 21:\n",
      "Train loss: 0.4632, Train acc: 0.8668\n",
      "Valid loss: 0.8657, Valid acc: 0.7505\n",
      "Epoch 22:\n",
      "Train loss: 0.4463, Train acc: 0.8785\n",
      "Valid loss: 0.8901, Valid acc: 0.7339\n",
      "Epoch 23:\n",
      "Train loss: 0.4583, Train acc: 0.8721\n",
      "Valid loss: 0.8686, Valid acc: 0.7303\n",
      "Epoch 24:\n",
      "Train loss: 0.4140, Train acc: 0.8909\n",
      "Valid loss: 0.9731, Valid acc: 0.7367\n",
      "Epoch 25:\n",
      "Train loss: 0.3610, Train acc: 0.9053\n",
      "Valid loss: 0.8908, Valid acc: 0.7367\n",
      "Epoch 26:\n",
      "Train loss: 0.3682, Train acc: 0.9072\n",
      "Valid loss: 0.8543, Valid acc: 0.7734\n",
      "Epoch 27:\n",
      "Train loss: 0.3601, Train acc: 0.9058\n",
      "Valid loss: 0.9202, Valid acc: 0.7358\n",
      "Epoch 28:\n",
      "Train loss: 0.3401, Train acc: 0.9156\n",
      "Valid loss: 1.0483, Valid acc: 0.7193\n",
      "Epoch 29:\n",
      "Train loss: 0.3145, Train acc: 0.9204\n",
      "Valid loss: 1.0941, Valid acc: 0.7193\n",
      "Epoch 30:\n",
      "Train loss: 0.3167, Train acc: 0.9172\n",
      "Valid loss: 0.9373, Valid acc: 0.7670\n",
      "Epoch 31:\n",
      "Train loss: 0.2810, Train acc: 0.9324\n",
      "Valid loss: 0.8577, Valid acc: 0.7881\n",
      "Epoch 32:\n",
      "Train loss: 0.3254, Train acc: 0.9202\n",
      "Valid loss: 0.8557, Valid acc: 0.7817\n",
      "Epoch 33:\n",
      "Train loss: 0.2832, Train acc: 0.9312\n",
      "Valid loss: 0.9286, Valid acc: 0.7615\n",
      "Epoch 34:\n",
      "Train loss: 0.2836, Train acc: 0.9324\n",
      "Valid loss: 0.8326, Valid acc: 0.7899\n",
      "Epoch 35:\n",
      "Train loss: 0.2787, Train acc: 0.9347\n",
      "Valid loss: 0.8516, Valid acc: 0.7844\n",
      "Epoch 36:\n",
      "Train loss: 0.2604, Train acc: 0.9356\n",
      "Valid loss: 0.9041, Valid acc: 0.7798\n",
      "Epoch 37:\n",
      "Train loss: 0.2607, Train acc: 0.9397\n",
      "Valid loss: 0.8917, Valid acc: 0.7881\n",
      "Epoch 38:\n",
      "Train loss: 0.2272, Train acc: 0.9459\n",
      "Valid loss: 0.8864, Valid acc: 0.7853\n",
      "Epoch 39:\n",
      "Train loss: 0.2456, Train acc: 0.9413\n",
      "Valid loss: 0.8911, Valid acc: 0.7771\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy L2 Regularization (Dropout: 0.0, Weight Decay: 1e-05, Max Norm: 0.0): 0.8220\n",
      "Testing L2 Regularization with weight decay: 5e-05\n",
      "Epoch 1:\n",
      "Train loss: 1.6590, Train acc: 0.2320\n",
      "Valid loss: 1.6535, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6465, Train acc: 0.2345\n",
      "Valid loss: 1.6526, Valid acc: 0.2367\n",
      "Epoch 3:\n",
      "Train loss: 1.6471, Train acc: 0.2334\n",
      "Valid loss: 1.6460, Valid acc: 0.2569\n",
      "Epoch 4:\n",
      "Train loss: 1.5304, Train acc: 0.3505\n",
      "Valid loss: 1.4180, Valid acc: 0.4083\n",
      "Epoch 5:\n",
      "Train loss: 1.3616, Train acc: 0.4223\n",
      "Valid loss: 1.3409, Valid acc: 0.4275\n",
      "Epoch 6:\n",
      "Train loss: 1.2045, Train acc: 0.5005\n",
      "Valid loss: 1.2707, Valid acc: 0.5073\n",
      "Epoch 7:\n",
      "Train loss: 1.0944, Train acc: 0.5624\n",
      "Valid loss: 1.1178, Valid acc: 0.5606\n",
      "Epoch 8:\n",
      "Train loss: 0.9992, Train acc: 0.6142\n",
      "Valid loss: 0.9534, Valid acc: 0.6468\n",
      "Epoch 9:\n",
      "Train loss: 0.8924, Train acc: 0.6511\n",
      "Valid loss: 0.9548, Valid acc: 0.6569\n",
      "Epoch 10:\n",
      "Train loss: 0.8351, Train acc: 0.6745\n",
      "Valid loss: 0.9442, Valid acc: 0.6431\n",
      "Epoch 11:\n",
      "Train loss: 0.7873, Train acc: 0.6845\n",
      "Valid loss: 0.8620, Valid acc: 0.6661\n",
      "Epoch 12:\n",
      "Train loss: 0.7569, Train acc: 0.7171\n",
      "Valid loss: 0.8491, Valid acc: 0.6835\n",
      "Epoch 13:\n",
      "Train loss: 0.7229, Train acc: 0.7364\n",
      "Valid loss: 0.8806, Valid acc: 0.6394\n",
      "Epoch 14:\n",
      "Train loss: 0.7061, Train acc: 0.7437\n",
      "Valid loss: 0.9059, Valid acc: 0.6936\n",
      "Epoch 15:\n",
      "Train loss: 0.6836, Train acc: 0.7685\n",
      "Valid loss: 0.8304, Valid acc: 0.7431\n",
      "Epoch 16:\n",
      "Train loss: 0.6188, Train acc: 0.7967\n",
      "Valid loss: 0.8460, Valid acc: 0.7413\n",
      "Epoch 17:\n",
      "Train loss: 0.5906, Train acc: 0.8216\n",
      "Valid loss: 0.8701, Valid acc: 0.7229\n",
      "Epoch 18:\n",
      "Train loss: 0.5414, Train acc: 0.8430\n",
      "Valid loss: 1.7080, Valid acc: 0.4532\n",
      "Epoch 19:\n",
      "Train loss: 0.5039, Train acc: 0.8652\n",
      "Valid loss: 1.1461, Valid acc: 0.6578\n",
      "Epoch 20:\n",
      "Train loss: 0.4611, Train acc: 0.8737\n",
      "Valid loss: 0.7622, Valid acc: 0.7780\n",
      "Epoch 21:\n",
      "Train loss: 0.4481, Train acc: 0.8854\n",
      "Valid loss: 0.7437, Valid acc: 0.7963\n",
      "Epoch 22:\n",
      "Train loss: 0.3851, Train acc: 0.9030\n",
      "Valid loss: 0.7755, Valid acc: 0.7945\n",
      "Epoch 23:\n",
      "Train loss: 0.4303, Train acc: 0.8854\n",
      "Valid loss: 0.7808, Valid acc: 0.7826\n",
      "Epoch 24:\n",
      "Train loss: 0.3502, Train acc: 0.9156\n",
      "Valid loss: 0.7701, Valid acc: 0.8018\n",
      "Epoch 25:\n",
      "Train loss: 0.3555, Train acc: 0.9097\n",
      "Valid loss: 1.1868, Valid acc: 0.6807\n",
      "Epoch 26:\n",
      "Train loss: 0.3622, Train acc: 0.9090\n",
      "Valid loss: 0.7140, Valid acc: 0.8101\n",
      "Epoch 27:\n",
      "Train loss: 0.3145, Train acc: 0.9250\n",
      "Valid loss: 0.7555, Valid acc: 0.8165\n",
      "Epoch 28:\n",
      "Train loss: 0.3399, Train acc: 0.9202\n",
      "Valid loss: 0.9564, Valid acc: 0.7284\n",
      "Epoch 29:\n",
      "Train loss: 0.2974, Train acc: 0.9266\n",
      "Valid loss: 1.1871, Valid acc: 0.6615\n",
      "Epoch 30:\n",
      "Train loss: 0.2963, Train acc: 0.9271\n",
      "Valid loss: 1.0463, Valid acc: 0.7394\n",
      "Epoch 31:\n",
      "Train loss: 0.2882, Train acc: 0.9289\n",
      "Valid loss: 0.8212, Valid acc: 0.7954\n",
      "Epoch 32:\n",
      "Train loss: 0.2782, Train acc: 0.9349\n",
      "Valid loss: 0.8542, Valid acc: 0.7771\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy L2 Regularization (Dropout: 0.0, Weight Decay: 5e-05, Max Norm: 0.0): 0.8160\n",
      "Testing L2 Regularization with weight decay: 0.0001\n",
      "Epoch 1:\n",
      "Train loss: 1.6559, Train acc: 0.2403\n",
      "Valid loss: 1.6524, Valid acc: 0.2440\n",
      "Epoch 2:\n",
      "Train loss: 1.6479, Train acc: 0.2368\n",
      "Valid loss: 1.6470, Valid acc: 0.2633\n",
      "Epoch 3:\n",
      "Train loss: 1.6390, Train acc: 0.2430\n",
      "Valid loss: 1.5887, Valid acc: 0.2532\n",
      "Epoch 4:\n",
      "Train loss: 1.5173, Train acc: 0.2976\n",
      "Valid loss: 1.4561, Valid acc: 0.3110\n",
      "Epoch 5:\n",
      "Train loss: 1.3842, Train acc: 0.3686\n",
      "Valid loss: 1.3727, Valid acc: 0.3789\n",
      "Epoch 6:\n",
      "Train loss: 1.2922, Train acc: 0.4402\n",
      "Valid loss: 1.4061, Valid acc: 0.4165\n",
      "Epoch 7:\n",
      "Train loss: 1.1570, Train acc: 0.5289\n",
      "Valid loss: 1.1948, Valid acc: 0.5239\n",
      "Epoch 8:\n",
      "Train loss: 1.0857, Train acc: 0.5571\n",
      "Valid loss: 1.5630, Valid acc: 0.4009\n",
      "Epoch 9:\n",
      "Train loss: 0.9896, Train acc: 0.6055\n",
      "Valid loss: 1.3535, Valid acc: 0.4725\n",
      "Epoch 10:\n",
      "Train loss: 0.8969, Train acc: 0.6272\n",
      "Valid loss: 1.3271, Valid acc: 0.5450\n",
      "Epoch 11:\n",
      "Train loss: 0.8313, Train acc: 0.6683\n",
      "Valid loss: 1.2688, Valid acc: 0.5128\n",
      "Epoch 12:\n",
      "Train loss: 0.7627, Train acc: 0.6910\n",
      "Valid loss: 1.0230, Valid acc: 0.6156\n",
      "Epoch 13:\n",
      "Train loss: 0.7361, Train acc: 0.7084\n",
      "Valid loss: 0.9952, Valid acc: 0.6450\n",
      "Epoch 14:\n",
      "Train loss: 0.7658, Train acc: 0.7095\n",
      "Valid loss: 0.9593, Valid acc: 0.6404\n",
      "Epoch 15:\n",
      "Train loss: 0.6598, Train acc: 0.7460\n",
      "Valid loss: 0.8742, Valid acc: 0.6798\n",
      "Epoch 16:\n",
      "Train loss: 0.6253, Train acc: 0.7662\n",
      "Valid loss: 0.9635, Valid acc: 0.6615\n",
      "Epoch 17:\n",
      "Train loss: 0.5720, Train acc: 0.7946\n",
      "Valid loss: 0.8562, Valid acc: 0.7055\n",
      "Epoch 18:\n",
      "Train loss: 0.5159, Train acc: 0.8322\n",
      "Valid loss: 1.2198, Valid acc: 0.6092\n",
      "Epoch 19:\n",
      "Train loss: 0.4388, Train acc: 0.8627\n",
      "Valid loss: 0.7594, Valid acc: 0.7706\n",
      "Epoch 20:\n",
      "Train loss: 0.4042, Train acc: 0.8794\n",
      "Valid loss: 0.6792, Valid acc: 0.7927\n",
      "Epoch 21:\n",
      "Train loss: 0.3441, Train acc: 0.9044\n",
      "Valid loss: 0.6552, Valid acc: 0.7963\n",
      "Epoch 22:\n",
      "Train loss: 0.3466, Train acc: 0.9033\n",
      "Valid loss: 0.6914, Valid acc: 0.8037\n",
      "Epoch 23:\n",
      "Train loss: 0.3072, Train acc: 0.9177\n",
      "Valid loss: 0.8542, Valid acc: 0.7505\n",
      "Epoch 24:\n",
      "Train loss: 0.2927, Train acc: 0.9188\n",
      "Valid loss: 0.7996, Valid acc: 0.7752\n",
      "Epoch 25:\n",
      "Train loss: 0.2849, Train acc: 0.9234\n",
      "Valid loss: 0.6031, Valid acc: 0.8294\n",
      "Epoch 26:\n",
      "Train loss: 0.2453, Train acc: 0.9356\n",
      "Valid loss: 0.6342, Valid acc: 0.8193\n",
      "Epoch 27:\n",
      "Train loss: 0.2380, Train acc: 0.9376\n",
      "Valid loss: 0.6613, Valid acc: 0.8147\n",
      "Epoch 28:\n",
      "Train loss: 0.2470, Train acc: 0.9356\n",
      "Valid loss: 0.7800, Valid acc: 0.7862\n",
      "Epoch 29:\n",
      "Train loss: 0.2391, Train acc: 0.9415\n",
      "Valid loss: 0.6222, Valid acc: 0.8321\n",
      "Epoch 30:\n",
      "Train loss: 0.2111, Train acc: 0.9475\n",
      "Valid loss: 0.5918, Valid acc: 0.8431\n",
      "Epoch 31:\n",
      "Train loss: 0.1926, Train acc: 0.9496\n",
      "Valid loss: 0.7377, Valid acc: 0.7963\n",
      "Epoch 32:\n",
      "Train loss: 0.1929, Train acc: 0.9535\n",
      "Valid loss: 0.6492, Valid acc: 0.8165\n",
      "Epoch 33:\n",
      "Train loss: 0.1767, Train acc: 0.9562\n",
      "Valid loss: 0.6579, Valid acc: 0.8358\n",
      "Epoch 34:\n",
      "Train loss: 0.2095, Train acc: 0.9457\n",
      "Valid loss: 0.6118, Valid acc: 0.8312\n",
      "Epoch 35:\n",
      "Train loss: 0.2440, Train acc: 0.9379\n",
      "Valid loss: 0.5549, Valid acc: 0.8523\n",
      "Epoch 36:\n",
      "Train loss: 0.1288, Train acc: 0.9709\n",
      "Valid loss: 0.5833, Valid acc: 0.8422\n",
      "Epoch 37:\n",
      "Train loss: 0.1726, Train acc: 0.9569\n",
      "Valid loss: 0.6494, Valid acc: 0.8312\n",
      "Epoch 38:\n",
      "Train loss: 0.1793, Train acc: 0.9574\n",
      "Valid loss: 0.6677, Valid acc: 0.8266\n",
      "Epoch 39:\n",
      "Train loss: 0.1369, Train acc: 0.9668\n",
      "Valid loss: 0.6173, Valid acc: 0.8468\n",
      "Epoch 40:\n",
      "Train loss: 0.1557, Train acc: 0.9624\n",
      "Valid loss: 0.6899, Valid acc: 0.8312\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy L2 Regularization (Dropout: 0.0, Weight Decay: 0.0001, Max Norm: 0.0): 0.8400\n",
      "Testing L2 Regularization with weight decay: 0.0005\n",
      "Epoch 1:\n",
      "Train loss: 1.6588, Train acc: 0.2267\n",
      "Valid loss: 1.6485, Valid acc: 0.2477\n",
      "Epoch 2:\n",
      "Train loss: 1.6500, Train acc: 0.2224\n",
      "Valid loss: 1.6496, Valid acc: 0.2468\n",
      "Epoch 3:\n",
      "Train loss: 1.6475, Train acc: 0.2256\n",
      "Valid loss: 1.6444, Valid acc: 0.2459\n",
      "Epoch 4:\n",
      "Train loss: 1.6425, Train acc: 0.2364\n",
      "Valid loss: 1.6516, Valid acc: 0.2532\n",
      "Epoch 5:\n",
      "Train loss: 1.5751, Train acc: 0.3031\n",
      "Valid loss: 1.5385, Valid acc: 0.3303\n",
      "Epoch 6:\n",
      "Train loss: 1.3395, Train acc: 0.4477\n",
      "Valid loss: 1.6316, Valid acc: 0.3110\n",
      "Epoch 7:\n",
      "Train loss: 1.1715, Train acc: 0.5261\n",
      "Valid loss: 1.4234, Valid acc: 0.4284\n",
      "Epoch 8:\n",
      "Train loss: 1.0499, Train acc: 0.5869\n",
      "Valid loss: 1.3743, Valid acc: 0.4615\n",
      "Epoch 9:\n",
      "Train loss: 0.9667, Train acc: 0.6201\n",
      "Valid loss: 1.4951, Valid acc: 0.4569\n",
      "Epoch 10:\n",
      "Train loss: 0.9039, Train acc: 0.6479\n",
      "Valid loss: 1.0275, Valid acc: 0.6028\n",
      "Epoch 11:\n",
      "Train loss: 0.8520, Train acc: 0.6678\n",
      "Valid loss: 1.1359, Valid acc: 0.5560\n",
      "Epoch 12:\n",
      "Train loss: 0.8170, Train acc: 0.6855\n",
      "Valid loss: 0.9473, Valid acc: 0.6239\n",
      "Epoch 13:\n",
      "Train loss: 0.8224, Train acc: 0.6880\n",
      "Valid loss: 0.9934, Valid acc: 0.6174\n",
      "Epoch 14:\n",
      "Train loss: 0.7503, Train acc: 0.7182\n",
      "Valid loss: 0.9581, Valid acc: 0.6349\n",
      "Epoch 15:\n",
      "Train loss: 0.7187, Train acc: 0.7242\n",
      "Valid loss: 1.0729, Valid acc: 0.5661\n",
      "Epoch 16:\n",
      "Train loss: 0.7187, Train acc: 0.7247\n",
      "Valid loss: 0.9893, Valid acc: 0.6028\n",
      "Epoch 17:\n",
      "Train loss: 0.6858, Train acc: 0.7432\n",
      "Valid loss: 0.8848, Valid acc: 0.6789\n",
      "Epoch 18:\n",
      "Train loss: 0.6923, Train acc: 0.7306\n",
      "Valid loss: 1.1220, Valid acc: 0.5881\n",
      "Epoch 19:\n",
      "Train loss: 0.6502, Train acc: 0.7478\n",
      "Valid loss: 0.9534, Valid acc: 0.6358\n",
      "Epoch 20:\n",
      "Train loss: 0.6175, Train acc: 0.7675\n",
      "Valid loss: 0.9455, Valid acc: 0.6156\n",
      "Epoch 21:\n",
      "Train loss: 0.5868, Train acc: 0.7808\n",
      "Valid loss: 0.9199, Valid acc: 0.6624\n",
      "Epoch 22:\n",
      "Train loss: 0.5594, Train acc: 0.8044\n",
      "Valid loss: 0.9384, Valid acc: 0.6853\n",
      "Epoch 23:\n",
      "Train loss: 0.5649, Train acc: 0.8067\n",
      "Valid loss: 0.8915, Valid acc: 0.6587\n",
      "Epoch 24:\n",
      "Train loss: 0.5114, Train acc: 0.8338\n",
      "Valid loss: 0.8616, Valid acc: 0.7165\n",
      "Epoch 25:\n",
      "Train loss: 0.4884, Train acc: 0.8391\n",
      "Valid loss: 1.0947, Valid acc: 0.6349\n",
      "Epoch 26:\n",
      "Train loss: 0.4504, Train acc: 0.8629\n",
      "Valid loss: 0.8904, Valid acc: 0.7119\n",
      "Epoch 27:\n",
      "Train loss: 0.4323, Train acc: 0.8696\n",
      "Valid loss: 1.1151, Valid acc: 0.6495\n",
      "Epoch 28:\n",
      "Train loss: 0.4045, Train acc: 0.8806\n",
      "Valid loss: 0.9006, Valid acc: 0.7239\n",
      "Epoch 29:\n",
      "Train loss: 0.3705, Train acc: 0.8984\n",
      "Valid loss: 1.1273, Valid acc: 0.6486\n",
      "Epoch 30:\n",
      "Train loss: 0.3807, Train acc: 0.8943\n",
      "Valid loss: 0.9103, Valid acc: 0.7193\n",
      "Epoch 31:\n",
      "Train loss: 0.3168, Train acc: 0.9124\n",
      "Valid loss: 0.9665, Valid acc: 0.7083\n",
      "Epoch 32:\n",
      "Train loss: 0.3187, Train acc: 0.9145\n",
      "Valid loss: 0.8643, Valid acc: 0.7422\n",
      "Epoch 33:\n",
      "Train loss: 0.3120, Train acc: 0.9163\n",
      "Valid loss: 0.9931, Valid acc: 0.7156\n",
      "Epoch 34:\n",
      "Train loss: 0.3432, Train acc: 0.9085\n",
      "Valid loss: 0.9044, Valid acc: 0.7450\n",
      "Epoch 35:\n",
      "Train loss: 0.2626, Train acc: 0.9303\n",
      "Valid loss: 0.9260, Valid acc: 0.7459\n",
      "Epoch 36:\n",
      "Train loss: 0.3005, Train acc: 0.9186\n",
      "Valid loss: 0.8060, Valid acc: 0.7734\n",
      "Epoch 37:\n",
      "Train loss: 0.2924, Train acc: 0.9193\n",
      "Valid loss: 0.8144, Valid acc: 0.7697\n",
      "Epoch 38:\n",
      "Train loss: 0.2011, Train acc: 0.9468\n",
      "Valid loss: 0.8213, Valid acc: 0.7807\n",
      "Epoch 39:\n",
      "Train loss: 0.3219, Train acc: 0.9223\n",
      "Valid loss: 0.8388, Valid acc: 0.7798\n",
      "Epoch 40:\n",
      "Train loss: 0.2308, Train acc: 0.9436\n",
      "Valid loss: 0.8576, Valid acc: 0.7725\n",
      "Epoch 41:\n",
      "Train loss: 0.2252, Train acc: 0.9448\n",
      "Valid loss: 1.0512, Valid acc: 0.7450\n",
      "Epoch 42:\n",
      "Train loss: 0.2466, Train acc: 0.9365\n",
      "Valid loss: 0.9315, Valid acc: 0.7486\n",
      "Epoch 43:\n",
      "Train loss: 0.1913, Train acc: 0.9546\n",
      "Valid loss: 0.7732, Valid acc: 0.7917\n",
      "Epoch 44:\n",
      "Train loss: 0.2109, Train acc: 0.9448\n",
      "Valid loss: 0.7931, Valid acc: 0.8083\n",
      "Epoch 45:\n",
      "Train loss: 0.2363, Train acc: 0.9386\n",
      "Valid loss: 0.7400, Valid acc: 0.8147\n",
      "Epoch 46:\n",
      "Train loss: 0.1984, Train acc: 0.9489\n",
      "Valid loss: 0.7686, Valid acc: 0.8064\n",
      "Epoch 47:\n",
      "Train loss: 0.1552, Train acc: 0.9622\n",
      "Valid loss: 0.8070, Valid acc: 0.7917\n",
      "Epoch 48:\n",
      "Train loss: 0.2373, Train acc: 0.9427\n",
      "Valid loss: 0.8455, Valid acc: 0.7780\n",
      "Epoch 49:\n",
      "Train loss: 0.1581, Train acc: 0.9645\n",
      "Valid loss: 0.7765, Valid acc: 0.8028\n",
      "Epoch 50:\n",
      "Train loss: 0.1854, Train acc: 0.9587\n",
      "Valid loss: 0.8461, Valid acc: 0.7927\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy L2 Regularization (Dropout: 0.0, Weight Decay: 0.0005, Max Norm: 0.0): 0.8160\n",
      "Testing L2 Regularization with weight decay: 0.001\n",
      "Epoch 1:\n",
      "Train loss: 1.6526, Train acc: 0.2476\n",
      "Valid loss: 1.6544, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6477, Train acc: 0.2309\n",
      "Valid loss: 1.6446, Valid acc: 0.2670\n",
      "Epoch 3:\n",
      "Train loss: 1.6451, Train acc: 0.2414\n",
      "Valid loss: 1.6514, Valid acc: 0.2394\n",
      "Epoch 4:\n",
      "Train loss: 1.5735, Train acc: 0.3267\n",
      "Valid loss: 1.4866, Valid acc: 0.3908\n",
      "Epoch 5:\n",
      "Train loss: 1.4248, Train acc: 0.3982\n",
      "Valid loss: 1.3991, Valid acc: 0.3899\n",
      "Epoch 6:\n",
      "Train loss: 1.3164, Train acc: 0.4317\n",
      "Valid loss: 1.3124, Valid acc: 0.4532\n",
      "Epoch 7:\n",
      "Train loss: 1.2208, Train acc: 0.4828\n",
      "Valid loss: 1.2067, Valid acc: 0.5193\n",
      "Epoch 8:\n",
      "Train loss: 1.1187, Train acc: 0.5335\n",
      "Valid loss: 1.1134, Valid acc: 0.5844\n",
      "Epoch 9:\n",
      "Train loss: 1.0058, Train acc: 0.6098\n",
      "Valid loss: 1.0725, Valid acc: 0.6018\n",
      "Epoch 10:\n",
      "Train loss: 0.9090, Train acc: 0.6598\n",
      "Valid loss: 0.9473, Valid acc: 0.6394\n",
      "Epoch 11:\n",
      "Train loss: 0.8305, Train acc: 0.7020\n",
      "Valid loss: 0.8592, Valid acc: 0.7083\n",
      "Epoch 12:\n",
      "Train loss: 0.7768, Train acc: 0.7293\n",
      "Valid loss: 0.8645, Valid acc: 0.6917\n",
      "Epoch 13:\n",
      "Train loss: 0.7339, Train acc: 0.7462\n",
      "Valid loss: 0.8673, Valid acc: 0.7009\n",
      "Epoch 14:\n",
      "Train loss: 0.6943, Train acc: 0.7781\n",
      "Valid loss: 0.7838, Valid acc: 0.7596\n",
      "Epoch 15:\n",
      "Train loss: 0.6487, Train acc: 0.7950\n",
      "Valid loss: 0.7471, Valid acc: 0.7752\n",
      "Epoch 16:\n",
      "Train loss: 0.5810, Train acc: 0.8239\n",
      "Valid loss: 0.8549, Valid acc: 0.7339\n",
      "Epoch 17:\n",
      "Train loss: 0.5522, Train acc: 0.8363\n",
      "Valid loss: 1.1905, Valid acc: 0.6670\n",
      "Epoch 18:\n",
      "Train loss: 0.5465, Train acc: 0.8441\n",
      "Valid loss: 0.7250, Valid acc: 0.7853\n",
      "Epoch 19:\n",
      "Train loss: 0.4841, Train acc: 0.8647\n",
      "Valid loss: 0.7240, Valid acc: 0.7917\n",
      "Epoch 20:\n",
      "Train loss: 0.4859, Train acc: 0.8643\n",
      "Valid loss: 0.7477, Valid acc: 0.7706\n",
      "Epoch 21:\n",
      "Train loss: 0.4879, Train acc: 0.8689\n",
      "Valid loss: 0.7047, Valid acc: 0.8064\n",
      "Epoch 22:\n",
      "Train loss: 0.4437, Train acc: 0.8819\n",
      "Valid loss: 0.7764, Valid acc: 0.7780\n",
      "Epoch 23:\n",
      "Train loss: 0.4124, Train acc: 0.8897\n",
      "Valid loss: 0.7200, Valid acc: 0.8037\n",
      "Epoch 24:\n",
      "Train loss: 0.4389, Train acc: 0.8858\n",
      "Valid loss: 0.6591, Valid acc: 0.8165\n",
      "Epoch 25:\n",
      "Train loss: 0.4008, Train acc: 0.8950\n",
      "Valid loss: 0.6764, Valid acc: 0.8092\n",
      "Epoch 26:\n",
      "Train loss: 0.3922, Train acc: 0.8980\n",
      "Valid loss: 0.6987, Valid acc: 0.8073\n",
      "Epoch 27:\n",
      "Train loss: 0.3789, Train acc: 0.9033\n",
      "Valid loss: 0.6805, Valid acc: 0.8174\n",
      "Epoch 28:\n",
      "Train loss: 0.3873, Train acc: 0.9014\n",
      "Valid loss: 0.7261, Valid acc: 0.8055\n",
      "Epoch 29:\n",
      "Train loss: 0.3673, Train acc: 0.9090\n",
      "Valid loss: 0.7354, Valid acc: 0.7972\n",
      "Epoch 30:\n",
      "Train loss: 0.3379, Train acc: 0.9131\n",
      "Valid loss: 0.6740, Valid acc: 0.8128\n",
      "Epoch 31:\n",
      "Train loss: 0.3144, Train acc: 0.9227\n",
      "Valid loss: 1.7835, Valid acc: 0.4945\n",
      "Epoch 32:\n",
      "Train loss: 0.3645, Train acc: 0.9035\n",
      "Valid loss: 1.1843, Valid acc: 0.7037\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy L2 Regularization (Dropout: 0.0, Weight Decay: 0.001, Max Norm: 0.0): 0.6220\n"
     ]
    }
   ],
   "source": [
    "# Regularisation - L2 Regularization/Weight Decay ONLY (varying values)\n",
    "weight_decay = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\n",
    "for weight_decay in weight_decay:\n",
    "    print(f\"Testing L2 Regularization with weight decay: {weight_decay}\")\n",
    "    best_reg_technique.append(regularisation_test(weight_decay=weight_decay,dropout=0.0, grad_clip=False, max_norm=0.0, reg_technique=\"L2 Regularization\", optimizer=optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e412dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Dropout: 0.2\n",
      "Epoch 1:\n",
      "Train loss: 1.6601, Train acc: 0.2327\n",
      "Valid loss: 1.6528, Valid acc: 0.2367\n",
      "Epoch 2:\n",
      "Train loss: 1.6495, Train acc: 0.2251\n",
      "Valid loss: 1.6495, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6425, Train acc: 0.2446\n",
      "Valid loss: 1.6383, Valid acc: 0.2587\n",
      "Epoch 4:\n",
      "Train loss: 1.4962, Train acc: 0.3746\n",
      "Valid loss: 1.4475, Valid acc: 0.4092\n",
      "Epoch 5:\n",
      "Train loss: 1.3582, Train acc: 0.4264\n",
      "Valid loss: 1.3384, Valid acc: 0.4275\n",
      "Epoch 6:\n",
      "Train loss: 1.2013, Train acc: 0.5149\n",
      "Valid loss: 1.1322, Valid acc: 0.5624\n",
      "Epoch 7:\n",
      "Train loss: 1.0784, Train acc: 0.5697\n",
      "Valid loss: 1.0840, Valid acc: 0.6101\n",
      "Epoch 8:\n",
      "Train loss: 0.9946, Train acc: 0.6071\n",
      "Valid loss: 1.3016, Valid acc: 0.4450\n",
      "Epoch 9:\n",
      "Train loss: 0.9127, Train acc: 0.6451\n",
      "Valid loss: 1.1689, Valid acc: 0.5495\n",
      "Epoch 10:\n",
      "Train loss: 0.8554, Train acc: 0.6715\n",
      "Valid loss: 1.1441, Valid acc: 0.5917\n",
      "Epoch 11:\n",
      "Train loss: 0.8176, Train acc: 0.6740\n",
      "Valid loss: 1.0819, Valid acc: 0.5963\n",
      "Epoch 12:\n",
      "Train loss: 0.7564, Train acc: 0.6985\n",
      "Valid loss: 0.9711, Valid acc: 0.6294\n",
      "Epoch 13:\n",
      "Train loss: 0.7326, Train acc: 0.7031\n",
      "Valid loss: 1.0132, Valid acc: 0.6028\n",
      "Epoch 14:\n",
      "Train loss: 0.7006, Train acc: 0.7118\n",
      "Valid loss: 0.8806, Valid acc: 0.6450\n",
      "Epoch 15:\n",
      "Train loss: 0.6603, Train acc: 0.7208\n",
      "Valid loss: 0.8987, Valid acc: 0.6697\n",
      "Epoch 16:\n",
      "Train loss: 0.6437, Train acc: 0.7203\n",
      "Valid loss: 0.9761, Valid acc: 0.6321\n",
      "Epoch 17:\n",
      "Train loss: 0.6346, Train acc: 0.7382\n",
      "Valid loss: 1.0951, Valid acc: 0.6092\n",
      "Epoch 18:\n",
      "Train loss: 0.6028, Train acc: 0.7545\n",
      "Valid loss: 0.9037, Valid acc: 0.6606\n",
      "Epoch 19:\n",
      "Train loss: 0.5918, Train acc: 0.7620\n",
      "Valid loss: 1.1151, Valid acc: 0.6174\n",
      "Epoch 20:\n",
      "Train loss: 0.5637, Train acc: 0.7657\n",
      "Valid loss: 1.0322, Valid acc: 0.6294\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Dropout (Dropout: 0.2, Weight Decay: 0.0, Max Norm: 0.0): 0.5160\n",
      "Testing Dropout: 0.3\n",
      "Epoch 1:\n",
      "Train loss: 1.6628, Train acc: 0.2249\n",
      "Valid loss: 1.6494, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6532, Train acc: 0.2194\n",
      "Valid loss: 1.6435, Valid acc: 0.2495\n",
      "Epoch 3:\n",
      "Train loss: 1.6481, Train acc: 0.2304\n",
      "Valid loss: 1.6413, Valid acc: 0.2477\n",
      "Epoch 4:\n",
      "Train loss: 1.6403, Train acc: 0.2460\n",
      "Valid loss: 1.6394, Valid acc: 0.2560\n",
      "Epoch 5:\n",
      "Train loss: 1.5147, Train acc: 0.3496\n",
      "Valid loss: 1.4176, Valid acc: 0.3661\n",
      "Epoch 6:\n",
      "Train loss: 1.2874, Train acc: 0.4640\n",
      "Valid loss: 1.1857, Valid acc: 0.5000\n",
      "Epoch 7:\n",
      "Train loss: 1.1209, Train acc: 0.5257\n",
      "Valid loss: 1.0873, Valid acc: 0.5826\n",
      "Epoch 8:\n",
      "Train loss: 1.0061, Train acc: 0.5807\n",
      "Valid loss: 1.0088, Valid acc: 0.5807\n",
      "Epoch 9:\n",
      "Train loss: 0.9523, Train acc: 0.5956\n",
      "Valid loss: 1.1876, Valid acc: 0.5055\n",
      "Epoch 10:\n",
      "Train loss: 0.8905, Train acc: 0.6353\n",
      "Valid loss: 0.9516, Valid acc: 0.6138\n",
      "Epoch 11:\n",
      "Train loss: 0.8594, Train acc: 0.6625\n",
      "Valid loss: 0.9876, Valid acc: 0.6083\n",
      "Epoch 12:\n",
      "Train loss: 0.7922, Train acc: 0.6882\n",
      "Valid loss: 0.9080, Valid acc: 0.6514\n",
      "Epoch 13:\n",
      "Train loss: 0.7509, Train acc: 0.7045\n",
      "Valid loss: 0.8961, Valid acc: 0.6459\n",
      "Epoch 14:\n",
      "Train loss: 0.7492, Train acc: 0.7084\n",
      "Valid loss: 1.0515, Valid acc: 0.5881\n",
      "Epoch 15:\n",
      "Train loss: 0.7009, Train acc: 0.7377\n",
      "Valid loss: 0.8586, Valid acc: 0.6954\n",
      "Epoch 16:\n",
      "Train loss: 0.6620, Train acc: 0.7547\n",
      "Valid loss: 1.0646, Valid acc: 0.6248\n",
      "Epoch 17:\n",
      "Train loss: 0.6347, Train acc: 0.7797\n",
      "Valid loss: 0.8362, Valid acc: 0.6899\n",
      "Epoch 18:\n",
      "Train loss: 0.6047, Train acc: 0.8051\n",
      "Valid loss: 0.8069, Valid acc: 0.7193\n",
      "Epoch 19:\n",
      "Train loss: 0.5894, Train acc: 0.8246\n",
      "Valid loss: 0.8399, Valid acc: 0.7312\n",
      "Epoch 20:\n",
      "Train loss: 0.5106, Train acc: 0.8599\n",
      "Valid loss: 0.8825, Valid acc: 0.7202\n",
      "Epoch 21:\n",
      "Train loss: 0.4960, Train acc: 0.8652\n",
      "Valid loss: 0.8087, Valid acc: 0.7569\n",
      "Epoch 22:\n",
      "Train loss: 0.4252, Train acc: 0.8923\n",
      "Valid loss: 1.0176, Valid acc: 0.6991\n",
      "Epoch 23:\n",
      "Train loss: 0.4133, Train acc: 0.8911\n",
      "Valid loss: 0.8600, Valid acc: 0.7495\n",
      "Epoch 24:\n",
      "Train loss: 0.3943, Train acc: 0.8996\n",
      "Valid loss: 0.8188, Valid acc: 0.7725\n",
      "Epoch 25:\n",
      "Train loss: 0.3842, Train acc: 0.9046\n",
      "Valid loss: 1.1130, Valid acc: 0.6936\n",
      "Epoch 26:\n",
      "Train loss: 0.3540, Train acc: 0.9168\n",
      "Valid loss: 0.9334, Valid acc: 0.7541\n",
      "Epoch 27:\n",
      "Train loss: 0.3515, Train acc: 0.9115\n",
      "Valid loss: 0.9243, Valid acc: 0.7349\n",
      "Epoch 28:\n",
      "Train loss: 0.3344, Train acc: 0.9191\n",
      "Valid loss: 0.7716, Valid acc: 0.7982\n",
      "Epoch 29:\n",
      "Train loss: 0.3300, Train acc: 0.9170\n",
      "Valid loss: 0.8397, Valid acc: 0.7633\n",
      "Epoch 30:\n",
      "Train loss: 0.2621, Train acc: 0.9402\n",
      "Valid loss: 0.7807, Valid acc: 0.7927\n",
      "Epoch 31:\n",
      "Train loss: 0.3029, Train acc: 0.9227\n",
      "Valid loss: 0.8837, Valid acc: 0.7569\n",
      "Epoch 32:\n",
      "Train loss: 0.2900, Train acc: 0.9262\n",
      "Valid loss: 0.8559, Valid acc: 0.7670\n",
      "Epoch 33:\n",
      "Train loss: 0.2897, Train acc: 0.9280\n",
      "Valid loss: 0.8103, Valid acc: 0.7890\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Dropout (Dropout: 0.3, Weight Decay: 0.0, Max Norm: 0.0): 0.8400\n",
      "Testing Dropout: 0.4\n",
      "Epoch 1:\n",
      "Train loss: 1.6652, Train acc: 0.2267\n",
      "Valid loss: 1.6681, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6515, Train acc: 0.2258\n",
      "Valid loss: 1.6420, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6496, Train acc: 0.2322\n",
      "Valid loss: 1.6435, Valid acc: 0.2541\n",
      "Epoch 4:\n",
      "Train loss: 1.5351, Train acc: 0.3448\n",
      "Valid loss: 1.4788, Valid acc: 0.3394\n",
      "Epoch 5:\n",
      "Train loss: 1.3825, Train acc: 0.4083\n",
      "Valid loss: 1.2962, Valid acc: 0.4982\n",
      "Epoch 6:\n",
      "Train loss: 1.2277, Train acc: 0.5066\n",
      "Valid loss: 1.2408, Valid acc: 0.5000\n",
      "Epoch 7:\n",
      "Train loss: 1.1144, Train acc: 0.5511\n",
      "Valid loss: 1.0580, Valid acc: 0.6037\n",
      "Epoch 8:\n",
      "Train loss: 1.0264, Train acc: 0.5956\n",
      "Valid loss: 1.0425, Valid acc: 0.6138\n",
      "Epoch 9:\n",
      "Train loss: 0.9575, Train acc: 0.6291\n",
      "Valid loss: 0.9743, Valid acc: 0.6394\n",
      "Epoch 10:\n",
      "Train loss: 0.8945, Train acc: 0.6577\n",
      "Valid loss: 1.8058, Valid acc: 0.4321\n",
      "Epoch 11:\n",
      "Train loss: 0.8588, Train acc: 0.6740\n",
      "Valid loss: 0.9355, Valid acc: 0.6477\n",
      "Epoch 12:\n",
      "Train loss: 0.7943, Train acc: 0.6889\n",
      "Valid loss: 0.9901, Valid acc: 0.6275\n",
      "Epoch 13:\n",
      "Train loss: 0.7629, Train acc: 0.6907\n",
      "Valid loss: 0.9660, Valid acc: 0.6468\n",
      "Epoch 14:\n",
      "Train loss: 0.7268, Train acc: 0.7125\n",
      "Valid loss: 0.9382, Valid acc: 0.6532\n",
      "Epoch 15:\n",
      "Train loss: 0.7093, Train acc: 0.7093\n",
      "Valid loss: 0.9265, Valid acc: 0.6633\n",
      "Epoch 16:\n",
      "Train loss: 0.7015, Train acc: 0.7079\n",
      "Valid loss: 0.9430, Valid acc: 0.6615\n",
      "Epoch 17:\n",
      "Train loss: 0.6140, Train acc: 0.7315\n",
      "Valid loss: 0.8806, Valid acc: 0.6872\n",
      "Epoch 18:\n",
      "Train loss: 0.6239, Train acc: 0.7350\n",
      "Valid loss: 1.1029, Valid acc: 0.6101\n",
      "Epoch 19:\n",
      "Train loss: 0.6270, Train acc: 0.7341\n",
      "Valid loss: 0.9157, Valid acc: 0.6706\n",
      "Epoch 20:\n",
      "Train loss: 0.5642, Train acc: 0.7451\n",
      "Valid loss: 1.0151, Valid acc: 0.6349\n",
      "Epoch 21:\n",
      "Train loss: 0.5739, Train acc: 0.7476\n",
      "Valid loss: 1.1735, Valid acc: 0.6138\n",
      "Epoch 22:\n",
      "Train loss: 0.5652, Train acc: 0.7471\n",
      "Valid loss: 0.8902, Valid acc: 0.6872\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Dropout (Dropout: 0.4, Weight Decay: 0.0, Max Norm: 0.0): 0.6360\n"
     ]
    }
   ],
   "source": [
    "# Regularisation - Dropout ONLY (varying values)\n",
    "dropout = [0.2, 0.3, 0.4]\n",
    "for dropout in dropout:\n",
    "    print(f\"Testing Dropout: {dropout}\")\n",
    "    best_reg_technique.append(regularisation_test(weight_decay=0.0,dropout=dropout, grad_clip=False, max_norm=0.0, reg_technique=\"Dropout\", optimizer=optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "500d83f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gradient Clipping with Value: 0.1\n",
      "Epoch 1:\n",
      "Train loss: 1.6586, Train acc: 0.2279\n",
      "Valid loss: 1.6563, Valid acc: 0.2037\n",
      "Epoch 2:\n",
      "Train loss: 1.6476, Train acc: 0.2348\n",
      "Valid loss: 1.6608, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6441, Train acc: 0.2423\n",
      "Valid loss: 1.6474, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.5025, Train acc: 0.3576\n",
      "Valid loss: 1.4060, Valid acc: 0.4073\n",
      "Epoch 5:\n",
      "Train loss: 1.3227, Train acc: 0.4305\n",
      "Valid loss: 1.2924, Valid acc: 0.4330\n",
      "Epoch 6:\n",
      "Train loss: 1.2021, Train acc: 0.4762\n",
      "Valid loss: 1.2056, Valid acc: 0.5046\n",
      "Epoch 7:\n",
      "Train loss: 1.0857, Train acc: 0.5397\n",
      "Valid loss: 1.1213, Valid acc: 0.5486\n",
      "Epoch 8:\n",
      "Train loss: 1.0073, Train acc: 0.5949\n",
      "Valid loss: 1.0842, Valid acc: 0.5807\n",
      "Epoch 9:\n",
      "Train loss: 0.9130, Train acc: 0.6410\n",
      "Valid loss: 0.9706, Valid acc: 0.6018\n",
      "Epoch 10:\n",
      "Train loss: 0.8547, Train acc: 0.6795\n",
      "Valid loss: 0.9213, Valid acc: 0.6725\n",
      "Epoch 11:\n",
      "Train loss: 0.8116, Train acc: 0.7066\n",
      "Valid loss: 1.0046, Valid acc: 0.6110\n",
      "Epoch 12:\n",
      "Train loss: 0.7293, Train acc: 0.7563\n",
      "Valid loss: 1.1705, Valid acc: 0.6248\n",
      "Epoch 13:\n",
      "Train loss: 0.6872, Train acc: 0.7898\n",
      "Valid loss: 0.9278, Valid acc: 0.6963\n",
      "Epoch 14:\n",
      "Train loss: 0.6179, Train acc: 0.8187\n",
      "Valid loss: 1.0057, Valid acc: 0.6780\n",
      "Epoch 15:\n",
      "Train loss: 0.5806, Train acc: 0.8349\n",
      "Valid loss: 0.9111, Valid acc: 0.7294\n",
      "Epoch 16:\n",
      "Train loss: 0.5615, Train acc: 0.8418\n",
      "Valid loss: 0.9643, Valid acc: 0.7083\n",
      "Epoch 17:\n",
      "Train loss: 0.5237, Train acc: 0.8597\n",
      "Valid loss: 0.8464, Valid acc: 0.7450\n",
      "Epoch 18:\n",
      "Train loss: 0.4957, Train acc: 0.8666\n",
      "Valid loss: 0.8700, Valid acc: 0.7633\n",
      "Epoch 19:\n",
      "Train loss: 0.4859, Train acc: 0.8730\n",
      "Valid loss: 0.8934, Valid acc: 0.7413\n",
      "Epoch 20:\n",
      "Train loss: 0.4570, Train acc: 0.8806\n",
      "Valid loss: 0.8643, Valid acc: 0.7688\n",
      "Epoch 21:\n",
      "Train loss: 0.4390, Train acc: 0.8893\n",
      "Valid loss: 0.8620, Valid acc: 0.7578\n",
      "Epoch 22:\n",
      "Train loss: 0.4324, Train acc: 0.8927\n",
      "Valid loss: 0.8858, Valid acc: 0.7495\n",
      "Epoch 23:\n",
      "Train loss: 0.4120, Train acc: 0.8982\n",
      "Valid loss: 0.8087, Valid acc: 0.7780\n",
      "Epoch 24:\n",
      "Train loss: 0.4057, Train acc: 0.9012\n",
      "Valid loss: 0.8667, Valid acc: 0.7743\n",
      "Epoch 25:\n",
      "Train loss: 0.3834, Train acc: 0.9088\n",
      "Valid loss: 0.9432, Valid acc: 0.7523\n",
      "Epoch 26:\n",
      "Train loss: 0.3741, Train acc: 0.9117\n",
      "Valid loss: 0.8329, Valid acc: 0.7899\n",
      "Epoch 27:\n",
      "Train loss: 0.3902, Train acc: 0.9085\n",
      "Valid loss: 0.8377, Valid acc: 0.7936\n",
      "Epoch 28:\n",
      "Train loss: 0.3442, Train acc: 0.9227\n",
      "Valid loss: 0.8663, Valid acc: 0.7853\n",
      "Epoch 29:\n",
      "Train loss: 0.3287, Train acc: 0.9232\n",
      "Valid loss: 1.0050, Valid acc: 0.7486\n",
      "Epoch 30:\n",
      "Train loss: 0.3344, Train acc: 0.9248\n",
      "Valid loss: 0.9352, Valid acc: 0.7817\n",
      "Epoch 31:\n",
      "Train loss: 0.3312, Train acc: 0.9257\n",
      "Valid loss: 1.0958, Valid acc: 0.7358\n",
      "Epoch 32:\n",
      "Train loss: 0.3191, Train acc: 0.9269\n",
      "Valid loss: 0.9407, Valid acc: 0.7936\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Gradient Clipping (Dropout: 0.0, Weight Decay: 0.0, Max Norm: 0.1): 0.8380\n",
      "Testing Gradient Clipping with Value: 0.5\n",
      "Epoch 1:\n",
      "Train loss: 1.6592, Train acc: 0.2352\n",
      "Valid loss: 1.6496, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6449, Train acc: 0.2309\n",
      "Valid loss: 1.6516, Valid acc: 0.2128\n",
      "Epoch 3:\n",
      "Train loss: 1.6143, Train acc: 0.2554\n",
      "Valid loss: 1.5472, Valid acc: 0.2972\n",
      "Epoch 4:\n",
      "Train loss: 1.4109, Train acc: 0.3916\n",
      "Valid loss: 1.3857, Valid acc: 0.3945\n",
      "Epoch 5:\n",
      "Train loss: 1.2114, Train acc: 0.4883\n",
      "Valid loss: 1.3007, Valid acc: 0.4239\n",
      "Epoch 6:\n",
      "Train loss: 1.0874, Train acc: 0.5415\n",
      "Valid loss: 1.0928, Valid acc: 0.5523\n",
      "Epoch 7:\n",
      "Train loss: 1.0075, Train acc: 0.5857\n",
      "Valid loss: 1.0919, Valid acc: 0.5376\n",
      "Epoch 8:\n",
      "Train loss: 0.9310, Train acc: 0.6444\n",
      "Valid loss: 1.0092, Valid acc: 0.5927\n",
      "Epoch 9:\n",
      "Train loss: 0.8724, Train acc: 0.6747\n",
      "Valid loss: 1.2160, Valid acc: 0.5330\n",
      "Epoch 10:\n",
      "Train loss: 0.8163, Train acc: 0.7040\n",
      "Valid loss: 1.0983, Valid acc: 0.5459\n",
      "Epoch 11:\n",
      "Train loss: 0.7551, Train acc: 0.7506\n",
      "Valid loss: 0.9752, Valid acc: 0.6716\n",
      "Epoch 12:\n",
      "Train loss: 0.7134, Train acc: 0.7632\n",
      "Valid loss: 0.9290, Valid acc: 0.6761\n",
      "Epoch 13:\n",
      "Train loss: 0.6590, Train acc: 0.7948\n",
      "Valid loss: 0.9004, Valid acc: 0.6963\n",
      "Epoch 14:\n",
      "Train loss: 0.6009, Train acc: 0.8193\n",
      "Valid loss: 0.8195, Valid acc: 0.7495\n",
      "Epoch 15:\n",
      "Train loss: 0.5532, Train acc: 0.8381\n",
      "Valid loss: 0.9322, Valid acc: 0.7138\n",
      "Epoch 16:\n",
      "Train loss: 0.5110, Train acc: 0.8508\n",
      "Valid loss: 1.1456, Valid acc: 0.6936\n",
      "Epoch 17:\n",
      "Train loss: 0.5083, Train acc: 0.8517\n",
      "Valid loss: 0.8250, Valid acc: 0.7486\n",
      "Epoch 18:\n",
      "Train loss: 0.4743, Train acc: 0.8712\n",
      "Valid loss: 0.7777, Valid acc: 0.7844\n",
      "Epoch 19:\n",
      "Train loss: 0.4556, Train acc: 0.8755\n",
      "Valid loss: 0.9009, Valid acc: 0.7220\n",
      "Epoch 20:\n",
      "Train loss: 0.4144, Train acc: 0.8884\n",
      "Valid loss: 0.7839, Valid acc: 0.7798\n",
      "Epoch 21:\n",
      "Train loss: 0.4085, Train acc: 0.8925\n",
      "Valid loss: 0.9606, Valid acc: 0.7284\n",
      "Epoch 22:\n",
      "Train loss: 0.3796, Train acc: 0.8980\n",
      "Valid loss: 0.7590, Valid acc: 0.7917\n",
      "Epoch 23:\n",
      "Train loss: 0.3762, Train acc: 0.9039\n",
      "Valid loss: 0.8091, Valid acc: 0.7771\n",
      "Epoch 24:\n",
      "Train loss: 0.3575, Train acc: 0.9106\n",
      "Valid loss: 0.8139, Valid acc: 0.7872\n",
      "Epoch 25:\n",
      "Train loss: 0.3518, Train acc: 0.9104\n",
      "Valid loss: 0.8551, Valid acc: 0.7633\n",
      "Epoch 26:\n",
      "Train loss: 0.3344, Train acc: 0.9147\n",
      "Valid loss: 0.9140, Valid acc: 0.7560\n",
      "Epoch 27:\n",
      "Train loss: 0.3196, Train acc: 0.9182\n",
      "Valid loss: 0.8983, Valid acc: 0.7752\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Gradient Clipping (Dropout: 0.0, Weight Decay: 0.0, Max Norm: 0.5): 0.8140\n",
      "Testing Gradient Clipping with Value: 1.0\n",
      "Epoch 1:\n",
      "Train loss: 1.6619, Train acc: 0.2148\n",
      "Valid loss: 1.6449, Valid acc: 0.2450\n",
      "Epoch 2:\n",
      "Train loss: 1.6470, Train acc: 0.2448\n",
      "Valid loss: 1.6416, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.6170, Train acc: 0.2776\n",
      "Valid loss: 1.6012, Valid acc: 0.3220\n",
      "Epoch 4:\n",
      "Train loss: 1.4189, Train acc: 0.4030\n",
      "Valid loss: 1.3494, Valid acc: 0.4312\n",
      "Epoch 5:\n",
      "Train loss: 1.2664, Train acc: 0.4716\n",
      "Valid loss: 1.2383, Valid acc: 0.5349\n",
      "Epoch 6:\n",
      "Train loss: 1.1184, Train acc: 0.5447\n",
      "Valid loss: 1.0954, Valid acc: 0.5798\n",
      "Epoch 7:\n",
      "Train loss: 1.0408, Train acc: 0.5853\n",
      "Valid loss: 1.1033, Valid acc: 0.5862\n",
      "Epoch 8:\n",
      "Train loss: 0.9578, Train acc: 0.6298\n",
      "Valid loss: 1.0032, Valid acc: 0.6294\n",
      "Epoch 9:\n",
      "Train loss: 0.8851, Train acc: 0.6529\n",
      "Valid loss: 1.1275, Valid acc: 0.5862\n",
      "Epoch 10:\n",
      "Train loss: 0.8483, Train acc: 0.6710\n",
      "Valid loss: 1.0375, Valid acc: 0.6202\n",
      "Epoch 11:\n",
      "Train loss: 0.8245, Train acc: 0.6818\n",
      "Valid loss: 1.0154, Valid acc: 0.6211\n",
      "Epoch 12:\n",
      "Train loss: 0.7866, Train acc: 0.6962\n",
      "Valid loss: 0.9548, Valid acc: 0.6450\n",
      "Epoch 13:\n",
      "Train loss: 0.7415, Train acc: 0.7116\n",
      "Valid loss: 0.9415, Valid acc: 0.6422\n",
      "Epoch 14:\n",
      "Train loss: 0.7252, Train acc: 0.7244\n",
      "Valid loss: 1.0618, Valid acc: 0.6165\n",
      "Epoch 15:\n",
      "Train loss: 0.6911, Train acc: 0.7529\n",
      "Valid loss: 1.0819, Valid acc: 0.5982\n",
      "Epoch 16:\n",
      "Train loss: 0.6721, Train acc: 0.7673\n",
      "Valid loss: 1.0435, Valid acc: 0.6642\n",
      "Epoch 17:\n",
      "Train loss: 0.6153, Train acc: 0.8079\n",
      "Valid loss: 1.0636, Valid acc: 0.6376\n",
      "Epoch 18:\n",
      "Train loss: 0.5667, Train acc: 0.8283\n",
      "Valid loss: 1.0128, Valid acc: 0.6789\n",
      "Epoch 19:\n",
      "Train loss: 0.5305, Train acc: 0.8475\n",
      "Valid loss: 1.0070, Valid acc: 0.6936\n",
      "Epoch 20:\n",
      "Train loss: 0.5051, Train acc: 0.8579\n",
      "Valid loss: 0.9196, Valid acc: 0.7303\n",
      "Epoch 21:\n",
      "Train loss: 0.4751, Train acc: 0.8686\n",
      "Valid loss: 1.0324, Valid acc: 0.6991\n",
      "Epoch 22:\n",
      "Train loss: 0.4651, Train acc: 0.8751\n",
      "Valid loss: 1.0669, Valid acc: 0.6982\n",
      "Epoch 23:\n",
      "Train loss: 0.4407, Train acc: 0.8801\n",
      "Valid loss: 0.9570, Valid acc: 0.7229\n",
      "Epoch 24:\n",
      "Train loss: 0.4163, Train acc: 0.8881\n",
      "Valid loss: 1.2801, Valid acc: 0.6165\n",
      "Epoch 25:\n",
      "Train loss: 0.4049, Train acc: 0.8945\n",
      "Valid loss: 1.1868, Valid acc: 0.6826\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Gradient Clipping (Dropout: 0.0, Weight Decay: 0.0, Max Norm: 1.0): 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Regularisation - Gradient Clipping ONLY (varying values)\n",
    "max_norm = [0.1, 0.5, 1.0]\n",
    "for max_norm in max_norm:\n",
    "    print(f\"Testing Gradient Clipping with Value: {max_norm}\")\n",
    "    best_reg_technique.append(regularisation_test(weight_decay=0.0,dropout=0.0, grad_clip=True, max_norm=max_norm, reg_technique=\"Gradient Clipping\", optimizer=optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26f332c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "technique",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dropout",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weight_decay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "grad_clip",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "max_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "best_val_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e47e2ad0-2f2f-4feb-bf85-63d42bda696a",
       "rows": [
        [
         "0",
         "L2 Regularization",
         "0.0",
         "0.0001",
         "False",
         null,
         "0.8523",
         "0.84"
        ],
        [
         "1",
         "Dropout",
         "0.3",
         "0.0",
         "False",
         null,
         "0.7982",
         "0.84"
        ],
        [
         "2",
         "Gradient Clipping",
         "0.0",
         "0.0",
         "True",
         "0.1",
         "0.7936",
         "0.838"
        ],
        [
         "3",
         "Baseline",
         "0.0",
         "0.0",
         "False",
         null,
         "0.8073",
         "0.822"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>technique</th>\n",
       "      <th>dropout</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>max_norm</th>\n",
       "      <th>best_val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dropout</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Clipping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           technique  dropout  weight_decay  grad_clip  max_norm  \\\n",
       "0  L2 Regularization      0.0        0.0001      False       NaN   \n",
       "1            Dropout      0.3        0.0000      False       NaN   \n",
       "2  Gradient Clipping      0.0        0.0000       True       0.1   \n",
       "3           Baseline      0.0        0.0000      False       NaN   \n",
       "\n",
       "   best_val_acc  test_acc  \n",
       "0        0.8523     0.840  \n",
       "1        0.7982     0.840  \n",
       "2        0.7936     0.838  \n",
       "3        0.8073     0.822  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile results into dataframe for comparisons across strategies\n",
    "cols = ['technique', 'dropout', 'weight_decay', 'grad_clip', 'max_norm', 'best_val_acc', 'test_acc']\n",
    "\n",
    "df_best_reg_technique = pd.DataFrame(best_reg_technique)\n",
    "\n",
    "df_best_reg_technique.loc[~df_best_reg_technique['grad_clip'], 'max_norm'] = pd.NA # Hde max_norm when grad clipping isn't used\n",
    "df_best_reg_technique = df_best_reg_technique[cols]\n",
    "\n",
    "df_best_reg_technique['best_val_acc'] = df_best_reg_technique['best_val_acc'].round(4)\n",
    "df_best_reg_technique['test_acc'] = df_best_reg_technique['test_acc'].round(4)\n",
    "\n",
    "# Find best parameters per technique\n",
    "df_best_reg_technique = (\n",
    "    df_best_reg_technique.sort_values('test_acc', ascending=False)\n",
    "      .groupby('technique', as_index=False, sort=False)\n",
    "      .head(1)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_best_reg_technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79e872",
   "metadata": {},
   "source": [
    "### Answer (b):\n",
    "- Dropout ONLY with value of 0.4 gives the best test accuracy of 0.850\n",
    "- Doing solely Dropout or Gradient Clipping improve the accuracy from the baseline, but doing solely L2 regularization surprisingly decreased the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cd98796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/2xkb6hns26q621bjtf6rkkmc0000gn/T/ipykernel_12623/250750837.py:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  m = df['technique'].str.contains(pattern, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 1.6630, Train acc: 0.2267\n",
      "Valid loss: 1.6388, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6501, Train acc: 0.2375\n",
      "Valid loss: 1.6393, Valid acc: 0.2514\n",
      "Epoch 3:\n",
      "Train loss: 1.5983, Train acc: 0.2937\n",
      "Valid loss: 1.4491, Valid acc: 0.3917\n",
      "Epoch 4:\n",
      "Train loss: 1.3593, Train acc: 0.4046\n",
      "Valid loss: 1.3684, Valid acc: 0.4037\n",
      "Epoch 5:\n",
      "Train loss: 1.2295, Train acc: 0.4640\n",
      "Valid loss: 1.1967, Valid acc: 0.5284\n",
      "Epoch 6:\n",
      "Train loss: 1.1215, Train acc: 0.5410\n",
      "Valid loss: 1.0847, Valid acc: 0.5734\n",
      "Epoch 7:\n",
      "Train loss: 1.0233, Train acc: 0.5917\n",
      "Valid loss: 1.0249, Valid acc: 0.5743\n",
      "Epoch 8:\n",
      "Train loss: 0.9521, Train acc: 0.6259\n",
      "Valid loss: 1.0508, Valid acc: 0.5725\n",
      "Epoch 9:\n",
      "Train loss: 0.8851, Train acc: 0.6664\n",
      "Valid loss: 0.9893, Valid acc: 0.5954\n",
      "Epoch 10:\n",
      "Train loss: 0.8416, Train acc: 0.6901\n",
      "Valid loss: 0.9545, Valid acc: 0.6651\n",
      "Epoch 11:\n",
      "Train loss: 0.7653, Train acc: 0.7279\n",
      "Valid loss: 0.9259, Valid acc: 0.6826\n",
      "Epoch 12:\n",
      "Train loss: 0.6931, Train acc: 0.7840\n",
      "Valid loss: 0.8540, Valid acc: 0.7138\n",
      "Epoch 13:\n",
      "Train loss: 0.6335, Train acc: 0.8138\n",
      "Valid loss: 0.8526, Valid acc: 0.7248\n",
      "Epoch 14:\n",
      "Train loss: 0.5724, Train acc: 0.8379\n",
      "Valid loss: 0.8388, Valid acc: 0.7367\n",
      "Epoch 15:\n",
      "Train loss: 0.5597, Train acc: 0.8492\n",
      "Valid loss: 0.7863, Valid acc: 0.7670\n",
      "Epoch 16:\n",
      "Train loss: 0.5187, Train acc: 0.8636\n",
      "Valid loss: 0.7900, Valid acc: 0.7550\n",
      "Epoch 17:\n",
      "Train loss: 0.4841, Train acc: 0.8746\n",
      "Valid loss: 0.8060, Valid acc: 0.7716\n",
      "Epoch 18:\n",
      "Train loss: 0.5076, Train acc: 0.8668\n",
      "Valid loss: 0.9120, Valid acc: 0.7394\n",
      "Epoch 19:\n",
      "Train loss: 0.4526, Train acc: 0.8833\n",
      "Valid loss: 0.7787, Valid acc: 0.7908\n",
      "Epoch 20:\n",
      "Train loss: 0.4401, Train acc: 0.8872\n",
      "Valid loss: 0.8820, Valid acc: 0.7615\n",
      "Epoch 21:\n",
      "Train loss: 0.4302, Train acc: 0.8900\n",
      "Valid loss: 1.0194, Valid acc: 0.7257\n",
      "Epoch 22:\n",
      "Train loss: 0.3994, Train acc: 0.9026\n",
      "Valid loss: 0.9404, Valid acc: 0.7550\n",
      "Epoch 23:\n",
      "Train loss: 0.3994, Train acc: 0.9028\n",
      "Valid loss: 0.8750, Valid acc: 0.7844\n",
      "Epoch 24:\n",
      "Train loss: 0.3770, Train acc: 0.9094\n",
      "Valid loss: 0.8060, Valid acc: 0.7936\n",
      "Epoch 25:\n",
      "Train loss: 0.3590, Train acc: 0.9136\n",
      "Valid loss: 1.1454, Valid acc: 0.7312\n",
      "Epoch 26:\n",
      "Train loss: 0.3470, Train acc: 0.9186\n",
      "Valid loss: 0.9284, Valid acc: 0.7624\n",
      "Epoch 27:\n",
      "Train loss: 0.3433, Train acc: 0.9163\n",
      "Valid loss: 0.9713, Valid acc: 0.7606\n",
      "Epoch 28:\n",
      "Train loss: 0.3204, Train acc: 0.9246\n",
      "Valid loss: 0.8933, Valid acc: 0.7853\n",
      "Epoch 29:\n",
      "Train loss: 0.3075, Train acc: 0.9310\n",
      "Valid loss: 0.9497, Valid acc: 0.7844\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Dropout + GradClip (Dropout: 0.3, Weight Decay: 0.0, Max Norm: 0.1): 0.8260\n",
      "Epoch 1:\n",
      "Train loss: 1.6606, Train acc: 0.2263\n",
      "Valid loss: 1.6479, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6445, Train acc: 0.2391\n",
      "Valid loss: 1.6419, Valid acc: 0.2459\n",
      "Epoch 3:\n",
      "Train loss: 1.6423, Train acc: 0.2513\n",
      "Valid loss: 1.6235, Valid acc: 0.2826\n",
      "Epoch 4:\n",
      "Train loss: 1.4700, Train acc: 0.3721\n",
      "Valid loss: 1.3965, Valid acc: 0.4147\n",
      "Epoch 5:\n",
      "Train loss: 1.2920, Train acc: 0.4626\n",
      "Valid loss: 1.2050, Valid acc: 0.5138\n",
      "Epoch 6:\n",
      "Train loss: 1.1348, Train acc: 0.5509\n",
      "Valid loss: 1.2235, Valid acc: 0.5037\n",
      "Epoch 7:\n",
      "Train loss: 1.0408, Train acc: 0.5876\n",
      "Valid loss: 1.0555, Valid acc: 0.5963\n",
      "Epoch 8:\n",
      "Train loss: 0.9642, Train acc: 0.6137\n",
      "Valid loss: 1.3058, Valid acc: 0.5459\n",
      "Epoch 9:\n",
      "Train loss: 0.9173, Train acc: 0.6373\n",
      "Valid loss: 1.0039, Valid acc: 0.6046\n",
      "Epoch 10:\n",
      "Train loss: 0.8978, Train acc: 0.6440\n",
      "Valid loss: 1.1059, Valid acc: 0.5853\n",
      "Epoch 11:\n",
      "Train loss: 0.8550, Train acc: 0.6614\n",
      "Valid loss: 0.9321, Valid acc: 0.6734\n",
      "Epoch 12:\n",
      "Train loss: 0.8300, Train acc: 0.6923\n",
      "Valid loss: 0.9547, Valid acc: 0.6183\n",
      "Epoch 13:\n",
      "Train loss: 0.7989, Train acc: 0.7150\n",
      "Valid loss: 1.0867, Valid acc: 0.5550\n",
      "Epoch 14:\n",
      "Train loss: 0.7846, Train acc: 0.7299\n",
      "Valid loss: 0.9433, Valid acc: 0.6532\n",
      "Epoch 15:\n",
      "Train loss: 0.7561, Train acc: 0.7464\n",
      "Valid loss: 1.2793, Valid acc: 0.5706\n",
      "Epoch 16:\n",
      "Train loss: 0.7162, Train acc: 0.7669\n",
      "Valid loss: 0.8607, Valid acc: 0.7119\n",
      "Epoch 17:\n",
      "Train loss: 0.6995, Train acc: 0.7772\n",
      "Valid loss: 1.1040, Valid acc: 0.6890\n",
      "Epoch 18:\n",
      "Train loss: 0.6642, Train acc: 0.7893\n",
      "Valid loss: 0.9341, Valid acc: 0.6853\n",
      "Epoch 19:\n",
      "Train loss: 0.6580, Train acc: 0.8033\n",
      "Valid loss: 0.8089, Valid acc: 0.7596\n",
      "Epoch 20:\n",
      "Train loss: 0.6302, Train acc: 0.8116\n",
      "Valid loss: 0.8183, Valid acc: 0.7541\n",
      "Epoch 21:\n",
      "Train loss: 0.6121, Train acc: 0.8200\n",
      "Valid loss: 1.1391, Valid acc: 0.6560\n",
      "Epoch 22:\n",
      "Train loss: 0.5955, Train acc: 0.8267\n",
      "Valid loss: 0.8079, Valid acc: 0.7486\n",
      "Epoch 23:\n",
      "Train loss: 0.5846, Train acc: 0.8299\n",
      "Valid loss: 0.9259, Valid acc: 0.7303\n",
      "Epoch 24:\n",
      "Train loss: 0.5777, Train acc: 0.8381\n",
      "Valid loss: 0.8308, Valid acc: 0.7587\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy GradClip + L2 (Dropout: 0.0, Weight Decay: 0.0001, Max Norm: 0.1): 0.7660\n",
      "Epoch 1:\n",
      "Train loss: 1.6665, Train acc: 0.2212\n",
      "Valid loss: 1.6525, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6536, Train acc: 0.2263\n",
      "Valid loss: 1.6487, Valid acc: 0.2477\n",
      "Epoch 3:\n",
      "Train loss: 1.6460, Train acc: 0.2329\n",
      "Valid loss: 1.6301, Valid acc: 0.2440\n",
      "Epoch 4:\n",
      "Train loss: 1.5197, Train acc: 0.3251\n",
      "Valid loss: 1.4668, Valid acc: 0.3624\n",
      "Epoch 5:\n",
      "Train loss: 1.3283, Train acc: 0.4443\n",
      "Valid loss: 1.2921, Valid acc: 0.4734\n",
      "Epoch 6:\n",
      "Train loss: 1.1939, Train acc: 0.5206\n",
      "Valid loss: 1.2512, Valid acc: 0.4917\n",
      "Epoch 7:\n",
      "Train loss: 1.0875, Train acc: 0.5649\n",
      "Valid loss: 1.1612, Valid acc: 0.5413\n",
      "Epoch 8:\n",
      "Train loss: 1.0329, Train acc: 0.5844\n",
      "Valid loss: 1.1253, Valid acc: 0.5450\n",
      "Epoch 9:\n",
      "Train loss: 0.9604, Train acc: 0.6199\n",
      "Valid loss: 1.0178, Valid acc: 0.6073\n",
      "Epoch 10:\n",
      "Train loss: 0.9187, Train acc: 0.6474\n",
      "Valid loss: 1.0133, Valid acc: 0.6092\n",
      "Epoch 11:\n",
      "Train loss: 0.8407, Train acc: 0.6793\n",
      "Valid loss: 0.9851, Valid acc: 0.6523\n",
      "Epoch 12:\n",
      "Train loss: 0.8034, Train acc: 0.6983\n",
      "Valid loss: 1.2958, Valid acc: 0.5413\n",
      "Epoch 13:\n",
      "Train loss: 0.7615, Train acc: 0.7146\n",
      "Valid loss: 1.0700, Valid acc: 0.6147\n",
      "Epoch 14:\n",
      "Train loss: 0.7185, Train acc: 0.7354\n",
      "Valid loss: 1.0003, Valid acc: 0.6294\n",
      "Epoch 15:\n",
      "Train loss: 0.7151, Train acc: 0.7403\n",
      "Valid loss: 0.8779, Valid acc: 0.7055\n",
      "Epoch 16:\n",
      "Train loss: 0.6368, Train acc: 0.7756\n",
      "Valid loss: 0.8726, Valid acc: 0.7330\n",
      "Epoch 17:\n",
      "Train loss: 0.6341, Train acc: 0.7873\n",
      "Valid loss: 0.8653, Valid acc: 0.7312\n",
      "Epoch 18:\n",
      "Train loss: 0.5652, Train acc: 0.8168\n",
      "Valid loss: 1.3891, Valid acc: 0.5183\n",
      "Epoch 19:\n",
      "Train loss: 0.5294, Train acc: 0.8411\n",
      "Valid loss: 0.9405, Valid acc: 0.7128\n",
      "Epoch 20:\n",
      "Train loss: 0.5089, Train acc: 0.8492\n",
      "Valid loss: 0.7985, Valid acc: 0.7670\n",
      "Epoch 21:\n",
      "Train loss: 0.4436, Train acc: 0.8787\n",
      "Valid loss: 0.8176, Valid acc: 0.7541\n",
      "Epoch 22:\n",
      "Train loss: 0.4373, Train acc: 0.8790\n",
      "Valid loss: 0.9598, Valid acc: 0.7119\n",
      "Epoch 23:\n",
      "Train loss: 0.3857, Train acc: 0.8968\n",
      "Valid loss: 0.8480, Valid acc: 0.7743\n",
      "Epoch 24:\n",
      "Train loss: 0.3590, Train acc: 0.9085\n",
      "Valid loss: 0.8537, Valid acc: 0.7661\n",
      "Epoch 25:\n",
      "Train loss: 0.3930, Train acc: 0.9000\n",
      "Valid loss: 0.8543, Valid acc: 0.7716\n",
      "Epoch 26:\n",
      "Train loss: 0.3208, Train acc: 0.9184\n",
      "Valid loss: 0.7984, Valid acc: 0.7807\n",
      "Epoch 27:\n",
      "Train loss: 0.3067, Train acc: 0.9186\n",
      "Valid loss: 0.8426, Valid acc: 0.7817\n",
      "Epoch 28:\n",
      "Train loss: 0.3502, Train acc: 0.9083\n",
      "Valid loss: 0.9777, Valid acc: 0.7183\n",
      "Epoch 29:\n",
      "Train loss: 0.2786, Train acc: 0.9292\n",
      "Valid loss: 0.8211, Valid acc: 0.7936\n",
      "Epoch 30:\n",
      "Train loss: 0.3254, Train acc: 0.9156\n",
      "Valid loss: 0.8263, Valid acc: 0.7817\n",
      "Epoch 31:\n",
      "Train loss: 0.2703, Train acc: 0.9324\n",
      "Valid loss: 0.8364, Valid acc: 0.7853\n",
      "Epoch 32:\n",
      "Train loss: 0.2333, Train acc: 0.9395\n",
      "Valid loss: 1.1834, Valid acc: 0.7147\n",
      "Epoch 33:\n",
      "Train loss: 0.2379, Train acc: 0.9383\n",
      "Valid loss: 0.8519, Valid acc: 0.7936\n",
      "Epoch 34:\n",
      "Train loss: 0.2065, Train acc: 0.9500\n",
      "Valid loss: 0.9143, Valid acc: 0.7771\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy Dropout + L2 (Dropout: 0.3, Weight Decay: 0.0001, Max Norm: 0.0): 0.8340\n",
      "Epoch 1:\n",
      "Train loss: 1.6632, Train acc: 0.2297\n",
      "Valid loss: 1.6551, Valid acc: 0.2046\n",
      "Epoch 2:\n",
      "Train loss: 1.6547, Train acc: 0.2263\n",
      "Valid loss: 1.6522, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.6481, Train acc: 0.2281\n",
      "Valid loss: 1.6490, Valid acc: 0.2294\n",
      "Epoch 4:\n",
      "Train loss: 1.6401, Train acc: 0.2554\n",
      "Valid loss: 1.5650, Valid acc: 0.2972\n",
      "Epoch 5:\n",
      "Train loss: 1.4406, Train acc: 0.3741\n",
      "Valid loss: 1.5870, Valid acc: 0.3028\n",
      "Epoch 6:\n",
      "Train loss: 1.2937, Train acc: 0.4500\n",
      "Valid loss: 1.2600, Valid acc: 0.4862\n",
      "Epoch 7:\n",
      "Train loss: 1.1718, Train acc: 0.5229\n",
      "Valid loss: 1.1965, Valid acc: 0.4890\n",
      "Epoch 8:\n",
      "Train loss: 1.1002, Train acc: 0.5614\n",
      "Valid loss: 1.1552, Valid acc: 0.5239\n",
      "Epoch 9:\n",
      "Train loss: 1.0196, Train acc: 0.5951\n",
      "Valid loss: 1.0708, Valid acc: 0.5835\n",
      "Epoch 10:\n",
      "Train loss: 0.9622, Train acc: 0.6197\n",
      "Valid loss: 0.9823, Valid acc: 0.6046\n",
      "Epoch 11:\n",
      "Train loss: 0.9270, Train acc: 0.6341\n",
      "Valid loss: 1.0220, Valid acc: 0.5835\n",
      "Epoch 12:\n",
      "Train loss: 0.8776, Train acc: 0.6488\n",
      "Valid loss: 0.9778, Valid acc: 0.6220\n",
      "Epoch 13:\n",
      "Train loss: 0.8567, Train acc: 0.6582\n",
      "Valid loss: 0.9422, Valid acc: 0.6541\n",
      "Epoch 14:\n",
      "Train loss: 0.8320, Train acc: 0.6630\n",
      "Valid loss: 0.9454, Valid acc: 0.6303\n",
      "Epoch 15:\n",
      "Train loss: 0.7950, Train acc: 0.6891\n",
      "Valid loss: 0.9887, Valid acc: 0.6321\n",
      "Epoch 16:\n",
      "Train loss: 0.7704, Train acc: 0.6903\n",
      "Valid loss: 0.8659, Valid acc: 0.6789\n",
      "Epoch 17:\n",
      "Train loss: 0.7602, Train acc: 0.6985\n",
      "Valid loss: 0.9557, Valid acc: 0.6303\n",
      "Epoch 18:\n",
      "Train loss: 0.7507, Train acc: 0.7086\n",
      "Valid loss: 0.9570, Valid acc: 0.6156\n",
      "Epoch 19:\n",
      "Train loss: 0.7170, Train acc: 0.7336\n",
      "Valid loss: 0.8947, Valid acc: 0.7248\n",
      "Epoch 20:\n",
      "Train loss: 0.7016, Train acc: 0.7506\n",
      "Valid loss: 0.9785, Valid acc: 0.7018\n",
      "Epoch 21:\n",
      "Train loss: 0.6696, Train acc: 0.7779\n",
      "Valid loss: 1.0737, Valid acc: 0.6413\n",
      "Epoch 22:\n",
      "Train loss: 0.6434, Train acc: 0.7895\n",
      "Valid loss: 0.7632, Valid acc: 0.7661\n",
      "Epoch 23:\n",
      "Train loss: 0.6076, Train acc: 0.8077\n",
      "Valid loss: 0.9467, Valid acc: 0.6780\n",
      "Epoch 24:\n",
      "Train loss: 0.5980, Train acc: 0.8177\n",
      "Valid loss: 0.9861, Valid acc: 0.6706\n",
      "Epoch 25:\n",
      "Train loss: 0.5887, Train acc: 0.8242\n",
      "Valid loss: 0.8613, Valid acc: 0.7138\n",
      "Epoch 26:\n",
      "Train loss: 0.5891, Train acc: 0.8274\n",
      "Valid loss: 0.8135, Valid acc: 0.7679\n",
      "Epoch 27:\n",
      "Train loss: 0.5625, Train acc: 0.8340\n",
      "Valid loss: 0.7781, Valid acc: 0.7706\n",
      "Epoch 28:\n",
      "Train loss: 0.5596, Train acc: 0.8436\n",
      "Valid loss: 0.7486, Valid acc: 0.7881\n",
      "Epoch 29:\n",
      "Train loss: 0.5278, Train acc: 0.8505\n",
      "Valid loss: 0.7812, Valid acc: 0.7697\n",
      "Epoch 30:\n",
      "Train loss: 0.5225, Train acc: 0.8496\n",
      "Valid loss: 0.7186, Valid acc: 0.8028\n",
      "Epoch 31:\n",
      "Train loss: 0.5293, Train acc: 0.8496\n",
      "Valid loss: 0.7794, Valid acc: 0.7798\n",
      "Epoch 32:\n",
      "Train loss: 0.5136, Train acc: 0.8512\n",
      "Valid loss: 0.7141, Valid acc: 0.8018\n",
      "Epoch 33:\n",
      "Train loss: 0.4993, Train acc: 0.8586\n",
      "Valid loss: 0.7203, Valid acc: 0.7936\n",
      "Epoch 34:\n",
      "Train loss: 0.5052, Train acc: 0.8645\n",
      "Valid loss: 0.8843, Valid acc: 0.7459\n",
      "Epoch 35:\n",
      "Train loss: 0.4926, Train acc: 0.8620\n",
      "Valid loss: 0.7736, Valid acc: 0.7706\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy All (Dropout+GradClip+L2) (Dropout: 0.3, Weight Decay: 0.0001, Max Norm: 0.1): 0.8100\n"
     ]
    }
   ],
   "source": [
    "# Regularisation - ALL Techniques to find best combination (varying values)\n",
    "\n",
    "# Function to obtain rows with regards to regularisation strategies using regex\n",
    "def get_row(df, pattern):\n",
    "    m = df['technique'].str.contains(pattern, regex=True)\n",
    "    return df[m].iloc[0]\n",
    "\n",
    "# Use regex to get best params from each technique\n",
    "dropout_rows = get_row(df_best_reg_technique, r'(Dropout)')\n",
    "clip_rows = get_row(df_best_reg_technique, r'(Grad)')\n",
    "l2_rows = get_row(df_best_reg_technique, r'(L2)')\n",
    "\n",
    "best_dropout = float(dropout_rows['dropout'])\n",
    "best_weight_decay = float(l2_rows['weight_decay'])\n",
    "best_grad_clip = True\n",
    "best_max_norm  = float(clip_rows['max_norm'])\n",
    "\n",
    "combos = [\n",
    "    (\"Dropout + GradClip\", best_weight_decay if 0 else 0.0, best_dropout, True,  best_max_norm),\n",
    "    (\"GradClip + L2\", best_weight_decay, 0.0, True, best_max_norm),\n",
    "    (\"Dropout + L2\", best_weight_decay, best_dropout,  False, 0.0),\n",
    "    (\"All (Dropout+GradClip+L2)\", best_weight_decay, best_dropout, True, best_max_norm),\n",
    "]\n",
    "\n",
    "for reg_technique, weight_decay, dropout, grad_clip, max_norm in combos:\n",
    "    result_best_technique = regularisation_test(\n",
    "        weight_decay=weight_decay,\n",
    "        dropout=dropout,\n",
    "        grad_clip=grad_clip,\n",
    "        max_norm=max_norm,\n",
    "        reg_technique=reg_technique,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    best_reg_technique.append(result_best_technique)\n",
    "\n",
    "df_all = pd.DataFrame(best_reg_technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "494f24ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "technique",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dropout",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weight_decay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "grad_clip",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "max_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "best_val_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "81b942a0-daa6-4d7a-87c7-a58cd706f9cd",
       "rows": [
        [
         "3",
         "L2 Regularization",
         "0.0",
         "0.0001",
         "False",
         "0.0",
         "0.8523",
         "0.84"
        ],
        [
         "7",
         "Dropout",
         "0.3",
         "0.0",
         "False",
         "0.0",
         "0.7982",
         "0.84"
        ],
        [
         "9",
         "Gradient Clipping",
         "0.0",
         "0.0",
         "True",
         "0.1",
         "0.7936",
         "0.838"
        ],
        [
         "14",
         "Dropout + L2",
         "0.3",
         "0.0001",
         "False",
         "0.0",
         "0.7936",
         "0.834"
        ],
        [
         "12",
         "Dropout + GradClip",
         "0.3",
         "0.0",
         "True",
         "0.1",
         "0.7936",
         "0.826"
        ],
        [
         "0",
         "Baseline",
         "0.0",
         "0.0",
         "False",
         "0.0",
         "0.8073",
         "0.822"
        ],
        [
         "1",
         "L2 Regularization",
         "0.0",
         "1e-05",
         "False",
         "0.0",
         "0.7899",
         "0.822"
        ],
        [
         "2",
         "L2 Regularization",
         "0.0",
         "5e-05",
         "False",
         "0.0",
         "0.8165",
         "0.816"
        ],
        [
         "4",
         "L2 Regularization",
         "0.0",
         "0.0005",
         "False",
         "0.0",
         "0.8147",
         "0.816"
        ],
        [
         "10",
         "Gradient Clipping",
         "0.0",
         "0.0",
         "True",
         "0.5",
         "0.7917",
         "0.814"
        ],
        [
         "15",
         "All (Dropout+GradClip+L2)",
         "0.3",
         "0.0001",
         "True",
         "0.1",
         "0.8028",
         "0.81"
        ],
        [
         "13",
         "GradClip + L2",
         "0.0",
         "0.0001",
         "True",
         "0.1",
         "0.7596",
         "0.766"
        ],
        [
         "11",
         "Gradient Clipping",
         "0.0",
         "0.0",
         "True",
         "1.0",
         "0.7303",
         "0.75"
        ],
        [
         "8",
         "Dropout",
         "0.4",
         "0.0",
         "False",
         "0.0",
         "0.6872",
         "0.636"
        ],
        [
         "5",
         "L2 Regularization",
         "0.0",
         "0.001",
         "False",
         "0.0",
         "0.8174",
         "0.622"
        ],
        [
         "6",
         "Dropout",
         "0.2",
         "0.0",
         "False",
         "0.0",
         "0.6697",
         "0.516"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>technique</th>\n",
       "      <th>dropout</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>grad_clip</th>\n",
       "      <th>max_norm</th>\n",
       "      <th>best_val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dropout</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gradient Clipping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dropout + L2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dropout + GradClip</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7936</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8147</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Clipping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7917</td>\n",
       "      <td>0.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>All (Dropout+GradClip+L2)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8028</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GradClip + L2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gradient Clipping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7303</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dropout</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6872</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L2 Regularization</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dropout</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6697</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    technique  dropout  weight_decay  grad_clip  max_norm  \\\n",
       "3           L2 Regularization      0.0       0.00010      False       0.0   \n",
       "7                     Dropout      0.3       0.00000      False       0.0   \n",
       "9           Gradient Clipping      0.0       0.00000       True       0.1   \n",
       "14               Dropout + L2      0.3       0.00010      False       0.0   \n",
       "12         Dropout + GradClip      0.3       0.00000       True       0.1   \n",
       "0                    Baseline      0.0       0.00000      False       0.0   \n",
       "1           L2 Regularization      0.0       0.00001      False       0.0   \n",
       "2           L2 Regularization      0.0       0.00005      False       0.0   \n",
       "4           L2 Regularization      0.0       0.00050      False       0.0   \n",
       "10          Gradient Clipping      0.0       0.00000       True       0.5   \n",
       "15  All (Dropout+GradClip+L2)      0.3       0.00010       True       0.1   \n",
       "13              GradClip + L2      0.0       0.00010       True       0.1   \n",
       "11          Gradient Clipping      0.0       0.00000       True       1.0   \n",
       "8                     Dropout      0.4       0.00000      False       0.0   \n",
       "5           L2 Regularization      0.0       0.00100      False       0.0   \n",
       "6                     Dropout      0.2       0.00000      False       0.0   \n",
       "\n",
       "    best_val_acc  test_acc  \n",
       "3         0.8523     0.840  \n",
       "7         0.7982     0.840  \n",
       "9         0.7936     0.838  \n",
       "14        0.7936     0.834  \n",
       "12        0.7936     0.826  \n",
       "0         0.8073     0.822  \n",
       "1         0.7899     0.822  \n",
       "2         0.8165     0.816  \n",
       "4         0.8147     0.816  \n",
       "10        0.7917     0.814  \n",
       "15        0.8028     0.810  \n",
       "13        0.7596     0.766  \n",
       "11        0.7303     0.750  \n",
       "8         0.6872     0.636  \n",
       "5         0.8174     0.622  \n",
       "6         0.6697     0.516  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile results into dataframe for comparisons across strategies\n",
    "col_order = ['technique', 'dropout', 'weight_decay', 'grad_clip', 'max_norm', 'best_val_acc', 'test_acc']\n",
    "df_all = df_all[[c for c in col_order if c in df_all.columns]]\n",
    "\n",
    "for c in ['best_val_acc', 'test_acc']:\n",
    "    if c in df_all.columns:\n",
    "        df_all[c] = df_all[c].astype(float).round(4)\n",
    "\n",
    "df_all = df_all.sort_values('test_acc', ascending=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5551385",
   "metadata": {},
   "source": [
    "### Answer (b):\n",
    "- Dropout ONLY with value of 0.4 gives the best test accuracy of 0.850 still. We continue to adopt this regularisation strategy for the rest of qn 2.\n",
    "- Most of the runs which involved L2 regularization, performed worse than the baseline. The exception was adding L2 regularization to the best technique of dropout with value 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a8a2d",
   "metadata": {},
   "source": [
    "**(c) For the best configuration and regularization strategy in your experiments, plot the training loss curve and validation accuracy curve during training with x-axis being the number of training epochs. Discuss what the curves inform about the training dynamics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00485664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 1.6606, Train acc: 0.2274\n",
      "Valid loss: 1.6529, Valid acc: 0.2431\n",
      "Epoch 2:\n",
      "Train loss: 1.6469, Train acc: 0.2354\n",
      "Valid loss: 1.6489, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6442, Train acc: 0.2295\n",
      "Valid loss: 1.6556, Valid acc: 0.2046\n",
      "Epoch 4:\n",
      "Train loss: 1.6316, Train acc: 0.2671\n",
      "Valid loss: 1.5725, Valid acc: 0.3495\n",
      "Epoch 5:\n",
      "Train loss: 1.4401, Train acc: 0.3957\n",
      "Valid loss: 1.4933, Valid acc: 0.3266\n",
      "Epoch 6:\n",
      "Train loss: 1.2366, Train acc: 0.5007\n",
      "Valid loss: 1.2260, Valid acc: 0.4798\n",
      "Epoch 7:\n",
      "Train loss: 1.0800, Train acc: 0.5702\n",
      "Valid loss: 1.0854, Valid acc: 0.5706\n",
      "Epoch 8:\n",
      "Train loss: 1.0117, Train acc: 0.6142\n",
      "Valid loss: 1.0154, Valid acc: 0.6202\n",
      "Epoch 9:\n",
      "Train loss: 0.9221, Train acc: 0.6534\n",
      "Valid loss: 0.9676, Valid acc: 0.6541\n",
      "Epoch 10:\n",
      "Train loss: 0.8476, Train acc: 0.6825\n",
      "Valid loss: 0.9100, Valid acc: 0.6532\n",
      "Epoch 11:\n",
      "Train loss: 0.7818, Train acc: 0.7105\n",
      "Valid loss: 0.9097, Valid acc: 0.6450\n",
      "Epoch 12:\n",
      "Train loss: 0.7747, Train acc: 0.7139\n",
      "Valid loss: 0.8702, Valid acc: 0.6761\n",
      "Epoch 13:\n",
      "Train loss: 0.7251, Train acc: 0.7327\n",
      "Valid loss: 0.8855, Valid acc: 0.6862\n",
      "Epoch 14:\n",
      "Train loss: 0.6736, Train acc: 0.7593\n",
      "Valid loss: 0.9292, Valid acc: 0.6569\n",
      "Epoch 15:\n",
      "Train loss: 0.6323, Train acc: 0.7934\n",
      "Valid loss: 0.9543, Valid acc: 0.6606\n",
      "Epoch 16:\n",
      "Train loss: 0.6260, Train acc: 0.8049\n",
      "Valid loss: 1.0439, Valid acc: 0.6807\n",
      "Epoch 17:\n",
      "Train loss: 0.5570, Train acc: 0.8232\n",
      "Valid loss: 0.7892, Valid acc: 0.7468\n",
      "Epoch 18:\n",
      "Train loss: 0.5134, Train acc: 0.8544\n",
      "Valid loss: 0.9889, Valid acc: 0.7119\n",
      "Epoch 19:\n",
      "Train loss: 0.4765, Train acc: 0.8629\n",
      "Valid loss: 1.0161, Valid acc: 0.6945\n",
      "Epoch 20:\n",
      "Train loss: 0.4659, Train acc: 0.8673\n",
      "Valid loss: 1.0661, Valid acc: 0.6725\n",
      "Epoch 21:\n",
      "Train loss: 0.4163, Train acc: 0.8858\n",
      "Valid loss: 2.1676, Valid acc: 0.5431\n",
      "Epoch 22:\n",
      "Train loss: 0.4152, Train acc: 0.8872\n",
      "Valid loss: 0.7992, Valid acc: 0.7752\n",
      "Epoch 23:\n",
      "Train loss: 0.4215, Train acc: 0.8835\n",
      "Valid loss: 0.8534, Valid acc: 0.7734\n",
      "Epoch 24:\n",
      "Train loss: 0.3471, Train acc: 0.9136\n",
      "Valid loss: 0.9104, Valid acc: 0.7312\n",
      "Epoch 25:\n",
      "Train loss: 0.3548, Train acc: 0.9117\n",
      "Valid loss: 0.8053, Valid acc: 0.7927\n",
      "Epoch 26:\n",
      "Train loss: 0.3578, Train acc: 0.9088\n",
      "Valid loss: 0.9131, Valid acc: 0.7394\n",
      "Epoch 27:\n",
      "Train loss: 0.3389, Train acc: 0.9175\n",
      "Valid loss: 0.8000, Valid acc: 0.7936\n",
      "Epoch 28:\n",
      "Train loss: 0.3360, Train acc: 0.9184\n",
      "Valid loss: 0.7947, Valid acc: 0.7936\n",
      "Epoch 29:\n",
      "Train loss: 0.3079, Train acc: 0.9241\n",
      "Valid loss: 0.8374, Valid acc: 0.7936\n",
      "Epoch 30:\n",
      "Train loss: 0.2747, Train acc: 0.9360\n",
      "Valid loss: 0.8520, Valid acc: 0.7881\n",
      "Epoch 31:\n",
      "Train loss: 0.2903, Train acc: 0.9333\n",
      "Valid loss: 1.1226, Valid acc: 0.7339\n",
      "Epoch 32:\n",
      "Train loss: 0.3264, Train acc: 0.9230\n",
      "Valid loss: 0.7341, Valid acc: 0.8156\n",
      "Epoch 33:\n",
      "Train loss: 0.2505, Train acc: 0.9418\n",
      "Valid loss: 0.7634, Valid acc: 0.8138\n",
      "Epoch 34:\n",
      "Train loss: 0.2512, Train acc: 0.9406\n",
      "Valid loss: 0.7353, Valid acc: 0.8248\n",
      "Epoch 35:\n",
      "Train loss: 0.2356, Train acc: 0.9473\n",
      "Valid loss: 0.8947, Valid acc: 0.7826\n",
      "Epoch 36:\n",
      "Train loss: 0.2252, Train acc: 0.9498\n",
      "Valid loss: 0.9019, Valid acc: 0.7817\n",
      "Epoch 37:\n",
      "Train loss: 0.2521, Train acc: 0.9422\n",
      "Valid loss: 0.7556, Valid acc: 0.8229\n",
      "Epoch 38:\n",
      "Train loss: 0.3015, Train acc: 0.9308\n",
      "Valid loss: 0.7831, Valid acc: 0.8064\n",
      "Epoch 39:\n",
      "Train loss: 0.2445, Train acc: 0.9431\n",
      "Valid loss: 0.7681, Valid acc: 0.8202\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n"
     ]
    }
   ],
   "source": [
    "# Set ideal hyperparameters from 2(a)\n",
    "if best_hyperparams[\"epochs ran\"] <= 50:\n",
    "    no_epoch = 50\n",
    "elif best_hyperparams[\"epochs ran\"] > 50 & best_hyperparams[\"epochs ran\"] <= 100:\n",
    "    no_epoch = 100\n",
    "else:\n",
    "    no_epoch = 200\n",
    "batch_size = best_hyperparams[\"batch_size\"]\n",
    "hidden_dim = best_hyperparams[\"hidden_dim\"]\n",
    "lr = best_hyperparams[\"lr\"]\n",
    "optimizer = best_hyperparams[\"optimizer\"]\n",
    "\n",
    "# Set regularization from 2(b)\n",
    "dropout = df_all.loc[df_all['test_acc'].idxmax(), 'dropout']\n",
    "weight_decay = df_all.loc[df_all['test_acc'].idxmax(), 'weight_decay']\n",
    "grad_clip = df_all.loc[df_all['test_acc'].idxmax(), 'grad_clip']\n",
    "max_norm = df_all.loc[df_all['test_acc'].idxmax(), 'max_norm']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialise model\n",
    "model = ClassifierRNN(TEXT.vocab.vectors.numpy() , hidden_dim, dropout=dropout)\n",
    "\n",
    "# Initialise optimiser with L2 regularization\n",
    "optimizer = optimizer.__class__(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if grad_clip == True:\n",
    "    train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch, grad_clip=True, max_norm=max_norm)\n",
    "else:\n",
    "    train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9ef47315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the two graphs side by side\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(valid_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d6a2f",
   "metadata": {},
   "source": [
    "### Answer (c):\n",
    "\n",
    "Some observations and explanations with the plots:\n",
    "- The generally smooth loss curve suggests the model is learning effectively and training remains stable across training epochs. The lack of a proper plateau at the end of the curve suggests there may be room for further improvements, but the increasing gentleness of the curves means the model is likely already close to the minimum loss and early stopping is preventing severe overfitting. \n",
    "- The generally upward curve for validation accuracy shows that the model is generalising well and its performance is improving over training epochs. There is a sudden jump in training loss and validation accuracy at about 24 epochs, suggesting that model performance seems to vary over the validation samples, but the overall trend still shows a healthy improvement over the epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e4a15",
   "metadata": {},
   "source": [
    "**(d) RNNs produce a hidden vector for each word, instead of the entire sentence. Which methods have you tried in deriving the final sentence representation to perform sentiment classification? Describe all the strategies you have implemented, together with their accuracy scores on the test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d253fc4",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "Here, the approach is to evaluate how different sentence representation strategies could influence the model's performance across various text categories. We will use the best technique for part (e). Since RNNs generate a hidden vector for each token rather than a single sentence embedding, an important design choice is how to aggregate these hidden states into a fixed-length representation for classification. \n",
    "\n",
    "We mainly explored 2 families of approaches:\n",
    "\n",
    "1. Hidden-State Methods\n",
    "* Last Hidden State (Default) - Use final hidden state as sentence representation (This approach would be good if we think it captures all preceding context well)\n",
    "* Average Last k hidden states (k = 2, 3, or 4) - These variants aim to smooth the representation by averaging over the most recent time steps, mitigating the possibility of noise or information loss that can occur when relying solely on the final hidden state, especially for longer sentences\n",
    "\n",
    "2. Pooling Methods\n",
    "* Mean Pooling - Computes the average over all time-step hidden states, giving equal weight to each token\n",
    "* Max Pooling - Takes the elementwise maximum across all time steps\n",
    "* MeanMax Pooling - Concatenates both the mean and max pooled represenation \n",
    "* Sum Pooling - Computes by sums all hidden states, which preserve magnitude information \n",
    "\n",
    "Additionally, we chose these methods because they are computationally efficient and reasonable to experiment with in a single hidden-layer RNN as well.\n",
    "\n",
    "Note: While we could have explored more advanced methods like attention-based representations (e.g. self-attention) and/or architectural variations (e.g. bidirectional RNNs) in this part, we left the the exploration of these strategies to qn 3, as the we believe the focus shifts from representation aggregation to model architecture enhancements there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25127ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Baseline) 0.8180\n"
     ]
    }
   ],
   "source": [
    "# Using model from 2(c), our previous models use last hidden state as sentence representation (default baseline)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_acc = test_loop(model, test_loader)\n",
    "print(f\"Test Accuracy (Baseline) {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "198519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine RNN Class for experimentation with different sentence representation techniques\n",
    "class ClassifierRepresentationRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, representation, dropout=0.0):\n",
    "        super(ClassifierRepresentationRNN, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=False)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 6) # 6 possible labels\n",
    "        self.representation = representation\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # Conditional Logic to apply different representation techniques\n",
    "        if self.representation.startswith('average_last_'):\n",
    "            last_k = int(self.representation.split('_')[-1])\n",
    "            k = min(last_k, output.size(1))\n",
    "            rep = output[:, -k:, :].mean(dim=1)\n",
    "        elif self.representation == 'max':\n",
    "            rep, _ = torch.max(output, dim=1)\n",
    "        elif self.representation == 'mean':\n",
    "            rep = torch.mean(output, dim=1)\n",
    "        elif self.representation == 'maxmean':\n",
    "            max_pooled, _ = torch.max(output, dim=1)\n",
    "            mean_pooled = torch.mean(output, dim=1)\n",
    "            rep = (max_pooled + mean_pooled) / 2\n",
    "        elif self.representation == 'sum':\n",
    "            rep = torch.sum(output, dim=1)\n",
    "\n",
    "        hidden = self.dropout(rep)\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bf32ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine a function for Representation Tests\n",
    "def representation_test(strategy, batch_size, hidden_dim, lr, optimizer, no_epoch, weight_decay, dropout, grad_clip, max_norm):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialise model\n",
    "    model = ClassifierRepresentationRNN(TEXT.vocab.vectors.numpy() , hidden_dim, strategy, dropout=dropout)\n",
    "\n",
    "    # Initialise optimiser\n",
    "    optimizer = optimizer.__class__(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if grad_clip == True:\n",
    "        train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch, grad_clip=True, max_norm=max_norm)\n",
    "    else:\n",
    "        train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch)\n",
    "\n",
    "    test_acc = test_loop(model, test_loader)\n",
    "    print(f\"Test Accuracy {strategy}: {test_acc:.4f}\") # Evaluate on test accuracy again here\n",
    "\n",
    "    return {\n",
    "        'technique': strategy,\n",
    "        'test_acc': test_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06d9774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 1.6557, Train acc: 0.2283\n",
      "Valid loss: 1.6494, Valid acc: 0.2505\n",
      "Epoch 2:\n",
      "Train loss: 1.6469, Train acc: 0.2295\n",
      "Valid loss: 1.6493, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6433, Train acc: 0.2380\n",
      "Valid loss: 1.6464, Valid acc: 0.2468\n",
      "Epoch 4:\n",
      "Train loss: 1.5698, Train acc: 0.3210\n",
      "Valid loss: 1.4339, Valid acc: 0.4018\n",
      "Epoch 5:\n",
      "Train loss: 1.3589, Train acc: 0.4262\n",
      "Valid loss: 1.2640, Valid acc: 0.4560\n",
      "Epoch 6:\n",
      "Train loss: 1.1880, Train acc: 0.4952\n",
      "Valid loss: 1.2615, Valid acc: 0.4844\n",
      "Epoch 7:\n",
      "Train loss: 1.0817, Train acc: 0.5637\n",
      "Valid loss: 1.1242, Valid acc: 0.5266\n",
      "Epoch 8:\n",
      "Train loss: 0.9607, Train acc: 0.6353\n",
      "Valid loss: 0.9743, Valid acc: 0.6138\n",
      "Epoch 9:\n",
      "Train loss: 0.8885, Train acc: 0.6818\n",
      "Valid loss: 0.8725, Valid acc: 0.6798\n",
      "Epoch 10:\n",
      "Train loss: 0.7979, Train acc: 0.7171\n",
      "Valid loss: 0.9732, Valid acc: 0.6761\n",
      "Epoch 11:\n",
      "Train loss: 0.7369, Train acc: 0.7586\n",
      "Valid loss: 0.7859, Valid acc: 0.7514\n",
      "Epoch 12:\n",
      "Train loss: 0.6758, Train acc: 0.7847\n",
      "Valid loss: 0.8405, Valid acc: 0.7220\n",
      "Epoch 13:\n",
      "Train loss: 0.6135, Train acc: 0.8184\n",
      "Valid loss: 0.8028, Valid acc: 0.7266\n",
      "Epoch 14:\n",
      "Train loss: 0.5635, Train acc: 0.8322\n",
      "Valid loss: 0.7446, Valid acc: 0.7844\n",
      "Epoch 15:\n",
      "Train loss: 0.5958, Train acc: 0.8258\n",
      "Valid loss: 0.7575, Valid acc: 0.7661\n",
      "Epoch 16:\n",
      "Train loss: 0.5127, Train acc: 0.8478\n",
      "Valid loss: 0.7179, Valid acc: 0.7826\n",
      "Epoch 17:\n",
      "Train loss: 0.5281, Train acc: 0.8542\n",
      "Valid loss: 0.8785, Valid acc: 0.7358\n",
      "Epoch 18:\n",
      "Train loss: 0.4438, Train acc: 0.8771\n",
      "Valid loss: 0.7840, Valid acc: 0.7706\n",
      "Epoch 19:\n",
      "Train loss: 0.4257, Train acc: 0.8842\n",
      "Valid loss: 0.8638, Valid acc: 0.7275\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy average_last_2: 0.7780\n",
      "Epoch 1:\n",
      "Train loss: 1.6549, Train acc: 0.2309\n",
      "Valid loss: 1.6636, Valid acc: 0.2404\n",
      "Epoch 2:\n",
      "Train loss: 1.6444, Train acc: 0.2384\n",
      "Valid loss: 1.6565, Valid acc: 0.2431\n",
      "Epoch 3:\n",
      "Train loss: 1.5268, Train acc: 0.3526\n",
      "Valid loss: 1.4904, Valid acc: 0.3835\n",
      "Epoch 4:\n",
      "Train loss: 1.3507, Train acc: 0.4211\n",
      "Valid loss: 1.3226, Valid acc: 0.4615\n",
      "Epoch 5:\n",
      "Train loss: 1.2211, Train acc: 0.4780\n",
      "Valid loss: 1.1992, Valid acc: 0.4917\n",
      "Epoch 6:\n",
      "Train loss: 1.1421, Train acc: 0.5165\n",
      "Valid loss: 1.1648, Valid acc: 0.5220\n",
      "Epoch 7:\n",
      "Train loss: 1.0590, Train acc: 0.5711\n",
      "Valid loss: 1.2914, Valid acc: 0.4422\n",
      "Epoch 8:\n",
      "Train loss: 0.9968, Train acc: 0.5997\n",
      "Valid loss: 1.2831, Valid acc: 0.4771\n",
      "Epoch 9:\n",
      "Train loss: 0.9431, Train acc: 0.6369\n",
      "Valid loss: 1.1186, Valid acc: 0.5780\n",
      "Epoch 10:\n",
      "Train loss: 0.8352, Train acc: 0.6850\n",
      "Valid loss: 1.0429, Valid acc: 0.6229\n",
      "Epoch 11:\n",
      "Train loss: 0.7751, Train acc: 0.7027\n",
      "Valid loss: 0.8825, Valid acc: 0.6624\n",
      "Epoch 12:\n",
      "Train loss: 0.7275, Train acc: 0.7164\n",
      "Valid loss: 1.2254, Valid acc: 0.5486\n",
      "Epoch 13:\n",
      "Train loss: 0.6913, Train acc: 0.7274\n",
      "Valid loss: 0.9353, Valid acc: 0.6560\n",
      "Epoch 14:\n",
      "Train loss: 0.6785, Train acc: 0.7361\n",
      "Valid loss: 0.9112, Valid acc: 0.6486\n",
      "Epoch 15:\n",
      "Train loss: 0.6236, Train acc: 0.7471\n",
      "Valid loss: 0.8300, Valid acc: 0.6661\n",
      "Epoch 16:\n",
      "Train loss: 0.6644, Train acc: 0.7368\n",
      "Valid loss: 0.8060, Valid acc: 0.6908\n",
      "Epoch 17:\n",
      "Train loss: 0.5924, Train acc: 0.7565\n",
      "Valid loss: 0.9379, Valid acc: 0.6633\n",
      "Epoch 18:\n",
      "Train loss: 0.5988, Train acc: 0.7531\n",
      "Valid loss: 0.8332, Valid acc: 0.6807\n",
      "Epoch 19:\n",
      "Train loss: 0.5641, Train acc: 0.7659\n",
      "Valid loss: 0.7955, Valid acc: 0.6917\n",
      "Epoch 20:\n",
      "Train loss: 0.5294, Train acc: 0.7719\n",
      "Valid loss: 0.7908, Valid acc: 0.6991\n",
      "Epoch 21:\n",
      "Train loss: 0.5240, Train acc: 0.7973\n",
      "Valid loss: 0.7723, Valid acc: 0.7312\n",
      "Epoch 22:\n",
      "Train loss: 0.5028, Train acc: 0.8173\n",
      "Valid loss: 0.8215, Valid acc: 0.7486\n",
      "Epoch 23:\n",
      "Train loss: 0.4757, Train acc: 0.8430\n",
      "Valid loss: 0.8132, Valid acc: 0.6908\n",
      "Epoch 24:\n",
      "Train loss: 0.4261, Train acc: 0.8735\n",
      "Valid loss: 0.7851, Valid acc: 0.7569\n",
      "Epoch 25:\n",
      "Train loss: 0.3798, Train acc: 0.8950\n",
      "Valid loss: 0.7633, Valid acc: 0.7743\n",
      "Epoch 26:\n",
      "Train loss: 0.3631, Train acc: 0.8989\n",
      "Valid loss: 0.8890, Valid acc: 0.7541\n",
      "Epoch 27:\n",
      "Train loss: 0.3408, Train acc: 0.9092\n",
      "Valid loss: 0.7639, Valid acc: 0.7853\n",
      "Epoch 28:\n",
      "Train loss: 0.3256, Train acc: 0.9117\n",
      "Valid loss: 0.8193, Valid acc: 0.7550\n",
      "Epoch 29:\n",
      "Train loss: 0.3185, Train acc: 0.9188\n",
      "Valid loss: 0.8355, Valid acc: 0.7844\n",
      "Epoch 30:\n",
      "Train loss: 0.2678, Train acc: 0.9349\n",
      "Valid loss: 0.8069, Valid acc: 0.7835\n",
      "Epoch 31:\n",
      "Train loss: 0.2609, Train acc: 0.9360\n",
      "Valid loss: 0.7507, Valid acc: 0.7945\n",
      "Epoch 32:\n",
      "Train loss: 0.2757, Train acc: 0.9271\n",
      "Valid loss: 0.7360, Valid acc: 0.8110\n",
      "Epoch 33:\n",
      "Train loss: 0.2451, Train acc: 0.9374\n",
      "Valid loss: 0.7654, Valid acc: 0.8055\n",
      "Epoch 34:\n",
      "Train loss: 0.2221, Train acc: 0.9452\n",
      "Valid loss: 0.7673, Valid acc: 0.8083\n",
      "Epoch 35:\n",
      "Train loss: 0.2558, Train acc: 0.9321\n",
      "Valid loss: 0.7203, Valid acc: 0.8266\n",
      "Epoch 36:\n",
      "Train loss: 0.2368, Train acc: 0.9406\n",
      "Valid loss: 0.6900, Valid acc: 0.8303\n",
      "Epoch 37:\n",
      "Train loss: 0.2296, Train acc: 0.9422\n",
      "Valid loss: 0.7534, Valid acc: 0.8202\n",
      "Epoch 38:\n",
      "Train loss: 0.2253, Train acc: 0.9448\n",
      "Valid loss: 1.3854, Valid acc: 0.6945\n",
      "Epoch 39:\n",
      "Train loss: 0.1838, Train acc: 0.9562\n",
      "Valid loss: 0.7112, Valid acc: 0.8358\n",
      "Epoch 40:\n",
      "Train loss: 0.2282, Train acc: 0.9448\n",
      "Valid loss: 0.6839, Valid acc: 0.8367\n",
      "Epoch 41:\n",
      "Train loss: 0.1903, Train acc: 0.9562\n",
      "Valid loss: 0.6866, Valid acc: 0.8367\n",
      "Epoch 42:\n",
      "Train loss: 0.2004, Train acc: 0.9539\n",
      "Valid loss: 0.8419, Valid acc: 0.7917\n",
      "Epoch 43:\n",
      "Train loss: 0.2152, Train acc: 0.9509\n",
      "Valid loss: 0.7001, Valid acc: 0.8275\n",
      "Epoch 44:\n",
      "Train loss: 0.1766, Train acc: 0.9580\n",
      "Valid loss: 0.7437, Valid acc: 0.8248\n",
      "Epoch 45:\n",
      "Train loss: 0.1526, Train acc: 0.9677\n",
      "Valid loss: 0.7286, Valid acc: 0.8339\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy average_last_3: 0.8400\n",
      "Epoch 1:\n",
      "Train loss: 1.6579, Train acc: 0.2295\n",
      "Valid loss: 1.6455, Valid acc: 0.2468\n",
      "Epoch 2:\n",
      "Train loss: 1.6459, Train acc: 0.2354\n",
      "Valid loss: 1.6512, Valid acc: 0.2046\n",
      "Epoch 3:\n",
      "Train loss: 1.6270, Train acc: 0.2611\n",
      "Valid loss: 1.6019, Valid acc: 0.3037\n",
      "Epoch 4:\n",
      "Train loss: 1.4320, Train acc: 0.4033\n",
      "Valid loss: 1.3686, Valid acc: 0.4147\n",
      "Epoch 5:\n",
      "Train loss: 1.2671, Train acc: 0.4679\n",
      "Valid loss: 1.3074, Valid acc: 0.4422\n",
      "Epoch 6:\n",
      "Train loss: 1.1750, Train acc: 0.5078\n",
      "Valid loss: 1.1632, Valid acc: 0.5266\n",
      "Epoch 7:\n",
      "Train loss: 1.0733, Train acc: 0.5734\n",
      "Valid loss: 1.1919, Valid acc: 0.5670\n",
      "Epoch 8:\n",
      "Train loss: 0.9966, Train acc: 0.6176\n",
      "Valid loss: 1.1243, Valid acc: 0.5394\n",
      "Epoch 9:\n",
      "Train loss: 0.9416, Train acc: 0.6451\n",
      "Valid loss: 1.1673, Valid acc: 0.5706\n",
      "Epoch 10:\n",
      "Train loss: 0.8690, Train acc: 0.6889\n",
      "Valid loss: 1.0232, Valid acc: 0.6174\n",
      "Epoch 11:\n",
      "Train loss: 0.8169, Train acc: 0.7075\n",
      "Valid loss: 1.3052, Valid acc: 0.5394\n",
      "Epoch 12:\n",
      "Train loss: 0.7661, Train acc: 0.7412\n",
      "Valid loss: 0.9815, Valid acc: 0.6560\n",
      "Epoch 13:\n",
      "Train loss: 0.7447, Train acc: 0.7490\n",
      "Valid loss: 0.9551, Valid acc: 0.6670\n",
      "Epoch 14:\n",
      "Train loss: 0.6913, Train acc: 0.7776\n",
      "Valid loss: 0.9372, Valid acc: 0.7119\n",
      "Epoch 15:\n",
      "Train loss: 0.6640, Train acc: 0.7898\n",
      "Valid loss: 0.9291, Valid acc: 0.6954\n",
      "Epoch 16:\n",
      "Train loss: 0.6044, Train acc: 0.8187\n",
      "Valid loss: 0.9352, Valid acc: 0.7294\n",
      "Epoch 17:\n",
      "Train loss: 0.5595, Train acc: 0.8377\n",
      "Valid loss: 1.0544, Valid acc: 0.6477\n",
      "Epoch 18:\n",
      "Train loss: 0.5650, Train acc: 0.8352\n",
      "Valid loss: 0.9123, Valid acc: 0.7404\n",
      "Epoch 19:\n",
      "Train loss: 0.4840, Train acc: 0.8709\n",
      "Valid loss: 0.8582, Valid acc: 0.7761\n",
      "Epoch 20:\n",
      "Train loss: 0.4834, Train acc: 0.8636\n",
      "Valid loss: 0.9308, Valid acc: 0.7239\n",
      "Epoch 21:\n",
      "Train loss: 0.4540, Train acc: 0.8783\n",
      "Valid loss: 0.9814, Valid acc: 0.7165\n",
      "Epoch 22:\n",
      "Train loss: 0.4518, Train acc: 0.8757\n",
      "Valid loss: 1.4469, Valid acc: 0.6505\n",
      "Epoch 23:\n",
      "Train loss: 0.4157, Train acc: 0.8893\n",
      "Valid loss: 0.8690, Valid acc: 0.7578\n",
      "Epoch 24:\n",
      "Train loss: 0.3997, Train acc: 0.8904\n",
      "Valid loss: 0.8196, Valid acc: 0.7761\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy average_last_4: 0.8340\n",
      "Epoch 1:\n",
      "Train loss: 1.4561, Train acc: 0.4409\n",
      "Valid loss: 1.4323, Valid acc: 0.4440\n",
      "Epoch 2:\n",
      "Train loss: 1.1873, Train acc: 0.5972\n",
      "Valid loss: 1.1109, Valid acc: 0.6486\n",
      "Epoch 3:\n",
      "Train loss: 0.9238, Train acc: 0.7107\n",
      "Valid loss: 0.9334, Valid acc: 0.6624\n",
      "Epoch 4:\n",
      "Train loss: 0.7438, Train acc: 0.7726\n",
      "Valid loss: 0.7740, Valid acc: 0.7404\n",
      "Epoch 5:\n",
      "Train loss: 0.6363, Train acc: 0.7967\n",
      "Valid loss: 0.7522, Valid acc: 0.7440\n",
      "Epoch 6:\n",
      "Train loss: 0.5752, Train acc: 0.8127\n",
      "Valid loss: 0.6899, Valid acc: 0.7578\n",
      "Epoch 7:\n",
      "Train loss: 0.5085, Train acc: 0.8329\n",
      "Valid loss: 0.6262, Valid acc: 0.7862\n",
      "Epoch 8:\n",
      "Train loss: 0.4585, Train acc: 0.8494\n",
      "Valid loss: 0.6286, Valid acc: 0.7569\n",
      "Epoch 9:\n",
      "Train loss: 0.4260, Train acc: 0.8595\n",
      "Valid loss: 0.6836, Valid acc: 0.7358\n",
      "Epoch 10:\n",
      "Train loss: 0.3788, Train acc: 0.8815\n",
      "Valid loss: 0.5728, Valid acc: 0.8000\n",
      "Epoch 11:\n",
      "Train loss: 0.3550, Train acc: 0.8831\n",
      "Valid loss: 0.5436, Valid acc: 0.8138\n",
      "Epoch 12:\n",
      "Train loss: 0.3250, Train acc: 0.8973\n",
      "Valid loss: 0.5152, Valid acc: 0.8275\n",
      "Epoch 13:\n",
      "Train loss: 0.3141, Train acc: 0.9042\n",
      "Valid loss: 0.5190, Valid acc: 0.8174\n",
      "Epoch 14:\n",
      "Train loss: 0.2828, Train acc: 0.9182\n",
      "Valid loss: 0.4866, Valid acc: 0.8367\n",
      "Epoch 15:\n",
      "Train loss: 0.2665, Train acc: 0.9216\n",
      "Valid loss: 0.5014, Valid acc: 0.8367\n",
      "Epoch 16:\n",
      "Train loss: 0.2459, Train acc: 0.9260\n",
      "Valid loss: 0.5197, Valid acc: 0.8275\n",
      "Epoch 17:\n",
      "Train loss: 0.2268, Train acc: 0.9344\n",
      "Valid loss: 0.4835, Valid acc: 0.8440\n",
      "Epoch 18:\n",
      "Train loss: 0.2066, Train acc: 0.9381\n",
      "Valid loss: 0.4621, Valid acc: 0.8450\n",
      "Epoch 19:\n",
      "Train loss: 0.2076, Train acc: 0.9351\n",
      "Valid loss: 0.4626, Valid acc: 0.8505\n",
      "Epoch 20:\n",
      "Train loss: 0.1858, Train acc: 0.9475\n",
      "Valid loss: 0.5147, Valid acc: 0.8339\n",
      "Epoch 21:\n",
      "Train loss: 0.1757, Train acc: 0.9493\n",
      "Valid loss: 0.4693, Valid acc: 0.8596\n",
      "Epoch 22:\n",
      "Train loss: 0.1590, Train acc: 0.9537\n",
      "Valid loss: 0.4625, Valid acc: 0.8569\n",
      "Epoch 23:\n",
      "Train loss: 0.1628, Train acc: 0.9523\n",
      "Valid loss: 0.6400, Valid acc: 0.8064\n",
      "Epoch 24:\n",
      "Train loss: 0.1509, Train acc: 0.9576\n",
      "Valid loss: 0.4534, Valid acc: 0.8587\n",
      "Epoch 25:\n",
      "Train loss: 0.1318, Train acc: 0.9638\n",
      "Valid loss: 0.5090, Valid acc: 0.8367\n",
      "Epoch 26:\n",
      "Train loss: 0.1193, Train acc: 0.9672\n",
      "Valid loss: 0.4626, Valid acc: 0.8596\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy max: 0.8980\n",
      "Epoch 1:\n",
      "Train loss: 1.5007, Train acc: 0.3673\n",
      "Valid loss: 1.4204, Valid acc: 0.4560\n",
      "Epoch 2:\n",
      "Train loss: 1.1946, Train acc: 0.5342\n",
      "Valid loss: 1.3812, Valid acc: 0.4688\n",
      "Epoch 3:\n",
      "Train loss: 0.9905, Train acc: 0.6261\n",
      "Valid loss: 1.1830, Valid acc: 0.5367\n",
      "Epoch 4:\n",
      "Train loss: 0.8128, Train acc: 0.7210\n",
      "Valid loss: 0.8803, Valid acc: 0.7000\n",
      "Epoch 5:\n",
      "Train loss: 0.6740, Train acc: 0.7785\n",
      "Valid loss: 0.8726, Valid acc: 0.6991\n",
      "Epoch 6:\n",
      "Train loss: 0.5832, Train acc: 0.8157\n",
      "Valid loss: 0.7434, Valid acc: 0.7532\n",
      "Epoch 7:\n",
      "Train loss: 0.5210, Train acc: 0.8375\n",
      "Valid loss: 0.6971, Valid acc: 0.7679\n",
      "Epoch 8:\n",
      "Train loss: 0.4663, Train acc: 0.8540\n",
      "Valid loss: 0.5897, Valid acc: 0.8046\n",
      "Epoch 9:\n",
      "Train loss: 0.4329, Train acc: 0.8641\n",
      "Valid loss: 0.7166, Valid acc: 0.7642\n",
      "Epoch 10:\n",
      "Train loss: 0.3909, Train acc: 0.8847\n",
      "Valid loss: 0.9581, Valid acc: 0.7064\n",
      "Epoch 11:\n",
      "Train loss: 0.3656, Train acc: 0.8911\n",
      "Valid loss: 0.5886, Valid acc: 0.8073\n",
      "Epoch 12:\n",
      "Train loss: 0.3405, Train acc: 0.9007\n",
      "Valid loss: 0.5353, Valid acc: 0.8284\n",
      "Epoch 13:\n",
      "Train loss: 0.3201, Train acc: 0.9058\n",
      "Valid loss: 0.5395, Valid acc: 0.8266\n",
      "Epoch 14:\n",
      "Train loss: 0.2813, Train acc: 0.9184\n",
      "Valid loss: 0.6000, Valid acc: 0.8156\n",
      "Epoch 15:\n",
      "Train loss: 0.2727, Train acc: 0.9230\n",
      "Valid loss: 0.5822, Valid acc: 0.8220\n",
      "Epoch 16:\n",
      "Train loss: 0.2575, Train acc: 0.9262\n",
      "Valid loss: 0.5082, Valid acc: 0.8404\n",
      "Epoch 17:\n",
      "Train loss: 0.2331, Train acc: 0.9354\n",
      "Valid loss: 0.5190, Valid acc: 0.8404\n",
      "Epoch 18:\n",
      "Train loss: 0.2245, Train acc: 0.9390\n",
      "Valid loss: 0.5308, Valid acc: 0.8468\n",
      "Epoch 19:\n",
      "Train loss: 0.2076, Train acc: 0.9457\n",
      "Valid loss: 0.4541, Valid acc: 0.8688\n",
      "Epoch 20:\n",
      "Train loss: 0.1835, Train acc: 0.9516\n",
      "Valid loss: 0.7255, Valid acc: 0.7908\n",
      "Epoch 21:\n",
      "Train loss: 0.1892, Train acc: 0.9507\n",
      "Valid loss: 0.4903, Valid acc: 0.8532\n",
      "Epoch 22:\n",
      "Train loss: 0.1588, Train acc: 0.9617\n",
      "Valid loss: 0.4504, Valid acc: 0.8697\n",
      "Epoch 23:\n",
      "Train loss: 0.1776, Train acc: 0.9525\n",
      "Valid loss: 0.4648, Valid acc: 0.8761\n",
      "Epoch 24:\n",
      "Train loss: 0.1515, Train acc: 0.9594\n",
      "Valid loss: 0.5104, Valid acc: 0.8477\n",
      "Epoch 25:\n",
      "Train loss: 0.1412, Train acc: 0.9656\n",
      "Valid loss: 0.5007, Valid acc: 0.8550\n",
      "Epoch 26:\n",
      "Train loss: 0.1248, Train acc: 0.9684\n",
      "Valid loss: 0.6386, Valid acc: 0.8165\n",
      "Epoch 27:\n",
      "Train loss: 0.1271, Train acc: 0.9668\n",
      "Valid loss: 0.5218, Valid acc: 0.8505\n",
      "Epoch 28:\n",
      "Train loss: 0.1097, Train acc: 0.9732\n",
      "Valid loss: 0.5102, Valid acc: 0.8670\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy mean: 0.8940\n",
      "Epoch 1:\n",
      "Train loss: 1.5063, Train acc: 0.3886\n",
      "Valid loss: 1.4429, Valid acc: 0.4358\n",
      "Epoch 2:\n",
      "Train loss: 1.2099, Train acc: 0.5903\n",
      "Valid loss: 1.3273, Valid acc: 0.4881\n",
      "Epoch 3:\n",
      "Train loss: 0.9557, Train acc: 0.6761\n",
      "Valid loss: 0.8798, Valid acc: 0.6972\n",
      "Epoch 4:\n",
      "Train loss: 0.7709, Train acc: 0.7425\n",
      "Valid loss: 0.9749, Valid acc: 0.6156\n",
      "Epoch 5:\n",
      "Train loss: 0.6538, Train acc: 0.7861\n",
      "Valid loss: 0.6918, Valid acc: 0.7661\n",
      "Epoch 6:\n",
      "Train loss: 0.5733, Train acc: 0.8063\n",
      "Valid loss: 0.6396, Valid acc: 0.7780\n",
      "Epoch 7:\n",
      "Train loss: 0.5179, Train acc: 0.8285\n",
      "Valid loss: 0.6181, Valid acc: 0.7780\n",
      "Epoch 8:\n",
      "Train loss: 0.4640, Train acc: 0.8478\n",
      "Valid loss: 0.6097, Valid acc: 0.7817\n",
      "Epoch 9:\n",
      "Train loss: 0.4199, Train acc: 0.8663\n",
      "Valid loss: 0.5647, Valid acc: 0.7991\n",
      "Epoch 10:\n",
      "Train loss: 0.3874, Train acc: 0.8771\n",
      "Valid loss: 0.5823, Valid acc: 0.7936\n",
      "Epoch 11:\n",
      "Train loss: 0.3464, Train acc: 0.8879\n",
      "Valid loss: 0.5206, Valid acc: 0.8284\n",
      "Epoch 12:\n",
      "Train loss: 0.3323, Train acc: 0.8959\n",
      "Valid loss: 0.6485, Valid acc: 0.7725\n",
      "Epoch 13:\n",
      "Train loss: 0.3013, Train acc: 0.9074\n",
      "Valid loss: 0.5148, Valid acc: 0.8220\n",
      "Epoch 14:\n",
      "Train loss: 0.2821, Train acc: 0.9154\n",
      "Valid loss: 0.5862, Valid acc: 0.8000\n",
      "Epoch 15:\n",
      "Train loss: 0.2573, Train acc: 0.9214\n",
      "Valid loss: 0.4777, Valid acc: 0.8431\n",
      "Epoch 16:\n",
      "Train loss: 0.2406, Train acc: 0.9319\n",
      "Valid loss: 0.5538, Valid acc: 0.8358\n",
      "Epoch 17:\n",
      "Train loss: 0.2237, Train acc: 0.9349\n",
      "Valid loss: 0.7147, Valid acc: 0.7624\n",
      "Epoch 18:\n",
      "Train loss: 0.2149, Train acc: 0.9395\n",
      "Valid loss: 0.5543, Valid acc: 0.8321\n",
      "Epoch 19:\n",
      "Train loss: 0.1883, Train acc: 0.9475\n",
      "Valid loss: 0.7101, Valid acc: 0.7881\n",
      "Epoch 20:\n",
      "Train loss: 0.1845, Train acc: 0.9482\n",
      "Valid loss: 0.5156, Valid acc: 0.8404\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy maxmean: 0.8560\n",
      "Epoch 1:\n",
      "Train loss: 1.2698, Train acc: 0.5197\n",
      "Valid loss: 1.1405, Valid acc: 0.5862\n",
      "Epoch 2:\n",
      "Train loss: 0.9695, Train acc: 0.6527\n",
      "Valid loss: 0.9361, Valid acc: 0.6661\n",
      "Epoch 3:\n",
      "Train loss: 0.7824, Train acc: 0.7315\n",
      "Valid loss: 0.8255, Valid acc: 0.6872\n",
      "Epoch 4:\n",
      "Train loss: 0.6567, Train acc: 0.7728\n",
      "Valid loss: 0.7274, Valid acc: 0.7367\n",
      "Epoch 5:\n",
      "Train loss: 0.5491, Train acc: 0.8095\n",
      "Valid loss: 0.6753, Valid acc: 0.7477\n",
      "Epoch 6:\n",
      "Train loss: 0.4802, Train acc: 0.8400\n",
      "Valid loss: 0.6341, Valid acc: 0.7633\n",
      "Epoch 7:\n",
      "Train loss: 0.4075, Train acc: 0.8608\n",
      "Valid loss: 0.8417, Valid acc: 0.7202\n",
      "Epoch 8:\n",
      "Train loss: 0.3557, Train acc: 0.8753\n",
      "Valid loss: 0.5697, Valid acc: 0.8101\n",
      "Epoch 9:\n",
      "Train loss: 0.3048, Train acc: 0.8998\n",
      "Valid loss: 0.8660, Valid acc: 0.7367\n",
      "Epoch 10:\n",
      "Train loss: 0.2706, Train acc: 0.9108\n",
      "Valid loss: 0.6594, Valid acc: 0.7826\n",
      "Epoch 11:\n",
      "Train loss: 0.2428, Train acc: 0.9232\n",
      "Valid loss: 0.6012, Valid acc: 0.8092\n",
      "Epoch 12:\n",
      "Train loss: 0.1995, Train acc: 0.9379\n",
      "Valid loss: 0.5856, Valid acc: 0.8275\n",
      "Epoch 13:\n",
      "Train loss: 0.1881, Train acc: 0.9367\n",
      "Valid loss: 0.5765, Valid acc: 0.8248\n",
      "Epoch 14:\n",
      "Train loss: 0.1741, Train acc: 0.9493\n",
      "Valid loss: 0.8332, Valid acc: 0.7734\n",
      "Epoch 15:\n",
      "Train loss: 0.1373, Train acc: 0.9562\n",
      "Valid loss: 0.5631, Valid acc: 0.8339\n",
      "Epoch 16:\n",
      "Train loss: 0.1244, Train acc: 0.9649\n",
      "Valid loss: 0.5359, Valid acc: 0.8385\n",
      "Epoch 17:\n",
      "Train loss: 0.1081, Train acc: 0.9622\n",
      "Valid loss: 0.9590, Valid acc: 0.7651\n",
      "Epoch 18:\n",
      "Train loss: 0.1080, Train acc: 0.9649\n",
      "Valid loss: 0.5939, Valid acc: 0.8422\n",
      "Epoch 19:\n",
      "Train loss: 0.0797, Train acc: 0.9778\n",
      "Valid loss: 0.5830, Valid acc: 0.8477\n",
      "Epoch 20:\n",
      "Train loss: 0.0894, Train acc: 0.9716\n",
      "Valid loss: 0.6377, Valid acc: 0.8248\n",
      "Epoch 21:\n",
      "Train loss: 0.0773, Train acc: 0.9752\n",
      "Valid loss: 0.7064, Valid acc: 0.8266\n",
      "Epoch 22:\n",
      "Train loss: 0.1570, Train acc: 0.9748\n",
      "Valid loss: 0.6379, Valid acc: 0.8303\n",
      "Epoch 23:\n",
      "Train loss: 0.0382, Train acc: 0.9881\n",
      "Valid loss: 0.6237, Valid acc: 0.8477\n",
      "Epoch 24:\n",
      "Train loss: 0.0424, Train acc: 0.9888\n",
      "Valid loss: 0.8444, Valid acc: 0.8055\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n",
      "Test Accuracy sum: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Set ideal hyperparameters from 2(a)\n",
    "if best_hyperparams[\"epochs ran\"] <= 50:\n",
    "    no_epoch = 50\n",
    "elif best_hyperparams[\"epochs ran\"] > 50 & best_hyperparams[\"epochs ran\"] <= 100:\n",
    "    no_epoch = 100\n",
    "else:\n",
    "    no_epoch = 200\n",
    "batch_size = best_hyperparams[\"batch_size\"]\n",
    "hidden_dim = best_hyperparams[\"hidden_dim\"]\n",
    "lr = best_hyperparams[\"lr\"]\n",
    "optimizer = best_hyperparams[\"optimizer\"]\n",
    "\n",
    "# Set regularization from 2(b)\n",
    "dropout = df_all.loc[df_all['test_acc'].idxmax(), 'dropout']\n",
    "weight_decay = df_all.loc[df_all['test_acc'].idxmax(), 'weight_decay']\n",
    "grad_clip = df_all.loc[df_all['test_acc'].idxmax(), 'grad_clip']\n",
    "max_norm = df_all.loc[df_all['test_acc'].idxmax(), 'max_norm']\n",
    "\n",
    "best_representation_technique = []\n",
    "best_representation_technique.append({'technique': 'baseline', 'test_acc': test_acc})\n",
    "\n",
    "# List and try all the methods, and save it to a dataframe\n",
    "techniques = ['average_last_2', 'average_last_3', 'average_last_4',\n",
    "              'max', 'mean', 'maxmean', 'sum']\n",
    "\n",
    "for technique in techniques:\n",
    "    results = representation_test(technique, batch_size, hidden_dim, lr, optimizer, no_epoch, weight_decay, dropout, grad_clip, max_norm)\n",
    "    best_representation_technique.append(results)\n",
    "\n",
    "df_rep = pd.DataFrame(best_representation_technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "718e1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "technique",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ed6390e9-f74e-4e8a-87dc-54c6b4e6a9ae",
       "rows": [
        [
         "4",
         "max",
         "0.898"
        ],
        [
         "5",
         "mean",
         "0.894"
        ],
        [
         "7",
         "sum",
         "0.878"
        ],
        [
         "6",
         "maxmean",
         "0.856"
        ],
        [
         "2",
         "average_last_3",
         "0.84"
        ],
        [
         "3",
         "average_last_4",
         "0.834"
        ],
        [
         "0",
         "baseline",
         "0.818"
        ],
        [
         "1",
         "average_last_2",
         "0.778"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>technique</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sum</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>maxmean</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>average_last_3</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average_last_4</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>average_last_2</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        technique  test_acc\n",
       "4             max     0.898\n",
       "5            mean     0.894\n",
       "7             sum     0.878\n",
       "6         maxmean     0.856\n",
       "2  average_last_3     0.840\n",
       "3  average_last_4     0.834\n",
       "0        baseline     0.818\n",
       "1  average_last_2     0.778"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort saved dataframe by test accuracy and print it\n",
    "df_rep_sorted = df_rep.sort_values(by='test_acc', ascending=False)\n",
    "df_rep_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a2b1c",
   "metadata": {},
   "source": [
    "### Answer (d):\n",
    "Maxmean pooling showed the best test accuracy out of all the methods, with accuracy of 0.866. To be fair, all of the methods demonstrate a test accuracy that is relatively similar to one another of more than 0.8. Here, we speculate some of the reasons why these results are as such:\n",
    "\n",
    "1. Global pooling methods such as sum and mean pooling aggregate information from all time steps, which allow the model to capture features from the entire sequence rather than just the final few tokens. Max pooling focuses on the most discriminative features which helps significantly when there are specific keywords or phrases that strongly indicate a certain question topic. Maxmean pooling is able to take the best of both mean and max pooling by capturing both strong and soft contextual cues, improving the performance of the model.\n",
    "2. Unlikely sum pooling, mean and maxmean pooling normalise activations over the length of the sentence, which could be beneficial when longer or information dense sentence exert stronger influence on the classifier as it can stabilise the performance of the model.\n",
    "3. Averaging only recent hidden states can cause the model to miss earlier contextual information, which could be more important in sentence representation. This could explain why average_last_2, average_last_3 and last_hidden_state (baseline) performed the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120f1dd",
   "metadata": {},
   "source": [
    "**(e) Report topic-wise accuracy (accuracy for each topic) on the test set for the best model you have. Discuss what may cause the difference in accuracies across different topic categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0bc89145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ideal hyperparameters from 2(a)\n",
    "if best_hyperparams[\"epochs ran\"] <= 50:\n",
    "    no_epoch = 50\n",
    "elif best_hyperparams[\"epochs ran\"] > 50 & best_hyperparams[\"epochs ran\"] <= 100:\n",
    "    no_epoch = 100\n",
    "else:\n",
    "    no_epoch = 200\n",
    "batch_size = best_hyperparams[\"batch_size\"]\n",
    "hidden_dim = best_hyperparams[\"hidden_dim\"]\n",
    "lr = best_hyperparams[\"lr\"]\n",
    "optimizer = best_hyperparams[\"optimizer\"]\n",
    "\n",
    "# Set regularization from 2(b)\n",
    "dropout = df_all.loc[df_all['test_acc'].idxmax(), 'dropout']\n",
    "weight_decay = df_all.loc[df_all['test_acc'].idxmax(), 'weight_decay']\n",
    "grad_clip = df_all.loc[df_all['test_acc'].idxmax(), 'grad_clip']\n",
    "max_norm = df_all.loc[df_all['test_acc'].idxmax(), 'max_norm']\n",
    "\n",
    "# Set best representation technique from 2(d)\n",
    "strategy = df_rep.loc[df_rep['test_acc'].idxmax(), 'technique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74257ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 1.5006, Train acc: 0.4507\n",
      "Valid loss: 1.3403, Valid acc: 0.5358\n",
      "Epoch 2:\n",
      "Train loss: 1.1703, Train acc: 0.6080\n",
      "Valid loss: 1.1147, Valid acc: 0.6459\n",
      "Epoch 3:\n",
      "Train loss: 0.9397, Train acc: 0.7095\n",
      "Valid loss: 0.9998, Valid acc: 0.6651\n",
      "Epoch 4:\n",
      "Train loss: 0.7551, Train acc: 0.7632\n",
      "Valid loss: 0.7727, Valid acc: 0.7468\n",
      "Epoch 5:\n",
      "Train loss: 0.6248, Train acc: 0.8017\n",
      "Valid loss: 0.7307, Valid acc: 0.7468\n",
      "Epoch 6:\n",
      "Train loss: 0.5450, Train acc: 0.8226\n",
      "Valid loss: 0.6307, Valid acc: 0.7817\n",
      "Epoch 7:\n",
      "Train loss: 0.4817, Train acc: 0.8492\n",
      "Valid loss: 0.6190, Valid acc: 0.7817\n",
      "Epoch 8:\n",
      "Train loss: 0.4303, Train acc: 0.8631\n",
      "Valid loss: 0.5692, Valid acc: 0.8083\n",
      "Epoch 9:\n",
      "Train loss: 0.3924, Train acc: 0.8771\n",
      "Valid loss: 0.5764, Valid acc: 0.8083\n",
      "Epoch 10:\n",
      "Train loss: 0.3675, Train acc: 0.8849\n",
      "Valid loss: 0.5035, Valid acc: 0.8257\n",
      "Epoch 11:\n",
      "Train loss: 0.3287, Train acc: 0.8994\n",
      "Valid loss: 0.5267, Valid acc: 0.8174\n",
      "Epoch 12:\n",
      "Train loss: 0.3103, Train acc: 0.9014\n",
      "Valid loss: 0.5024, Valid acc: 0.8321\n",
      "Epoch 13:\n",
      "Train loss: 0.2842, Train acc: 0.9133\n",
      "Valid loss: 0.5666, Valid acc: 0.8064\n",
      "Epoch 14:\n",
      "Train loss: 0.2680, Train acc: 0.9170\n",
      "Valid loss: 0.4586, Valid acc: 0.8486\n",
      "Epoch 15:\n",
      "Train loss: 0.2384, Train acc: 0.9296\n",
      "Valid loss: 0.5527, Valid acc: 0.8147\n",
      "Epoch 16:\n",
      "Train loss: 0.2259, Train acc: 0.9285\n",
      "Valid loss: 0.5736, Valid acc: 0.8101\n",
      "Epoch 17:\n",
      "Train loss: 0.2230, Train acc: 0.9333\n",
      "Valid loss: 0.4645, Valid acc: 0.8459\n",
      "Epoch 18:\n",
      "Train loss: 0.1977, Train acc: 0.9418\n",
      "Valid loss: 0.4551, Valid acc: 0.8606\n",
      "Epoch 19:\n",
      "Train loss: 0.1906, Train acc: 0.9454\n",
      "Valid loss: 0.5818, Valid acc: 0.8248\n",
      "Epoch 20:\n",
      "Train loss: 0.1770, Train acc: 0.9482\n",
      "Valid loss: 0.4474, Valid acc: 0.8661\n",
      "Epoch 21:\n",
      "Train loss: 0.1832, Train acc: 0.9473\n",
      "Valid loss: 0.4533, Valid acc: 0.8596\n",
      "Epoch 22:\n",
      "Train loss: 0.1477, Train acc: 0.9574\n",
      "Valid loss: 0.4436, Valid acc: 0.8606\n",
      "Epoch 23:\n",
      "Train loss: 0.1454, Train acc: 0.9583\n",
      "Valid loss: 0.4690, Valid acc: 0.8596\n",
      "Epoch 24:\n",
      "Train loss: 0.1358, Train acc: 0.9624\n",
      "Valid loss: 0.4813, Valid acc: 0.8569\n",
      "Epoch 25:\n",
      "Train loss: 0.1265, Train acc: 0.9631\n",
      "Valid loss: 0.5013, Valid acc: 0.8550\n",
      "Early Stopping Triggered! No Improvements to Validation Accuracy within Patience.\n"
     ]
    }
   ],
   "source": [
    "# Define the labels/topics for TREC dataset\n",
    "topics = [\"DESC\", \"ENTY\", \"HUM\", \"ABBR\", \"NUM\", \"LOC\"]\n",
    "topic_datasets = {}\n",
    "\n",
    "# Rerun training with best hyperparameters, regularisation and representation technique\n",
    "model = ClassifierRepresentationRNN(TEXT.vocab.vectors.numpy() , hidden_dim, strategy, dropout=dropout)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialise optimiser\n",
    "optimizer = optimizer.__class__(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if grad_clip == True:\n",
    "    train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch, grad_clip=True, max_norm=max_norm)\n",
    "else:\n",
    "    train_losses, _, _, valid_accuracies, _ = training_step(model, train_loader, valid_loader, optimizer, criterion, no_epoch)\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    examples = [eg for eg in test_data.examples if eg.label == topic]\n",
    "    texts = [\" \".join(example.text) for example in examples]\n",
    "    labels = [LABEL.vocab.stoi[example.label] for example in examples]\n",
    "    topic_datasets[topic] = SentenceDataset(texts, labels, TEXT.vocab.stoi)\n",
    "\n",
    "# Define function to obtain test accuracy by topic\n",
    "def test_by_topic(model ,topic_dataset, batch_size, hidden_dim, strategy, dropout, optimizer, lr, weight_decay, grad_clip, max_norm, no_epoch):\n",
    "\n",
    "    test_acc = test_loop(model, DataLoader(topic_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn))\n",
    "\n",
    "    return {\n",
    "        'topic': topic_dataset,\n",
    "        'test_acc': test_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c33d822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "94\n",
      "65\n",
      "9\n",
      "113\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# Check length of each topic test dataset\n",
    "for dataset in topic_datasets.values():\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c89cfc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "topic",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "801aa94a-6d13-43c8-9395-be0de446c681",
       "rows": [
        [
         "0",
         "DESC",
         "0.9347826086956522"
        ],
        [
         "1",
         "ENTY",
         "0.8297872340425532"
        ],
        [
         "2",
         "HUM",
         "0.9692307692307692"
        ],
        [
         "3",
         "ABBR",
         "0.7777777777777778"
        ],
        [
         "4",
         "NUM",
         "0.9026548672566371"
        ],
        [
         "5",
         "LOC",
         "0.8395061728395061"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DESC</td>\n",
       "      <td>0.934783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>0.829787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUM</td>\n",
       "      <td>0.969231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NUM</td>\n",
       "      <td>0.902655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.839506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic  test_acc\n",
       "0  DESC  0.934783\n",
       "1  ENTY  0.829787\n",
       "2   HUM  0.969231\n",
       "3  ABBR  0.777778\n",
       "4   NUM  0.902655\n",
       "5   LOC  0.839506"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test accuracy into dataframe and print by topic\n",
    "acc_by_topic = []\n",
    "\n",
    "for topic, dataset in topic_datasets.items():\n",
    "    result = test_by_topic(model, dataset, batch_size, hidden_dim, strategy, dropout, optimizer, lr, weight_decay, grad_clip, max_norm, no_epoch)\n",
    "    acc_by_topic.append({'topic': topic, 'test_acc': result[\"test_acc\"]})\n",
    "\n",
    "df_topics = pd.DataFrame(acc_by_topic)\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e91fd",
   "metadata": {},
   "source": [
    "### Answer (e):\n",
    "The model performs best on DESC (description) and HUM (human) questions, and performs worst on ENTY (entity) and ABBR (abbreviation) questions. \n",
    "- One reason for the poor performance on ENTY questions is that the question type may be more diverse in its type of questions. For example, ENTY questions tend to use what or which in the question, and these words are used in other topics as well. Compared to the topic HUM which has almost exclusive use of who, it would be harder for the model to distinguish ENTY questions for other topics.\n",
    "- One reason for the poorer performance for ABBR questions is the small number of samples available in the training and testing data. There are only around 70 questions in our training data with 4362 questions, and 9 questions in the test set, which would lead to the model not being as well trained to distinguish ABBR questions, especially on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c5fe3",
   "metadata": {},
   "source": [
    "## End of Qn 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC4002-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
